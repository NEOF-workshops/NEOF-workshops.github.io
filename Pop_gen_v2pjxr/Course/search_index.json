[["01-Intro.html", "Population Genomics Chapter 1 Introduction Table of contents", " Population Genomics Helen Hipperson, Katy Maher, Victor Soria-Carrasco, Graeme Fox 2023-09-05 Chapter 1 Introduction In this practical session we will introduce you to analysing SNP datasets for population genomic studies. Table of contents Background Cluster Sequence QC Mapping sequences to reference SNP and genotype calling Population structure Population summary statistics Detecting selection with HMM GWAS Appendix This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Background.html", "Chapter 2 Background 2.1 Example data: Whole-genome resequencing of killer whales", " Chapter 2 Background In this workshop we will work through examples of analyses that can be performed using large datasets of genetic variation (single nucleotide polymorphisms - SNPs) obtained through next-generation sequencing. 2.1 Example data: Whole-genome resequencing of killer whales For the first set of exercises on Monday afternoon, we will use whole-genome resequencing data from Foote et al. 2019. These data were generated to examine population genetic structure and admixture in killer whale populations from around the globe. A total of 56 individuals were analysed in the study - combining data sets of low-coverage whole-genome re-sequencing and RADseq. All the data were generated using Illumina sequencing. To allow us to run analyses in a timely manner for the workshop, we will initially perform QC and map a subset of sequencing reads from one individual, call and filter SNPs for three individuals, and perform some downstream analyses on 16 individuals. Here are some figures from the Foote et al. paper showing sampling locations and genetic structure using PCA plots and a plot of admixture proportions. We will generate similar plots in Chapter 8, although they will look different to these as we are using a subset of the data from this paper. "],["03-Cluster_Introduction.html", "Chapter 3 Cluster Introduction 3.1 Logon instructions 3.2 The Terminal Window 3.3 Accessing the example data", " Chapter 3 Cluster Introduction 3.1 Logon instructions For this workshop, we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent. You will now be in a logged-in Linux VNC desktop with two terminals. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance. If the VNC is taking up too much/little space of your browser, you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal. These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop, we would recommend Ubuntu. The following link is a guide to install Ubuntu: https://www.ubuntu.com/download/desktop/install-ubuntu-desktop. If you use a USB you need to create a bootable USB stick. The following link will assist: https://www.ubuntu.com/download/desktop/create-a-usb-stick-on-windows 3.2 The Terminal Window In our case, the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences, it may look different. Already, there is useful information for us on the terminal window. nsc006: This is the login name, also known as the username. In this case, nsc006 is a demonstrator's account. Your screen should show a different account name, which will be your username for the Linux machine/cluster you are logged into. gauss03: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux OS and others '~' is a shortcut to the user's home directory. Everything after the '$' is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen and choose Terminal 3.3 Accessing the example data Before we can start, we first need to make a directory which will be used to contain all the files you generate throughout this workshop. To do this, type the following commands. #Change directory to home cd ~ #Make a directory in home called &quot;popgenomics&quot; mkdir popgenomics #Change directory to &quot;popgenomics&quot; cd popgenomics Then copy the example data for the workshop into your 'popgenomics' directory. These are the example files for all days of the workshop and will take several minutes to copy across. cp -r /pub14/tea/nsc206/NEOF/popgenomics/data/ . You will need to activate the popgenomics conda environment before continuing. Carry this out with the following command. Note: Ensure you include the dot and space (. ) at the start before usepopgenomics. . usepopgenomics You're now ready to start the analyses! "],["04-QC.html", "Chapter 4 QC Tutorials 4.1 Workshop data 4.2 Killer whale genomic dataset 4.3 Quality assessment 4.4 Quality control", " Chapter 4 QC Tutorials This tutorial will give hands on experience with quality control of genomic Illumina data. We will first look at the quality of the data 4.1 Workshop data Before we can carry on with the workshop, ensure you are in your working directory. cd ~/popgenomics Before running any examples, let us change directory to the raw fastq data directory. cd data/fastq/ 4.2 Killer whale genomic dataset This dataset contains an example fastq file from one individual (sample name SRR8861574) of a re-sequencing project on killer whales. The directory contains the forward (1) and reverse (2) read fastq files, and has been downsized to allow processing in a short amount of time for the workshop. List the contents of the current directory to confirm the two files are there. ls Have a look at the structure of the input fastq file: zcat SRR8861574_1_small.fastq.gz | head These fastq files are compressed (with the .gz file extension). zcat is a command for viewing the contents of compressed files, the pipe | symbol then passes this to head to display only the first few lines of the file. Fastq files contain a header line, the nucleotide sequence, and its corresponding quality scores. 4.3 Quality assessment We’ll run the raw sequence data through FastQC to summarise the data quality. First, make a directory for the output. mkdir raw_fastqc_output Now we can run FastQC. Note: The below command can be run over one line excluding the \\ Alternatively, you can type \\ during a command and then press the enter key. The next line on the command line will start with &gt;. This will allow you to continue typing the command on the line. This can be used to type one command over multiple lines. fastqc -o raw_fastqc_output \\ SRR8861574_1_small.fastq.gz SRR8861574_2_small.fastq.gz Normally, when you run a command it will run the process in the foreground. This means your command line will be locked whilst the process is running. You will only be able to type a new command in the terminal once the foreground process is finished. This is normally wanted but not always, for example when you run a process like firefox. To run a process in the background, so you can type and run new commands, you can add the symbol &amp; to the end of a command. We will use this for running firefox. Please follow this link for more info on foreground and background processing. Using firefox, have a look at the output html reports and answer the questions further below. To look at R1 fastQC output firefox raw_fastqc_output/SRR8861574_1_small_fastqc.html &amp; You may see a warning message on your terminal, but you can press enter to continue to type in the terminal. To look at R2 fastQC output firefox raw_fastqc_output/SRR8861574_2_small_fastqc.html &amp; How many total reads are there in both the files? 350000 355071 500000 565000 What is the length of the reads? 35-76 50-76 75-100 75-150 In the read 2 file, at what base position does the quality of all the reads go below 28 (i.e. the position where not all of the box plot is in the green)? 70-71 72-73 74-75 76 Are there any sequences with adapter content or overrepresented sequences? Yes No In this case the reads seem to be very high quality. This is likely as these reads are relatively short for an Illumina sequencing run. Longer sequences can be of poorer quality and may contain adapter sequence, especially towards the ends of the reads. Unfortunately, data from a sequencing machine will often not look so nice and will require quality trimming and filtering. For most datasets: The quality decreases towards the end of the reads The R2 reads have poorer quality than the R1 reads The read sizes have a range compared to all being one size. However, most of the reads are towards the long end of the range. Generally, even if data do look very nice we would carry out quality control to get rid of any poor data that is masked by the very good data and to remove any adapter sequences. 4.4 Quality control Quality control generally comes in two forms: Trimming: This is directly cutting off bits of sequence. This is typical in the form of trimming off low quality bases from the end of reads and trimming off adapters at the start of reads. Filtering: This occurs when entire reads are removed. A typical occurrence of this is when a read is too short as we do not want reads below a certain length. To carry this out, we are going to use Trimmomatic. To run Trimmomatic with the reads we will use the command below. trimmomatic \\ PE -phred33 \\ SRR8861574_1_small.fastq.gz SRR8861574_2_small.fastq.gz \\ SRR8861574_1_out_paired.fastq SRR8861574_1_out_unpaired.fastq \\ SRR8861574_2_out_paired.fastq SRR8861574_2_out_unpaired.fastq \\ ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 \\ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:30 MINLEN:50 The parameter meanings are: PE: Input data consists of paired-end reads. -phred33: Type of Phred quality encoding. In this case, it is phred33 as our fastq files use the Sanger encoding like most Illumina data. Next the raw input forward (1) and then reverse (2) read files for quality control are specified. On the next line, the output files for read 1 are specified, first for paired reads and second for unpaired reads. This comes about when one read from a pair is filtered out but the other one is not. We can normally ignore this file after the trimming. However, trimmomatic must have this value to run. Next the output files for read 2 are specified, first paired then unpaired. ILLUMINACLIP: These settings are used to find and remove Illumina adapters. First, a fasta file of known adapter sequences is given, followed by the number of mismatches allowed between the adapter and read sequence and then thresholds for how accurate the alignment is between the adapter and read sequence. LEADING: The minimum quality value required to keep a base at the start of the read. TRAILING: The minimum quality value required to keep a base at the end of the read. SLIDINGWINDOW: This specifies to scan the read quality over a 4 bp window, cutting when the average quality drops below 30. MINLEN: This specifies the minimum length of a read to keep; any reads shorter than 50 bp are discarded. Once the command is run, it will give you some useful output to the screen. Based on it, approximately what percentage of paired reads were kept? 95 83 71 11 4.4.1 Post Quality control check To see how successful the quality control has been, we need to run fastQC on the Trimmomatic output. First create a new directory for the FastQC output mkdir trim_fastqc_output Now run FastQC on the two paired read files fastqc -o trim_fastqc_output \\ SRR8861574_1_out_paired.fastq SRR8861574_2_out_paired.fastq We can open the FastQC output with the following commands firefox \\ trim_fastqc_output/SRR8861574_1_out_paired_fastqc.html &amp; firefox \\ trim_fastqc_output/SRR8861574_2_out_paired_fastqc.html &amp; To see how well the reads have improved, let’s answer the questions below and compare these to the answers of the raw read FastQC questions. How many total reads are there in both the files? 350000 355071 500000 565000 What is the length of the reads? 35-76 50-76 75-100 75-150 In the read 2 file, at what base position does the quality of all the reads go below 28 (i.e. the position where not all of the box plot is in the green)? 72-73 74-75 76 It does not What is the average GC content (%) of the reads? 40 43 45 50 Some things to note: The amount of reads has significantly decreased due to quality control. This is expected, but will need to be taken into account. If you do not have enough reads for the downstream analysis you will need to be less stringent on the trimming or filtering. What is the minimum number of reads required for downstream analysis? Unfortunately, this depends on the specific analysis at hand. For genome assemblies, the general rule of thumb is you want at least 20 times as many bases in the reads as would be in the final assembly (based on the genome size of the organisms). For calling SNPs, you need less coverage. R2 quality is still slightly worse than R1 quality towards the end of the reads. Again, this is normal. You could be more stringent but as previously mentioned you may be removing too many reads. "],["05-Mapping.html", "Chapter 5 Mapping sequences to the reference genome using BWA 5.1 Reference sequence preparation 5.2 Read mapping 5.3 Assessing mapping results", " Chapter 5 Mapping sequences to the reference genome using BWA BWA is a program for mapping sequences to a reference genome. Here we will use BWA to map our paired, trimmed sequences to the killer whale reference genome (GCF_000331955.2_Oorc_1.1_genomic.fna), and output a BAM (Binary sequence Alignment/Map format) file of the results. Further information on the reference genome, such as the sex of the sequenced individual, the technology and assembly programs used, the number of scaffolds/contigs and the sequence coverage can be found on the NCBI website page for this assembly. 5.1 Reference sequence preparation BWA uses indexing of the reference genome to speed up the mapping. The index needs to be prepared before we map our reads. As this takes some time, this has been run for you, but for information the command used to do this was: bwa index GCF_000331955.2_Oorc_1.1_genomic.fna This produces index files with the extensions '.sa', '.pac', '.ann', '.amb' and '.bwt' that will be automatically detected and used in the mapping step below. 5.2 Read mapping To map our paired-end, trimmed reads type the following: bwa mem \\ /pub14/tea/nsc206/NEOF/popgenomics/genome/GCF_000331955.2_Oorc_1.1_genomic.fna \\ SRR8861574_1_out_paired.fastq SRR8861574_2_out_paired.fastq | \\ samtools sort -o SRR8861574_paired.bam bwa mem is an alignment algorithm well suited to Illumina-length sequences. The default output is a SAM (Sequence Alignment Map format). However, here we pipe the output to samtools, a program for writing, viewing and manipulating alignment files, to sort and generate a BAM format, a binary, compressed version of SAM format. The mapping will take a few minutes to run, during which some output messages will be printed to the screen. When it has finished you will see the command prompt $ and flashing cursor again. We can view this file using samtools view and head to display only the first few lines: samtools view -h SRR8861574_paired.bam | head The header section consists of lines starting with '@'. In this case, we have a header line for each of the scaffolds in the killer whale reference genome with their name (SN) and length in bp (LN) given. To look at the information in the alignment section, we can leave out the -h option: samtools view SRR8861574_paired.bam | head Here is an example of one line, representing information from one sequence read. Each column contains information on where and how well the read aligns to the reference. After column 11 there can be several optional tags, often specific to the aligner used and can be useful for downstream analysis. The image above is from a different alignment file to the one you have generated here. On the first line of your alignment file what is the query sequence name? What is the reference contig name? In order to process the BAM files for further analysis, it is usually required that they are indexed. This can also be done with samtools: samtools index SRR8861574_paired.bam Use ls to view the contents of your directory - you should now have an index file with the extensions .bam.bai. 5.3 Assessing mapping results Lastly, we can use the samtools command flagstat to find information on how the reads mapped: samtools flagstat SRR8861574_paired.bam How many reads have mapped? 710696 710142 705371 687024 What percentage of reads have mapped? 90.32 96.74 99.25 100 This information is a summary of the FLAG values - the second field in the sam/bam file. These values can be used if you want to extract or exclude a specific set of reads for downstream analysis. For example, to exclude unmapped reads from our alignment file we can run: samtools view -F 4 -b SRR8861574_paired.bam &gt; SRR8861574_paired_mapped.bam The options used are: -F: only include reads without this flag -b: output in bam format Conversely, you can use -f to include the reads with the specified flag. The Broad Institute has a useful website to interpret the FLAG values, likewise they are listed in the samtools documentation. Run flagstat on the bam file from which we have excluded the unmapped reads. How many reads are in the filtered file (compare this to the unfiltered file)? 710696 705371 704817 687024 What proportion of the reads in this file are now mapped (compare this to the unfiltered file)? 90.32 96.74 99.25 100 In the next step, we will use alignment files from several samples to detect genetic variation. "],["06-SNPs.html", "Chapter 6 SNP and genotype calling using SAMtools and BCFtools 6.1 File preparation 6.2 Variant calling 6.3 Variant filtering", " Chapter 6 SNP and genotype calling using SAMtools and BCFtools BCFtools is a very popular program to call SNPs and genotypes (and also to manipulate and filter vcf/bcf files, as we will see afterwards). SNP calling is a relatively intensive process, so to speed things up we will be restricting variant calling to one scaffold from the killer whale genome (NW_004438415.1). We will call SNPs for three samples (SRR8861574, SRR8861575 and SRR8861576). The alignments have been provided for you in '.bam' format - this is a binary, compressed version of a sam file. First navigate to the 'aligned' directory: cd ~/popgenomics/data/aligned/ Check that you have bam files for the three samples and their corresponding index files using ls. 6.1 File preparation Before we start SNP calling, we will copy over the genome assembly file and index it using samtools: cp /pub14/tea/nsc206/NEOF/popgenomics/genome/GCF_000331955.2_Oorc_1.1_genomic.fna . samtools faidx GCF_000331955.2_Oorc_1.1_genomic.fna This will produce a .fai index file. Also, we need to make a list of the BAM files before we start: ls *.bam &gt; bamFiles.txt We will use this list file to call SNPs from all three samples rather than having to specify each sample name in the command. 6.2 Variant calling We will use BCFtools mpileup to produce genotype likelihoods for the three samples/BAM files, and pipe this output to BCFtools call to generate a file of genetic variants. Here is the BCFtools command: bcftools mpileup -Ou \\ --max-depth 10000 -q 20 -Q 20 \\ -P ILLUMINA -a FORMAT/DP,FORMAT/AD \\ -f GCF_000331955.2_Oorc_1.1_genomic.fna \\ -r NW_004438415.1 \\ -b bamFiles.txt | \\ bcftools call -m -v -P 1e-6 -f GQ \\ -O b -o orca_3samp_1scaf.bcf The options we are using are: mpileup -Ou: ouput an uncompressed bam file. This is the option to use when piping the output to another command for optimum performance/speed. --max-depth 10000: the maximum number of sequences considered per position -q 20: filter out alignments with mapping quality &lt;20 -Q 20: filter out bases with QS &lt; 20 -P ILLUMINA: use Illumina platform for indels -a FORMAT/DP,FORMAT/AD: output depth and allelic depth -f: specify the genome reference file, which must be faidx-indexed -r: restrict mpileup output to this specific region -b: list of input bam alignment files call -m: use the multiallelic caller -v: output variants only -P 1e-6: prior for expected substitution rate -f GQ: output genotype quality -O b: output in BCF format This command generates a '.bcf' file, which is a binary version of a '.vcf'. One of the most popular text-file formats for storing genetic variation information is the Variant Call Format (VCF). There are different versions of the format, but the core elements are the same. You can find a full description of the latest iteration here. Due to the large amount of data usually involved, files tend to be stored compressed as a BCF file. BCF is designed to be much more efficient to process and store large amounts of data. The practical downside is that the contents can only be accessed with bcftools view. The VCF format is composed of meta-information lines (prefixed wih ##), a header line (prefixed with #), and data lines each containing information about a position in the genome and genotype information on samples for each position (text fields separated by tabs). There are 8 fixed field lines, usually followed by an additional FORMAT field and an arbitrary number of sample fields when genotypes are included. Description of VCF fields Some notes about genotype fields: GT: Allele values are 0 for the reference and 1+ for alternate alleles. Alleles are separated with / if unphased and with | if phased. There are as many alleles separated by / or | as the ploidy. A . is used for missing data, i.e. when a genotype could not be called. More information on phasing is provided in the Appendix. GL or PL: Genotype likelihoods as floating-point log10-scaled or phred-scaled format. A diploid biallelic site has three values: homozygote for reference, heterozygote, and homozygote for alternate. Triallelic sites have 6 values: homozygote for reference, heterozygote of reference and alternate allele 1, homozygote for alternate allele 1, heterozygote of reference and alternate allele 2, heterozygote of alternate alleles 1 and 2, and homozygote for alternate allele 2. A graphical explanation of the different parts and fields can be found here. Let's have a look at our BCF file: bcftools view orca_3samp_1scaf.bcf | less -S The file starts with a few lines specifying the VCF and bcftools versions, the command used with bcftools mpileup, followed by a long list of the &gt;1,000 scaffolds that comprise the genome assembly (starting with ##contig), the descriptions of the fields for ALT, INFO and FORMAT, and any additional commands that were executed to produce this file (bcftools call in this case). To focus only on the header or the SNPs, the flags -h and -H can be used: bcftools view -h orca_3samp_1scaf.bcf | less -S bcftools view -H orca_3samp_1scaf.bcf | less -S Are all of the variants SNPs or are there also INDELs in the file? Only SNPs A good mix of SNPs and indels A few indels Are the variant quality scores (QUAL field) all high (&gt; 20)? Yes No Information in the BCF file can be used to filter to retain, for example, only biallelic SNPs, or to filter out variants with a low quality score or read depth. A list of the samples contained in the file can be obtained using bcftools query, and can be counted with wc: bcftools query -l orca_3samp_1scaf.bcf bcftools query -l orca_3samp_1scaf.bcf | wc -l Lastly, let's index the bcf file in order to be able to process it below: bcftools index orca_3samp_1scaf.bcf 6.3 Variant filtering The number of variants can be counted using bcftools view -H and wc, but it includes all kinds of variants. To check particular ones, we need to use the flags -v/-V (or --types/--exclude-types) to include or exclude certain variant: # all variants bcftools view -H orca_3samp_1scaf.bcf | wc -l # only SNPs bcftools view -H -v snps orca_3samp_1scaf.bcf | wc -l # only indels bcftools view -H -v indels orca_3samp_1scaf.bcf | wc -l Biallelic SNPs (with just 1 alternate allele) can be extracted with the -m and -M flags: bcftools view -H -v snps -m 2 -M 2 orca_3samp_1scaf.bcf | less -S -m 2 is the bcftools command that means “-m, –min-alleles INT: print sites with at least INT alleles listed in REF and ALT columns”. So here we are asking for all lines where there at least 2 REF and ALT alleles in total. -M 2 is the bcftools command that means “-M, –max-alleles INT: print sites with at most INT alleles listed in REF and ALT columns”. So here we are asking for all lines where there at most 2 REF and ALT alleles in total. 6.3.1 Extracting information Partial information can be extracted using bcftools query. In the example above, we saw how to get the list of samples using the -l option, but it can also be used to extract any fields using the -f option. For example, you can simply extract the list of positions with: bcftools query -f &#39;%POS\\n&#39; orca_3samp_1scaf.bcf | less -S Or combine multiple fields in the output, for example, scaffold/chromosome, position, reference and alternate alleles: bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT\\n&#39; orca_3samp_1scaf.bcf | less -S You can also extract genotypes (notice the position of the tab to avoid adding an extra tab at the end of the lines): bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT[\\t%GT]\\n&#39; \\ orca_3samp_1scaf.bcf | less -S Or genotype likelihoods: bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT[\\t%PL]\\n&#39; \\ orca_3samp_1scaf.bcf | less -S You can even combine this with awk, Perl or other tools to do simple calculations. For example, you can calculate allele frequencies from the AC and AN counts: bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT\\t%AC\\t%AN\\n&#39; \\ orca_3samp_1scaf.bcf | less -S bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT\\t%AC\\t%AN\\n&#39; \\ orca_3samp_1scaf.bcf | awk &#39;{print $1,$2,$3,$4,$5,$6,$5/$6}&#39; | less -S Or the mean genotype quality: # For all genotypes (uncalled ones = 0) bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT[\\t%GQ]\\n&#39; \\ orca_3samp_1scaf.bcf | \\ awk &#39;{SUM=0; N=0; for(i=5; i&lt;=NF; i++){N+=1; SUM+=$i}; \\ AVG=SUM/N; print $1,$2,$3,$4,AVG}&#39; | \\ less -S # Only for called genotypes: bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT[\\t%GQ]\\n&#39; \\ orca_3samp_1scaf.bcf | \\ awk &#39;{SUM=0; N=0; for(i=5; i&lt;=NF; i++){SUM+=$i; if ($i&gt;0) N+=1}; \\ AVG=SUM/N; print $1,$2,$3,$4,AVG}&#39; | less -S 6.3.2 Subsetting files We can subset SNPs within particular regions. For example, we can extract all the SNPs within a region on a particular chromosome/scaffold: bcftools view -H orca_3samp_1scaf.bcf NW_004438415.1:50000-100000 | wc -l bcftools view -H orca_3samp_1scaf.bcf NW_004438415.1:50000-100000 | less -S Subsetting a number of samples is also possible, for example to get only the first 2 samples: SAMPLES=$(bcftools query -l orca_3samp_1scaf.bcf | head -n 2 | tr &#39;\\n/&#39; &#39;,&#39;| sed &#39;s/,$//&#39;) bcftools view -s $SAMPLES orca_3samp_1scaf.bcf -O b -o orca_3samp_1scaf_first2.bcf bcftools index orca_3samp_1scaf_first2.bcf 6.3.3 Filtering Usually, we want to apply some filters depending on the sort of downstream analyses that we intend to do. Filtering usually consists of establishing some sort of threshold for the SNP quality in the QUAL field or some of the variables encoded in the INFO field (e.g. depth, mapping quality, etc.), but more sophisticated filtering is possible based on genotype-specific (i.e. per-sample) variables (e.g. discarding genotypes below a certain quality or with too few reads). Although it is possible to do soft-filtering of SNPs and simply tag them as PASSED or FAILED in the VCF/BCF file, it is more useful in practice to do hard-filtering and simply remove all those SNPs (and genotypes) that did not meet our thresholds. We are going to focus on the SNPs only, so let’s extract only the SNPs from all the variants: bcftools view -v snps orca_3samp_1scaf.bcf -O b &gt; orca_3samp_1scaf_snps.bcf bcftools index orca_3samp_1scaf_snps.bcf bcftools view -H orca_3samp_1scaf_snps.bcf | wc -l bcftools allows applying filters on many of its commands, but usually they are used with bcftools view or with bcftools filter. Filtering can be done using information encoded in the QUAL or INFO fields, also allowing expression with multiple conditions and basic arithmetics more details here. Here are some examples: SNP quality An obvious filter is to exclude (-e) SNPs below a quality threshold: bcftools view -e &#39;QUAL&lt;20&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.QS20.bcf Filters can also be specified as include (-i), the equivalent of the one above is: bcftools view -i &#39;QUAL&gt;=20&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.QS20i.bcf You can see they have the same number of SNPs: bcftools view -H orca_3samp_1scaf_snps.QS20.bcf | wc -l bcftools view -H orca_3samp_1scaf_snps.QS20i.bcf | wc -l SNP depth An example to exclude SNPs with depth &lt;10: bcftools view -e &#39;INFO/DP&lt;10&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.DP10.bcf # You can see this is quite a stringent filter for this subset of data: bcftools view -H orca_3samp_1scaf_snps.DP10.bcf | wc -l How many of the 267 SNPs have a sequencing depth of 10 or more reads? 2 3 8 29 An important note here is to be aware that if the file is further processed so that only part of the individuals are used, the fields in INFO may not be updated and it would be then unreliable to filter out using any information from that field. Mapping quality Another typical filter is to remove SNPs with low mapping quality (RMS MQ: root mean square of the mapping quality of reads across all samples). For examples, all SNPs with RMS MQ &lt; 30 can be discarded with the following command: bcftools view -e &#39;MQ&lt;30&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.MQ30.bcf How many SNPs has this removed? bcftools view -H orca_3samp_1scaf_snps.bcf | wc -l bcftools view -H orca_3samp_1scaf_snps.MQ30.bcf | wc -l All but one of the SNPs had MQ&gt;30, and most have MQ=60: bcftools query -f &#39;%MQ\\n&#39; orca_3samp_1scaf_snps.bcf | sort | uniq -c Allele frequency For population genetic analyses, it is frequently advised to remove variants below a certain allele frequency, as these ones are difficult to tell apart from sequencing errors. For example, to exclude all SNPs with a minor allele frequency (MAF) below 5% we would run: bcftools view -e &#39;MAF&lt;0.05&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.MAF005.bcf How many SNPs have a minor allele frequency less than 5% (i.e. 0.05)? 8 29 257 259 Sample coverage Another typical situation is to want to exclude all SNPs for which only a small fracion of all the individuals have sequence data. That requires using the total number of alleles in the called genotypes (AN). In the case of a diploid species, this can be used to filter SNPs by number of individuals genotyped. For example, we can remove all SNPs genotyped for less than 3 individuals: bcftools view -e &#39;AN/2&lt;3&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.SAMP3.bcf How many SNPs have been called and genotyped all three of the samples/individuals? 3 8 10 29 Genotype-based filtering Filtering using the genotype fields can allow for some more precise filtering. For example, in cases of high depth heterogeneity among samples, it may be better to filter out by median genotype depth than by total depth across all samples. An example removing all SNPs where the mean genotype depth is below 3: bcftools view -e &#39;AVG(FMT/DP)&lt;3&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.MEANGTDP3.bcf How many SNPs have a mean genotyping depth of 3 or more? 3 8 10 29 Sometimes it is reasonable to ignore genotype calls based on few reads. The following command remove all genotype calls (i.e. genotypes are substituted by ./.) informed by less than 5 reads: bcftools filter -S . -e &#39;FMT/DP&lt;5&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.NOGTDP5.bcf Combining multiple filters Multiple filters can be combined in a single command using or piping several ones. For example, we can combine a few of the filters we have used above: bcftools filter -S . -e &#39;FMT/DP&lt;3&#39; orca_3samp_1scaf_snps.bcf | \\ bcftools view -e &#39;AVG(FMT/DP)&lt;3 || QUAL&lt;20 || AN/2&lt;2&#39; -O b &gt; \\ orca_3samp_1scaf_snps.NOGTDP3.MEANGTDP3.Q20.SAMP2.bcf # This results in quite a dramatic reduction in the number of SNPs: bcftools view -H orca_3samp_1scaf_snps.NOGTDP3.MEANGTDP3.Q20.SAMP2.bcf | wc -l Final note: It is important to be careful with the order of the filters, as different combinations can result in different end results, especially if we subsample the individuals at some point. Also, some positions may end up being invariant or only variant with respect to the reference (i.e. private), or not having any individuals genotyped. We have also worked on a small bcf file with these examples - just one scaffold for three individuals. With larger amounts of data you will likely want to change some of the threshold values from those used here. "],["07-NGSAdmix.html", "Chapter 7 Population structure with NGSadmix and PCA 7.1 NGSadmix 7.2 Jupyter 7.3 Admixture plots 7.4 PCA", " Chapter 7 Population structure with NGSadmix and PCA 7.1 NGSadmix NGSadmix is a program (part of the package ANGSD) for inferring admixture from genotype likelihoods, thus taking into account the uncertainty inherent to NGS data. It is similar to the famous program STRUCTURE, but better suited for low-coverage NGS data. For those not familiar with these kind of approaches, these programs allow investigating population structure by inferring individual admixture proportions given a number of populations (K). This allows us to assign individuals to populations and identify migrants or admixed individuals. For this step, we will use an already prepared BCF file containing filtered genome-wide variants for 16 individual whales. First navigate to the 'structure' directory: cd ~/popgenomics/data/structure/ Check that you have the bcf file plus a text file of sample location information using ls. We then need to convert the bcf file to beagle format with angsd: mkdir ngsadmix angsd -vcf-pl \\ orca16_filtered.bcf \\ -fai ../aligned/GCF_000331955.2_Oorc_1.1_genomic.fna.fai \\ -doMaf 3 -nInd 16 \\ -domajorminor 1 -doglf 2 \\ -out ngsadmix/snps Now we need to run NGSadmix for K=2 to K=4. We will first run K=2: NGSadmix \\ -likes ngsadmix/snps.beagle.gz \\ -K 2 \\ -o ngsadmix/snps_K2 Repeat for K=3 and K=4. We can now look at the '.qopt' files that contain the admixture proportions: head ngsadmix/snps_K2.qopt You should get something that looks like this, which represents the admixture proportions for each of the K populations (here K=2) for the first 10 samples: 0.02279066532333781994 0.97720933467666215577 0.17946055065158281194 0.82053944934841716030 0.00000902492365512320 0.99999097507634482351 0.00000000100000000000 0.99999999900000002828 0.00000001699039481555 0.99999998300960513120 0.99999999900000002828 0.00000000100000000000 0.99999999900000002828 0.00000000100000000000 0.99999999900000002828 0.00000000100000000000 0.99999999900000002828 0.00000000100000000000 0.03125527561634464102 0.96874472438365533122 Now we are going to visualize the admixture estimates using 'R'. We will need some more information about the samples to produce meaningful plots. A file with information about the sampling location of each individual can be found in your \"structure\" directory. Both the population and ocean each sample is from is listed. head sample_location.tsv We will be using Jupyter-notebook to run our R code and generate some plots for the admixture inferences when K=2. 7.2 Jupyter Jupyter-notebook is a nice browser-based method to write, edit, and run code. It was initally created for Python coding, but has since branched out to many other languages, such as R. We are using it in this workshop for a variety of its properties: It is popular and well maintained. It is lightweight. Other heavier weight programs, such as RStudio, would struggle in our HPC due to the graphical and CPU load. It is interactive and displays code output. It allows for easier annotation, editing, and debugging than the command line. It provides a graphical interface for changing directories and choosing files. Before carrying out any analysis, we will go through a quick tutorial of jupyter-notebook. 7.2.1 Open Jupyter-notebook The first step is to open jupyter-notebook. Run the below command in your (popgenomics) environment. jupyter-notebook This will open jupyter-notebook in firefox. We won't need to access the linux terminal anymore. Leave the terminal running jupyter-notebook and full screen your firefox. You should see something like below. 7.2.2 Create R notebook The next step is to create a R notebook. Click on the \"New\" button towards the top right, right of the \"Upload\" button. From the dropdown click \"R\" below \"Python 3 (ipykernel)\". This will open up a new R notebook like below. 7.2.3 Cells and code Jupyter-notebook uses cells (the gray boxes) to separate code. This is very useful to compartmentalise our code. There will already be one cell. Within the cell, type in the below commands. 1+1 2-3 When pressing enter in cells it will create a new line. To run all commands in a cell press CTRL + enter. Run your current cell and you should see something like below. 7.2.4 Create new cells You can create new cells by 2 different means. Press the + button on the tool bar (between the floppy disk and scissors ). This will add a cell below your currently selected cell. Click on the Insert button and use the dropdown to add a cell above or below your currently selected cell. Tip: Hover over the toolbar icons to display a text based description of its function. With that knowledge add a second cell below the first cell. Add the following code to your second cell but do not run it. num_1 &lt;- 3 num_2 &lt;- 10 Tip: Notice there are green lines around your selected cell. Insert a third cell and add the following code to it. Do not run the code. num_1 * num_2 7.2.5 Running code Try to run the code in the third cell. There should be an error as we have not created the objects num_1 &amp; num_2. We have only written the code for these objects but not run them. We can run all the code in a notebook starting from the first cell to the last cell. To run all cells from the start: Click on the \"Cell\" button. Click \"Run All\" from the drop-down options. You should then see something like the below in your notebook. There is no output printed for cell 2 because we are assigning variables. However, the correct output for Cell 3 is below it. This is because the variables were assigned in cell 2 before cell 3 was run. 7.2.6 Saving the file As with RStudio and other good coding interfaces, we can save our notebook. First, we should rename the file. Rename the notebook to \"jupyter_tut\": Click on the name of the notebook, currently called \"Untitled\". This is at the very top of the notebook, right of the Jupyter logo. A pop-up called \"Rename Notebook\" will appear. Change the Name to \"jupyter_tut\". Click \"Rename\". Now we can save the file. Two methods to save are: Click the floppy disk on the toolbar. Click on the \"File\" button. Click \"Save and Checkpoint\" from the dropdown options. 7.2.7 Title cells with markdown We will be using multiple notebooks in this workshop. We will also have multiple sections per notebook. It will be useful to create header cells with markdown to create visual separation of the different sections. To add a header cell to the top of our notebook: Create a new cell at the top of the notebook. Click on the \"Code\" drop down and select \"Markdown\". The \"Heading\" option no longer works. Add the following to the \"Markdown\" cell to create a first level header. Ensure you have a space between the # and header text (\"Tutorial\"). # Tutorial Great, we can now add nice headers in our notebooks. Save the notebook once more before carrying on to the next section. Markdown You won't need to know more about Markdown but if you are interested please see the Markdown guide. 7.2.8 Close the notebook To close the notebook: Click on \"File\". From the dropdown options click \"Close and Halt\". When you are back in the file explorer page, you may not yet see the new file you saved. If so, you will need to refresh the page with the Refresh button towards the top right. With that quick tutorial of jupyter-notebook, we can start visualizing our admixture plots. 7.2.9 Video tutorial 7.3 Admixture plots As mentioned above, we will be using R to generate some plots for the admixture inferences when K=2 within Jupyter Notebooks. Create a new notebook called \"Chp07a-Admixture\". - Add a markdown cell with the first level header: # Admixture analysis Now let’s load the admixture proportions, the information about the individuals, and let’s do some plotting. Remember this is typed into a code cell and then run. Please create your own coding and markdown cells where you think appropriate. # Load admixture proportions admix&lt;-t(as.matrix(read.table(&quot;ngsadmix/snps_K2.qopt&quot;))) # Load info about samples id.info&lt;-read.table(&quot;sample_location.tsv&quot;, sep=&quot;\\t&quot;, header=T) # Palette for plotting mypal&lt;-c(&quot;#E41A1C&quot;,&quot;#377EB8&quot;,&quot;#4DAF4A&quot;,&quot;#984EA3&quot;) # sort by sampling area and plot admix&lt;-admix[,order(id.info$area)] id.info&lt;-id.info[order(id.info$area),] barplot(admix,col=mypal,space=0,border=NA,xlab=&quot;Samples&quot;,ylab=&quot;admixture&quot;) text(tapply(1:nrow(id.info),id.info$area,min),-0.05,unique(id.info$area),xpd=T, srt=-45, adj=0.2) This generates a plot for K=2 which should look something like this: Do individuals appear to be genetically structured in relation to the ocean they are sampled in? Plot the admixture proportions for K=3. Can you infer anything further than with the K=2 plot? There appears to be some similarities between, for example, Australasian and Pacific samples. However, without knowing the exact sampling locations, as you hopefully would for your own study!, it is hard to interpret these plots robustly, but hopefully this code will be useful for making your own structure plots from your SNP datasets. Important: You may notice the results vary across runs. This happens when the maximum-likelihood searches gets stuck in different local optima. In production runs, it is recommended to run multiple searches for each K and check the likelihoods. 7.4 PCA Next we are going to carry out PCA using R. For that, we have converted our BCF file to mean genotype format (based on the BIMBAM format), where genotypes are encoded as mean genotypes. A mean genotype is a value between 0 and 2 that can be interpreted as the minor allele dosage: 0 is homozygous for the major allele, 1 is a heterozygote, and 2 is a homozygote for the minor allele. Although genotype calls can be used, it is also possible to use genotype likelihoods to get intermediate values that incorporate uncertainty in genotype calling. We have already done this for you for the purposes of the course. First navigate to the 'pca' directory within Jupyter. We will use R within Jupyter to perform the PCA with the mean genotypes. Create a new Jupyter notebook called \"Chp07b-PCA\". - Add a markdown cell with the first level header: # PCA analysis First, let’s load the genotypes in a new markdown cell: ## load genotypes genotypes&lt;-read.table(&quot;orca16_filtered.bbgeno&quot;,header=T, check.names=F) head(genotypes) We can now do a the PCA with the genotype matrix, excluding the first 3 columns that contain information for SNP id, reference allele and alternate allele: pca.genotypes&lt;-prcomp(t(genotypes[,-(1:3)]), center=TRUE, scale=FALSE) Then we can have a look at the proportion of variance explained by each PC: summary(pca.genotypes) As you can see, ~30% of the variance is explained by the first 4 PCs. We will extract the PCs vectors and plot them by pairs (i.e. PC1xPC2, PC2xPC3, PC3xPC4): # Get PCs pcs&lt;-pca.genotypes$x # Load info about samples id.info&lt;-read.table(&quot;sample_location.tsv&quot;, sep=&quot;\\t&quot;, header=T) # sort as in the pcs id.info&lt;-id.info[match(rownames(pcs), id.info$run_accession),] # colours id.colours&lt;-as.character(id.info$area) id.colours[id.colours==&quot;Atlantic_Ocean&quot;]&lt;-&quot;red&quot; # Atlantic as red id.colours[id.colours==&quot;Australasia&quot;]&lt;-&quot;blue&quot; # Australasia as blue id.colours[id.colours==&quot;Indian_Ocean&quot;]&lt;-&quot;green&quot; # Indian as green id.colours[id.colours==&quot;Pacific_Ocean&quot;]&lt;-&quot;orange&quot; # Pacific as orange id.colours[id.colours==&quot;Southern_Ocean&quot;]&lt;-&quot;purple&quot; # Southern as purple plot(pcs[,1], pcs[,2], main = &quot;PCA using genotype matrix&quot;, xlab = &quot;PC1&quot;, ylab = &quot;PC2&quot;, col=id.colours, pch=16) plot(pcs[,2], pcs[,3], main = &quot;PCA using genotype matrix&quot;, xlab = &quot;PC2&quot;, ylab = &quot;PC3&quot;, col=id.colours, pch=16) plot(pcs[,3], pcs[,4], main = &quot;PCA using genotype matrix&quot;, xlab = &quot;PC3&quot;, ylab = &quot;PC4&quot;, col=id.colours, pch=16) The first plot should look like this: There is some structure evident: PC1 separates out the Australasian and Pacific Ocean samples, the Atlantic Ocean samples are split, and the Indian Ocean sample does not cluster closely with any other. In reality, you would want more specific information about the sampling locations, and ideally more than 16 samples! However, hopefully this code will be useful in generating your own PCA plots. "],["08-Summary_stats.html", "Chapter 8 Calculating population summary statistics using PopGenome 8.1 Getting started", " Chapter 8 Calculating population summary statistics using PopGenome Generating summary statistics of your dataset can be useful to help you get a feel for your data and complement the visual structure plots and PCAs created in the previous chapter. We will now generate a summary of nucleotide diversity and FST using one scaffold (to save time) from the vcf file we have generated using the package PopGenome in R. PopGenome is a powerful and flexible R package that can be used to calculate a diverse range of population genetics statistics. We will use only a couple of very basic features and analyses here, but we strongly recommend reading the paper and manual for greater understanding of its capabilities. Advantages of PopGenome include its abilty to: Read data in a variety of input formats Implement a comprehensive range of population genetics/genomics analyses and statistics Read associated annotation files and allow to systematically select regions of interest Analyse individual loci, multiple loci, and sliding windows Open source and be easily extendable by the scientific community to incorporate new types of analyses Integrated with powerful numerical and graphical capabilities Platform independent We will now generate a couple of commonly used population genomic statistics, FST and nucleutide diversity (π). FST F statistics are one of the of the most common methods for quantifying genetic differentiation between subpopulations. FST (fixation index) estimates genetic differentiation between subpopulations. It reflects the degree of inbreeding within a subpopulation relative to the total population, as the probability that two alleles drawn at random from a subpopulation are identical by descent. FST is bound between 0 and 1. If FST = 0, there is no genetic differentiation between populations. If FST = 1, the two populations are very strongly separated. Equation: \\[ F _{ST} = \\frac{(H_T - H_S)} {H_S} \\] HT expected heterozygosity of the total population HS expected heterozygosity if the subpopulation were in Hardy–Weinberg Equilibrium π Nucleotide diversity (π) quantifies the mean divergence between sequences and reflects the extent to which sequences differ from each other. π is the average pairwise difference between all possible pairs of individuals in your sample. Equation: \\[ \\pi = \\sum f_i f_jp_{ij} \\] fi and fi frequencies of the _i_th and _j_th haplotypes in the population pij sequence divergence between haplotypes 8.1 Getting started First navigate to the 'pca' directory in a terminal window session (not in Jupyter) and then launch jupyter-notebook. Create a new notebook called \"Chp08-PopGenome\". Create a new markdown cell titled # Summary Statistics using PopGenome Load the PopGenome package in a new code cell. library(PopGenome) We will now load in our SNP data, PopGenome can handle vcf files. # Load the data GENOME.class &lt;- readData(&quot;/pub14/tea/nsc206/NEOF/popgenomics/data/popgenome/&quot;, format=&quot;VCF&quot;) This makes a complex object which we can use to extract useful data. First we can get a summary of the vcf we have imported. # This is complex object, with several slots get.sum.data(GENOME.class) n.sites - this is the position of the last SNP in the vcf file - in this case 17,452,779 n.biallelic.sites - Number of biallelic sites (SNPs) - there are 7,511 biallelic SNPs in this scaffold file - note SNPs are only counted if they have a 'PASS' description in the filter column (you can get the full number of SNPs in the vcf by typing GENOME.class@n.sites2) n.gaps - Number of gaps observed in the data - 0 n.unknowns - Number of unknown positions - 778 this corresponds to the number of SNPs without the 'PASS' description in the filter column n.valid.sites - Number of sites without gaps (number reported as NA for vcf files) n.polyallelic.sites - Number of sites with more than two variants - 0 trans.transv.ratio - Transition (changes from A ↔︎ G and C ↔︎ T) - transversion (changes from A ↔︎ C, A ↔︎T, G ↔︎ C or G ↔︎ T) ratio The object GENOME.class contains all the information of the different data that you can import and assign to the vcf and the statistics that can be run. To get a summary, type: show.slots(GENOME.class) These slots can be accessed by using the \"@\" sign. For example the number of sites: GENOME.class@n.sites For more information on the slots available using PopGenome please refer to the manual We can now assign population information to the object. This is needed to calculate FST and π. For now we will specify just two of the populations. # Make a list of individuals for each population # Note: the sample names in the vcf contain the path and the .bam file extension # Here we are just making a list of sample names as they appear in the vcf - # we are not importing any additional files pop1&lt;-c(&quot;./aligned/SRR8861576_paired.bam&quot;,&quot;./aligned/SRR8861581_paired.bam&quot;, &quot;./aligned/SRR8861586_paired.bam&quot;,&quot;/aligned/SRR8861592_paired.bam&quot;) pop2&lt;-c(&quot;./aligned/SRR8861578_paired.bam&quot;,&quot;./aligned/SRR8861583_paired.bam&quot;, &quot;./aligned/SRR8861589_paired.bam&quot;,&quot;./aligned/SRR8861591_paired.bam&quot;) # Set populations GENOME.class &lt;- set.populations(GENOME.class, list(pop1,pop2)) # check if setting the populations worked GENOME.class@populations # You can see that the sample names have been assigned to #two different within @populations Since the calculation of certain population genetic parameters can be computationally intensive, they have to be executed separately beforehand. For this, modules have to be run. To calculate FST and π we can use the F_st.stats module. # Perform the calculations and add the results to the appropriate slots GENOME.class &lt;- F_ST.stats(GENOME.class) # Print FST GENOME.class@nucleotide.F_ST # Fst is very low between these two populations # Print nucleotide diversity along the scaffold and average nucleotide diversity per site GENOME.class@Pi GENOME.class@Pi/GENOME.class@n.sites2 This can give you a quick feel for your dataset but in reality diversity will vary throughout the genome. It is often more appropriate and interesting to examine how different regions of the genome have diverged between populations. We will explore one method for estimating differentiation between a pair of populations in the next chapter. "],["09-Detecting_selection.html", "Chapter 9 Delimitation of contiguous regions of differentiation using Hidden Markov Models (HMM) 9.1 Initial set up 9.2 Data formatting 9.3 Allele frequency estimation 9.4 FST estimation 9.5 Delimitation of contiguous regions of differentiation using a HMM model", " Chapter 9 Delimitation of contiguous regions of differentiation using Hidden Markov Models (HMM) The aim of this chapter is to estimate differentiation (i.e. FST) between a pair of populations and identify contiguous regions of accentuated differentiation across the genome using a Hidden Markov Model (HMM) approach. We are going to switch datasets and use whole genome data of a pair of parapatric populations of Timema cristinae stick insects that live on different host plants and experience different selective pressures. The HVA population was sampled from Adenostoma, where insects with a white strip are favoured, whereas the HVC population was sampled from Ceanothus, where green insects have an advantage. This data belongs to larger dataset of four population pairs used in Soria-Carrasco et al. 2014. In that work, an approach similar to that of Hofer et al. 2012 was followed to investigate parallel evolution by identifying unique and shared regions of accentuated differentiation in four pairs of natural populations. Resources Bhatia et al. 2013 - Excellent paper about FST estimation and interpretation. Hofer et al. 2012 - First study where HMMs were used to delimit regions of differentiation. Wolf &amp; Ellegren 2017 - Review about what different evolutionary processes can generate genomics islands of elevated differentiation. HiddenMarkov - R package to fit Hidden Markov Models 9.1 Initial set up First you will need to move into the folder for this practical. cd ~/popgenomics/data/fst_hmm 9.2 Data formatting Now let’s have a look at the data. You should have the following input files: ls -lh data There is a vcf file containing single nucleotide polymorphisms (SNPs) from whole genome sequences of 20 individuals for each population (HVA and HVC). vcf is a very popular format for genetic variants, you can find more info here. You can have a look at the file content with the following commands: zless -S data/timemaHVA.vcf.gz # or with bcftools bcftools view data/timemaHVA.vcf.gz | less -S # excluding long header bcftools view -H data/timemaHVA.vcf.gz | less -S They need to be converted to the stripped down format (gl, genotype likelihood). Genotype likelihoods take into account base quality scores and other error rates (e.g. mapping errors). Genotype likelihoods are required for estpEM, the program we are going to use for allele frequency estimation later. This is done with a custom Perl script called bcf2gl.pl, which has a very simple syntax. For example, if we were to convert the file for the HVA population, the command that you would run is: perl scripts/bcf2gl.pl -i data/timemaHVA.vcf.gz -o timemaHVA.gl To speed things up by processing the files in the cluster in parallel in different nodes, you can use SGE array job that calls the Perl script for each file. Let’s have a look at the bcf2gl.sh script. less scripts/bcf2gl.sh Do not run the following command as it will take a long time to run. If you were to run the script you could type: scripts/bcf2gl.sh As this will take a while to run, we have provided the output files in the data folder. Output .gl files should look like this: less -S data/timemaHVA.gl The first line shows the number of samples and the number of SNPs, the second line show the samples IDs, and the third line specifies what samples will be used for analyses. The lines below represent a SNP per line. The first field is the ID (scaffold:position in this case) and it is followed by the phred scaled genotype likelihoods for the three possible genotypes (all variants are biallelic) for each individual. Therefore there are 3 x number of samples fields genotype likelihoods on each line. 9.3 Allele frequency estimation We will infer allele frequencies from genotype likelihoods by maximum likelihood, using an implementation of the iterative soft expectation-maximization algorithm (EM) described in Li 2011 and also used in bcftools. This algorithm has been implemented in the program estpEM, with code kindly provided by Zach Gompert, Utah State University. As before, this script is designed to use SGE array to parallelize jobs. In this case it will run estpEM to estimate allele frequencies from genotype likelihoods. You can have a look at the script: less scripts/alfreq.sh Again we will not run this now as it would take too long but if you were to run it on your own data you could do so like this: scripts/alfreq.sh You can check an example output file. It should look something like this: less -S results/timemaHVA.alfreq.txt The columns are: locus, allele frequency from counts, and allele frequency inferred by maximum likelihood. Now we are going to merge the results from both populations into a single file: # create header echo -e &quot;locus\\tafHVA\\tafHVC&quot; &gt; timemaHVAxHVC.alfreq.txt # join files by locus (1st field), retain only locus id (1st field) # and ML AFs (3rd and 5th fields) join -j 1 results/timemaHVA.alfreq.txt results/timemaHVC.alfreq.txt | \\ awk &#39;{OFS=&quot;\\t&quot;; print $1,$3,$5}&#39; &gt;&gt; timemaHVAxHVC.alfreq.txt # show file less -S timemaHVAxHVC.alfreq.txt 9.4 FST estimation We are going to calculate genetic differentiation between two populations and for a large number of variants using FST. We are going to use the FST Hudson’s estimator for every SNP: Equation: \\[ F ^{Hudson}_{ST} = 1-\\frac{Hw} {Hb} = \\frac{p_1(1-p_1)+p_2(1-p_2)}{p_1(1-p_2)+p_2(1-p_1)} \\] Hw is the within-population heterozygosity Hb is the between-population heterozygosity p1 and p2 are the allele frequencies in each population We are going to use the code provided in the script fst.R for FST estimation. This script can be run in batch mode like executing: Rscript scripts/fst.R, but you are likely to learn much more if you follow the steps below to run it step by step. Be aware that this is R code. First we change the working directory and then we load a library that will greatly speed up the loading of bit files and a library containing some functions required for calculations in the next section. To do this we must first launch an R session (not in Jupyter) by typing R. We are now working within an R session so we must now use R code: # change the working directory setwd(&quot;~/popgenomics/data/fst_hmm/&quot;) # load library to speed up loading of big tables library(data.table) # install a library of functions for population genetics calculations install.packages(&quot;/pub14/tea/nsc206/NEOF/popgenomics/popgenfunctions/&quot;, repos=NULL) library(popgenfunctions) You will likely get a message like the one below if you are installing the library for the first time. Type y and press enter. You will likely then see this message: Type y again and press enter. Lastly, when this has installed load the package: # load a library of functions for population genetics calculations library(popgenfunctions) If this is successful we can now exit the R session by typing q() and then n when prompted. You should now be back in your terminal session. Launch jupyter-notebook and create a new notebook called \"Chp09a-FST-estimation\". Type the following into a new code cell: # change the working directory setwd(&quot;~/popgenomics/data/fst_hmm/&quot;) # load library to speed up loading of big tables library(data.table) # load a library of functions for population genetics calculations library(popgenfunctions) Then we load allele frequencies: # load file with allele frequencies pops.af&lt;-fread(&quot;timemaHVAxHVC.alfreq.txt&quot;, header=T, sep=&quot;\\t&quot;) # check table head(pops.af) And now calculate the FSTs for every SNP: # Calculate Fst for all loci fst.HVAxHVC&lt;-hudsonFst(locus=pops.af$locus, p1=pops.af$afHVA,p2=pops.af$afHVC) # check results head(fst.HVAxHVC) Write results to file: # Write table to file write.table(fst.HVAxHVC, file=&quot;timemaHVAxHVC.fst.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) And plot the FST histogram: # plot FST histogram hist(fst.HVAxHVC$fst, breaks=100) Now genome-wide FST statistics are calculated as “ratio of averages” (i.e. averaging the variance components - numerator and denominator - separately), as suggested in Bhatia et al. 2013 # Genome-wide FST statistics # ------------------------------------------------------------------------------ # Calculate genome-wide FST as &quot;ratio of averages&quot; following Bhatia et al. 2013 nloci&lt;-length(na.exclude(fst.HVAxHVC$fst)) fst.mean&lt;-1-((sum(fst.HVAxHVC$numerator)/nloci)/(sum(fst.HVAxHVC$denominator)/nloci)) fst.mean You can see “average of ratios” (i.e. mean of FSTs) produces a noticeable different (underestimated) value: # &quot;average of ratios&quot; yields a noticeable lower value mean(fst.HVAxHVC$fst,na.rm=T) Calculate median, upper quantiles (95%, 99%, 99.9%, 99.99%), and max: # median and upper quantiles for all loci fst.quantile&lt;-quantile(fst.HVAxHVC$fst, prob=c(0.5,0.95,0.99,0.999,0.9999), na.rm=T) fst.quantile # max fst.max&lt;-max(fst.HVAxHVC$fst,na.rm=T) fst.max # put all together in a vector fst.genome&lt;-c(&quot;genome&quot;,nloci,fst.mean,fst.quantile,fst.max) # ------------------------------------------------------------------------------ Calculate FST separately for each major linkage group (~chromosomes): # FST stats by linkage group # ------------------------------------------------------------------------------ # get linkage groups from loci names lgs&lt;-unique(gsub(&quot;_ord.*&quot;,&quot;&quot;, fst.HVAxHVC$locus)) # calculate fst for each linkage group fst.stats.lgs&lt;-lapply(lgs, fst.lg) # put them together in a data frame fst.stats.lgs&lt;-data.frame(lg=lgs,do.call(&quot;rbind&quot;,fst.stats.lgs),stringsAsFactors=F) # ------------------------------------------------------------------------------ Put together FST estimates for whole genome and linkage groups and write to a text file: # summarize results # ------------------------------------------------------------------------------ # put together whole genome and linkage group estimates fst.stats&lt;-rbind(fst.genome,fst.stats.lgs) # put names of columns colnames(fst.stats)&lt;-c(&quot;lg&quot;,&quot;nloci&quot;,&quot;mean&quot;,&quot;median&quot;, &quot;q95%&quot;,&quot;q99%&quot;,&quot;q99.9%&quot;,&quot;q99.99%&quot;,&quot;max&quot;) # and check fst.stats # Write summary table to file write.table(fst.stats,file=&quot;timemaHVAxHVC.lgs.fst.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) # ---------------------------------------------------------------------------- 9.5 Delimitation of contiguous regions of differentiation using a HMM model We will use a 3-state discrete homogeneous Hidden Markov Model (HMM) to delimit contiguous regions of genetic differentiation, following the approach of Hofer et al. 2012. We will use the package HiddenMarkov to classify the genome in regions of low, medium, and high differentiation (LDI, IDI, and HDI, respectively). This will allow us to investigate the number, size, and distribution of regions of differentiation across the genome (and potentially under selection). We are going to use the code provided in the script fitHMM.R for that. This script can be run in batch mode like executing: Rscript scripts/fit_hmm.R, but you are likely to learn much more if you follow the steps below to run it step by step. Be aware that this is R code and will have to run within an R session. Create a new Jupyter notebook titled \"Chp09b-Delimitation_contiguous_regions\" Change the working directory in a code cell: setwd(&quot;~/popgenomics/data/fst_hmm&quot;) We will then re-load the package popgenfunctions, and load packages HiddenMarkov to fit HMMs and data.table to speed up loading big files: # library to fit Hidden Markov Models library(HiddenMarkov) # library to speed up loading of big tables library(data.table) # the library of popgen functions library(popgenfunctions) We will use a custom function called Mstep.normc (based on the Mstep.norm included in the HiddenMarkov package) to model the observed states (i.e. FST) as a normal distribution with the standard deviation fixed to the genome-wide FST standard deviation estimate. This function will be used for fitting a HMM later. This Mstep.normc function is contained within the 'popgenfunctions' library that we loaded earlier. #rnormc, dnormc, pnormc and qnormc are used further on in the ‘dthmm’ function that uses #the normc distribution from mstep.normc rnormc &lt;- rnorm #random number generator dnormc &lt;- dnorm #density pnormc &lt;- pnorm #distribution function qnormc &lt;- qnorm #quantile function Now we load the data: # Prepare data # ------------------------------------------------------------------------ # Load Fst fst.HVAxHVC&lt;-fread(&quot;timemaHVAxHVC.fst.dsv&quot;, header=T, sep=&quot;\\t&quot;) and restrict the analyses to a single linkage group: # Use only one linkage group for this example fst&lt;-fst.HVAxHVC[grep(&quot;^lg01_&quot;, fst.HVAxHVC$locus),]$fst FST is transformed using a logit function so that we can assume a normal distribution of FST for each HMM state. Missing values (“NA”) are discarded and very low FST values that may result in “Inf” values after transformation or affect the HMM fitting process are replaced by low, but tractable FST values (i.e. 1e-4) beforehand. We also store the number of SNPs, the mean and the standard deviation of logit FST, as these values will be used later. # Remove missing data fst&lt;-fst[!is.na(fst)] # Replace too low Fst values by tractable value fst[fst&lt;1e-4]&lt;-1e-4 # logit transform Fst, we will assume a normal logit distribution # for each HMM state lfst&lt;-log(fst/(1-fst)) # specify nloci, mean and sd (used below) nloci&lt;-length(lfst) mean.lfst&lt;-mean(lfst) sd.lfst&lt;-sd(lfst) # ------------------------------------------------------------------------ We now will fit a homogenous Hidden Markov Model of 3 discrete states: high (1), medium (2), and low (3) differentiation. We will use a reasonable starting transition matrix: low transition rates to and from medium state, but disallowing direct transitions between extremes (from low to high or high to low). It is highly recommended to run the algorithm multiple times (tens, hundreds, or even thousands) using different starting matrices, because the maximum-likelihood searching algorithm could be stuck in local optima. We will not do this in this practical for time reasons, but code to generate random matrices is included. # Fit homogenous HMM of 3 discrete states # ------------------------------------------------------------------------ # initial m x m transition probability matrix # disable transitions from 1 to 3 and 3 to 1 (set value to zero) # (direct transitions from low to high and high to low differentiation) Pi&lt;-matrix(c(0.9,0.1,0, 0.05,0.9,0.05, 0,0.1,0.9),nrow=3,byrow=T) Pi # it is strongly recommended to run the EM algorithm multiple times (hundreds # or thousands) with different starting matrices to avoid maximum-likelihood # local optima; e.g. code to produce 100 random matrices: # random.matrix&lt;-function(x){ # x&lt;-runif(3) # matrix(c(x[1],1-x[1],0, # x[2]/2,1-x[2],x[2]/2, # 0,x[3],1-x[3]),nrow=3,byrow=T)} # matrices&lt;-lapply(1:100, random.matrix) We also set an initial marginal probability distribution for the m hidden states (delta) and a list with the initial values for the distribution of observed process (i.e. logit FST; pm). In this case the distribution is a normal with the standard deviation fixed to the standard deviation estimated genome-wide and reasonable starting values for the means around the genome-wide mean. # initial marginal probability distribution of the m hidden states delta&lt;-c(0,1,0) # initial list of parameter values associated with the distribution # of observed process # they must coincide with the parameters of the function use # we are using a normal and must give 3 means and 3 sds pm&lt;-list(mean=c(mean.lfst*0.9,mean.lfst,mean.lfst*1.1), sd=rep(sd.lfst,3)) We finally set a discrete time HMM object with the initial values of the transition probability matrix (Pi), the marginal probability distribution of the hidden states (delta) at the start, using a modified normal distribution (normc) with variance fixed to the initial genome-wide variance estimate, and a list of initial observed process values (pm). # set discrete time HMM object and initial values # normc is modified distribution so that variance is fixed # to genome-wide estimate (sd.lfst) init&lt;-dthmm(x=lfst, Pi=Pi, delta=delta, distn=&quot;normc&quot;, pm=pm, discrete=FALSE) We estimate the parameters of our HMM using the Baum-Welch EM algorithm. This will take a few minutes. # estimate parameters of HMM using Baum-Welch EM algorithm # this may take a while (a few minutes) # set maximum number of iterations and convergence criterion ctrl&lt;-bwcontrol(maxiter=1000, tol=1e-6) # run BaumWelch algorithm paramHMM&lt;-BaumWelch(init, ctrl) # compare initial and estimated transition matrices # initial Pi # estimate paramHMM$Pi # compare initial and estimated distribution parameters # (normal means and sds) # initial pm # estimate paramHMM$pm Lastly we infer the sequence of Markov hidden states using the Viterbi algorithm (i.e. state for each loci: low, medium, or high). This will take less than 1 minute. # predict Markov states using Viterbi algorithm fitHMM&lt;-Viterbi(paramHMM) # ------------------------------------------------------------------------------ and summarize the results: # Summarize results # ------------------------------------------------------------------------------ # mean FSTs of each state: high, medium, and low differentiation # (reverse transform logit FSTs means) fst.states&lt;-round(1/(1+exp(-1 * paramHMM$pm$mean)),5) fst.states # number of loci in each state nloci.states&lt;-table(fitHMM) nloci.states # proportion of loci in each state proploci.states&lt;-nloci.states/nloci proploci.states # number of contiguous regions for each state nregions.states&lt;-unlist(lapply(1:3, function(x) sum((fitHMM==x)[1:(nloci-1)]-(fitHMM==x)[2:nloci]==1))) nregions.states # put everything together in a table summary.table&lt;-cbind(state=c(1,2,3), mean.fst=fst.states, nloci=nloci.states, proploci=proploci.states, nregions=nregions.states) # show table summary.table # save to file write.table(summary.table, file=&quot;fst_hmm_lg01_regions_summary.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) # ------------------------------------------------------------------------------ We are going to plot the regions using a low, medium, and high differentiation legend. # plot regions using differentiation legend # ------------------------------------------------------------------------------ par(mar=c(10,3,10,4)+1.5) colours&lt;-c(&quot;red&quot;,&quot;dark grey&quot;,&quot;blue&quot;) plot(fst, col=colours[fitHMM], ylim=c(0,0.8), xlab=&quot;relative position&quot;, ylab=expression(&quot;F&quot;[ST]), main=&quot;regions of differentiation&quot;, cex=1, cex.axis=1, cex.lab=1, cex.main=1.3, mgp=c(7,3,0)) # add horizontal line to illustrate top 0.1% FST values abline(h=quantile(fst, prob=0.999,na.rm=T), col=&quot;black&quot;, lty=3, cex=4, lwd=3) text(0,quantile(fst, prob=0.999,na.rm=T)+0.025, label=expression(&quot;&quot;&gt;=&quot; 0.1%&quot;), adj=1, pos=2, cex=1) # legend legend(&quot;topright&quot;,legend=c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;), pch=16, col=colours, title=&quot;differentiation&quot;, bty=&quot;n&quot;, cex=1) # ------------------------------------------------------------------------------ We have obtained the size of the regions in number of SNPs so far, let’s get the actual size in base pairs. First, we use a function to get the size of regions in bp given their coordinates (lb=lower boundaries, ub=upper boundaries) and some information about the length and order of the scaffolds in the linkage groups. This has an extra complication because of the particularities of the T. cristinae draft genome, where scaffolds were assigned to linkage groups in a particular order using additional genetic information from crosses. Actually, the values obtained here are only approximations, because although the order of the scaffolds in a linkage group is known, the distance among them (i.e. the number of bp) is not. We need to get some info about the order and size of the scaffolds from the file lg_ord_sca_length.dsv # load the order and size of scaffolds in the draft genome # ------------------------------------------------------------------------------ lgordscalen&lt;-read.table(&quot;data/lg_ord_sca_length.dsv&quot;,header=T,sep=&quot;\\t&quot;) lgordscalen&lt;-lgordscalen[lgordscalen$lg==1,] loci.pos&lt;-as.numeric(gsub(&quot;.*:&quot;,&quot;&quot;,fst.HVAxHVC$locus)) loci.ord&lt;-as.numeric(gsub(&quot;.*ord|_scaf.*&quot;,&quot;&quot;,fst.HVAxHVC$locus)) loci.sca&lt;-as.numeric(gsub(&quot;.*_scaf|:.*&quot;,&quot;&quot;,fst.HVAxHVC$locus)) # ------------------------------------------------------------------------------ Now get sizes of regions of high differentiation: # high differentiation regions # ------------------------------------------------------------------------------ lb.locindex&lt;-which(fitHMM[2:nloci]==1 &amp; fitHMM[1:(nloci-1)]!=1) ub.locindex&lt;-which(fitHMM[1:(nloci-1)]==1 &amp; fitHMM[2:nloci]!=1) if (length(lb.locindex)&lt;length(ub.locindex)) lb.locindex&lt;-c(1,lb.locindex) if (length(lb.locindex)&gt;length(ub.locindex)) ub.locindex&lt;-c(ub.locindex,nloci) hidif.regions&lt;-get.regions(lb.locindex,ub.locindex,loci.ord,loci.pos,lgordscalen) write.table(hidif.regions, file=&quot;fst_hmm_lg01_hidif_regions.dsv&quot;, quote=F, sep=&quot;\\t&quot;) # ------------------------------------------------------------------------------ medium differentiation: # medium differentiation regions # ------------------------------------------------------------------------------ lb.locindex&lt;-which(fitHMM[2:nloci]==2 &amp; fitHMM[1:(nloci-1)]!=2) ub.locindex&lt;-which(fitHMM[1:(nloci-1)]==2 &amp; fitHMM[2:nloci]!=2) if (length(lb.locindex)&lt;length(ub.locindex)) lb.locindex&lt;-c(1,lb.locindex) if (length(lb.locindex)&gt;length(ub.locindex)) ub.locindex&lt;-c(ub.locindex,nloci) medif.regions&lt;-get.regions(lb.locindex,ub.locindex,loci.ord,loci.pos,lgordscalen) write.table(medif.regions, file=&quot;fst_hmm_lg01_medif_regions.dsv&quot;, quote=F, sep=&quot;\\t&quot;) # ------------------------------------------------------------------------------ and low differentiation: # low differentiation regions # ------------------------------------------------------------------------------ lb.locindex&lt;-which(fitHMM[2:nloci]==3 &amp; fitHMM[1:(nloci-1)]!=3) ub.locindex&lt;-which(fitHMM[1:(nloci-1)]==3 &amp; fitHMM[2:nloci]!=3) if (length(lb.locindex)&lt;length(ub.locindex)) lb.locindex&lt;-c(1,lb.locindex) if (length(lb.locindex)&gt;length(ub.locindex)) ub.locindex&lt;-c(ub.locindex,nloci) lodif.regions&lt;-get.regions(lb.locindex,ub.locindex,loci.ord,loci.pos,lgordscalen) write.table(lodif.regions, file=&quot;fst_hmm_lg01_lodif_regions.dsv&quot;, quote=F, sep=&quot;\\t&quot;) # ------------------------------------------------------------------------------ Finally, we plot the distribution of sizes for high, medium, and low differentiation regions. # Compare distribution of sizes # ------------------------------------------------------------------------------ par(mfrow=c(1,3)) par(mar=c(5,5,10,3)+1) hist(hidif.regions$length/1000,main=&quot;high differentiation regions&quot;, xlim=c(0,300),xlab=&quot;size (Kbp)&quot;,breaks=50, cex.axis=1, cex.lab=1, cex.main=1) hist(medif.regions$length/1000,main=&quot;medium differentiation regions&quot;, xlim=c(0,300), xlab=&quot;size (Kbp)&quot;,breaks=50, cex=4, cex.axis=1, cex.lab=1, cex.main=1) hist(lodif.regions$length/1000,main=&quot;low differentiation regions&quot;, xlim=c(0,300), xlab=&quot;size (Kbp)&quot;, breaks=50, cex=4, cex.axis=1, cex.lab=1, cex.main=1) # ------------------------------------------------------------------------------ "],["10-GWAS.html", "Chapter 10 Genetic architecture of traits using multi-locus Genome Wide Association (GWA) mapping with GEMMA 10.1 Initial set up 10.2 Data formatting 10.3 Running GEMMA to fit a Bayesian sparse linear mixed model (BSLMM) 10.4 Analysing GEMMA BSLMM output", " Chapter 10 Genetic architecture of traits using multi-locus Genome Wide Association (GWA) mapping with GEMMA The aim of this chapter is to use a multi-SNP method to study the genetic architecture of colour pattern phenotype (i.e. white stripe) in Timema cristinae stick insects. In particular, we will use the Bayesian Sparse Linear Mixed Model (BSLMM) model implemented in the program GEMMA, a piece of software that also allows taking into account any potential population structure. This model is a hybrid approach that allows inferring whether the genetic basis of a trait is highly polygenic or rather determined by a few loci (i.e. oligogenic), as well as identifying the loci that show the strongest association with the trait. We will use the dataset of Comeault et al. 2015, which consists of 602 individuals sampled from a phenotypically variable population called FHA. We will infer the proportion of phenotypic variance explained by the genotypes, what fraction of such variance can be explained by a reduced number of major effect loci, and will identify what loci show the most significant association with this trait. Resources Zhou et al. 2013 - Paper where the Bayesian Sparse Linear Mixed Model approach implemented in GEMMA is described. Zhou Lab website - You can obtain GEMMA and a few other interesting programs for GWAS here. GEMMA manual 10.1 Initial set up First you will need move into the folder for this practical. cd ~/popgenomics/data/gwas_gemma 10.2 Data formatting ls -lh data There is a vcf file containing single nucleotide polymorphisms (SNPs) from RAD data of 602 individuals from a single polymorphic population of Timema cristinae (population code FHA). You can have a look at the file content with the following commands: zless -S data/fha.vcf.gz # or with bcftools bcftools view data/fha.vcf.gz | less -S # excluding long header bcftools view -H data/fha.vcf.gz | less -S There are two files containing the phenotypes of the same individuals in the same order as in the vcf file (NA when the phenotype is missing). These phenotypes encode the dorsal white stripe either as a continuous trait (standardised white stripe area; fha.pheno) or as a discrete binary trait (presence/absence of the stripe; fha.pheno2). We will be using the first one for this practical (but it would be a good exercise to repeat the analyses using the other one with the probit model using - bslmm 3). You can have a look at the files content: head data/fha.pheno head data/fha.pheno2 GEMMA, the program we will be using to carry out multi-variant GWA, needs two input files: one with the genotypes and another one with the phenotypes. Most GWA programs rely on called genotypes, but GEMMA can work with genotype probabilities, which allows incorporating genotype uncertainty in the analyses. GEMMA accepts a number of formats for genotypes, we are going to use the mean genotype format (based on the BIMBAM format), where genotypes are encoded as mean genotypes. A mean genotype is a value between 0 and 2 that can be interpreted as the minor allele dosage: 0 is homozygous for the major allele, 1 is a heterozygote, and 2 is a homozygote for the minor allele. Thus, intermediate values reflect the uncertainty in genotypes. We are going to use the custom Perl script bcf2bbgeno.pl to calculate the mean genotypes. First, the script removes all the SNPs that have more than two alleles. This is common practice, because most of the models used for inference are based on biallelic variants. Then, the script calculates empirical posterior genotype probabilities from the genotype likelihoods in the vcf file under the assumption that the population is in Hardy-Weinberg equilibrium (HWE). Specifically, the script uses inferred allele frequencies to set HWE priors: Equation: \\[ p(AA) = p ^{2}; p(aa) = (1-p)^{2}; p(Aa) = 2p(1-p) \\] p being the allele frequency of major/reference allele A. Genotype likelihoods are multiplied by these priors to obtain genotype posterior probabilities that are then encoded as mean genotypes and saved to a .bbgeno file. You can get some info about how to run the Perl script: # show help perl scripts/bcf2bbgeno.pl -h To calculate the genotype posterior probabilites and save them in mean genotype format, we would need to run a command like this one (followed by compression to save some space, given that GEMMA can handle gzipped files): perl scripts/bcf2bbgeno.pl -i data/fha.vcf.gz -o fha.bbgeno -p H-W -s -r gzip fha.bbgeno However, this process may take quite a while. Therefore it would be better to use a batch job (see bcf2bbgeno.sh). Even this will take some time, so lets skip this step altogether for the purpose of this tutorial and use the .bbgeno.gz and .bbgeno.ids.txt files from the results folder. First cancel the command if you ran it (Control+C) and then copy the files needed from the results folder. cp results/*.bbgeno.* . You should now have two new files: fha.bbgeno.gz, which contains the mean genotypes, and fha.bbgeno.ids.txt, which contains the IDs of the samples (i.e. individuals) in the same order as shown in the genotypes file. You can have a look at them: zless -S fha.bbgeno.gz The first column is the SNP ID, second one is major/reference allele and the third one is the minor/alternate allele. Having major and minor or reference and alternate alleles depends on how you called variants. This does not matter for downstream analyses with GEMMA, but should be kept in mind when interpreting the results. The rest of the columns are the mean genotypes for all the individuals (one for each). The other file is simply to keep track of the order of the individuals in the genotype file: less -S fha.bbgeno.ids.txt 10.3 Running GEMMA to fit a Bayesian sparse linear mixed model (BSLMM) GEMMA (Genome-wide Efficient Mixed Model Association) is a complex piece of software with many options. It is highly recommended that you read the manual carefully. Let’s have a look at the options, with emphasis on the specific ones we will be using. # general help gemma -h # quick guide gemma -h 1 #We will be using the Bayesian sparse linear mixed model (BSLMM): gemma -h 10 #but first we will have to calculate the relatedness matrix to take into account population structure: gemma -h 4 #and it would also be interesting do some filtering such as excluding rare variants, which are difficult to tell apart from sequencing errors: gemma -h 3 GEMMA uses a relatedness matrix to account for any population structure in the data that may affect results. The easiest way to obtain such matrix is using GEMMA itself. We are going to use the submission script gemma_relmatrix.sh. Let’s have a look at the script: less scripts/gemma_relmatrix.sh Now let’s run the script (it should take just a few minutes): chmod +x scripts/gemma_relmatrix.sh scripts/gemma_relmatrix.sh When finished, it should have two files in the newly created output directory: relmatrix.log.txt, a log file, and relmatrix.cXX.txt, the actual relatedness matrix. If you have a look at them, they should look similar to these: less output/relmatrix.log.txt less -S output/relmatrix.cXX.txt Now we are going to run GEMMA to fit a BSLMM model. Since this will take a long time using the teaching cluster, we are not going to submit it now. The script has been designed to run as batch job gemma_bslmm.sh. GEMMA is run using the relatedness matrix we estimated before, using the BSLMM model with the continuous phenotypes and excluding variants with a minor allele frequency of less than 1%. We will run a MCMC chain for 1,000,000 steps, after an initial burn-in of 250,000, saving every 100th step, so that we will end up with 10,000 samples to approximate the posterior distribution of the parameters. Let’s have a look : less scripts/gemma_bslmm.sh Do not run this now. However, if you were to submit the script you would do so using the following: scripts/gemma_bslmm.sh Copy the following files into the output folder: cp results/output/bslmm* output/ ls -lh output The new files generated are: bslmm.bv.txt -&gt; posterior samples of breeding values (~estimated random effects) bslmm.gamma.txt -&gt; posterior samples for the gamma (i.e. sparse effects) listing the SNPs included in the model in each iteration, that is the loci with detectable (large) effects bslmm.hyp.txt -&gt; posterior samples of hyper-parameters, including the proportion of the variance explained (pve), the proportion of the genetic variance (i.e. pve) explained by the sparse effect loci (pge), the proportion of SNPs with non-zero effects (pi) and the estimated number of loci with large effects (n_gamma) bslmm.log.txt -&gt; log file bslmm.param.txt -&gt; posterior samples of parameters: random effects (alpha), fixed effects (beta), and sparse effects (gamma) Let’s have a look at the log. Notice the number of individuals with both genotypic and phenotypic data is 546 out of 602 and the number of SNPs was reduced from 518,232 to 346,660 after excluding SNPs missing genotypes for over 5% individuals (default threshold) and rare variants (minor allele frequency of less than 1%): less -S output/bslmm.log.txt the hyperparameters: less -S output/bslmm.hyp.txt h - approximation to PVE: proportion of phenotypic variance explained by loci pve - proportion of phenotypic variance explained by the genotypes rho - approximation to PGE: proportion of genetic variance explained by sparse effect terms (~major effect loci) (rho=0 =&gt; pure LMM, highly polygenic; rho=1 =&gt; pure BVSR, few loci) pge - proportion of genetic variance explained by major effect loci pi - proportion of variants with non-zero effects n_gamma - number of variants with major effect and the parameters (notice we didn’t specify chromosomes (chr) or positions (pos) and therefore there are only SNP IDs (rs column): less -S output/bslmm.param.txt For more information please see the GEMMA manual 10.4 Analysing GEMMA BSLMM output We will now make some plots to help us to visualise the results from the hyperparamter and paramter files. Create a new Jupyter notebook called \"Chp10-GWAS\". First, we are going to play with the hyperparameter estimates, which will inform us of the genetic architecture of the trait. First we change the working directory: setwd(&quot;~/popgenomics/data/gwas_gemma/output&quot;) And then load the hyperparameter file: # Load hyperparameter file # ============================================================================== hyp.params&lt;-read.table(&quot;bslmm.hyp.txt&quot;,header=T) # ============================================================================== Let’s get some statistics (mean, median, and 95% ETPI - equal-tail posterior probability intervals): # Get mean, median, and 95% ETPI of hyperparameters # ============================================================================== # pve -&gt; proportion of phenotypic variance explained by the genotypes pve&lt;-c(&quot;PVE&quot;, mean(hyp.params$pve),quantile(hyp.params$pve, probs=c(0.5,0.025,0.975))) # pge -&gt; proportion of genetic variance explained by major effect loci pge&lt;-c(&quot;PGE&quot;,mean(hyp.params$pge),quantile(hyp.params$pge, probs=c(0.5,0.025,0.975))) # pi -&gt; proportion of variants with non-zero effects pi&lt;-c(&quot;pi&quot;,mean(hyp.params$pi),quantile(hyp.params$pi, probs=c(0.5,0.025,0.975))) # n.gamma -&gt; number of variants with major effect n.gamma&lt;-c(&quot;n.gamma&quot;,mean(hyp.params$n_gamma),quantile(hyp.params$n_gamma, probs=c(0.5,0.025,0.975))) # ============================================================================== # get table of hyperparameters # ============================================================================== hyp.params.table&lt;-as.data.frame(rbind(pve,pge,pi,n.gamma),row.names=F) colnames(hyp.params.table)&lt;-c(&quot;hyperparam&quot;, &quot;mean&quot;,&quot;median&quot;,&quot;2.5%&quot;, &quot;97.5%&quot;) # show table hyp.params.table # write table to file write.table(hyp.params.table, file=&quot;hyperparameters.dsv&quot;, sep=&quot;\\t&quot;, quote=F) # ============================================================================== Values should be similar to this: hyperparam mean median 2.5% 97.5% PVE 0.50758534164 0.49616535 0.3290844075 0.753359795 PGE 0.70427977221 0.70412075 0.4118065175 0.9789640325 pi 1.29564889986e-05 9.376132e-06 3.25441665e-06 4.3316011e-05 n.gamma 4.574 4 1 14 Now let’s plot the MCMC traces and the posterior distributions of the hyperparameters and save them into a pdf file. # plot traces and distributions of hyperparameters # ============================================================================== layout(matrix(c(1,1,2,3,4,4,5,6), 4, 2, byrow = TRUE)) # PVE # ------------------------------------------------------------------------------ plot(hyp.params$pve, type=&quot;l&quot;, ylab=&quot;PVE&quot;, main=&quot;PVE - trace&quot;) hist(hyp.params$pve, main=&quot;PVE - posterior distribution&quot;, xlab=&quot;PVE&quot;) plot(density(hyp.params$pve), main=&quot;PVE - posterior distribution&quot;, xlab=&quot;PVE&quot;) # ------------------------------------------------------------------------------ # PGE # ------------------------------------------------------------------------------ plot(hyp.params$pge, type=&quot;l&quot;, ylab=&quot;PGE&quot;, main=&quot;PGE - trace&quot;) hist(hyp.params$pge, main=&quot;PGE - posterior distribution&quot;, xlab=&quot;PGE&quot;) plot(density(hyp.params$pge), main=&quot;PGE - posterior distribution&quot;, xlab=&quot;PGE&quot;) # ------------------------------------------------------------------------------ # pi # ------------------------------------------------------------------------------ plot(hyp.params$pi, type=&quot;l&quot;, ylab=&quot;pi&quot;, main=&quot;pi&quot;) hist(hyp.params$pi, main=&quot;pi&quot;, xlab=&quot;pi&quot;) plot(density(hyp.params$pi), main=&quot;pi&quot;, xlab=&quot;pi&quot;) # ------------------------------------------------------------------------------ # No gamma # ------------------------------------------------------------------------------ plot(hyp.params$n_gamma, type=&quot;l&quot;, ylab=&quot;n_gamma&quot;, main=&quot;n_gamma - trace&quot;) hist(hyp.params$n_gamma, main=&quot;n_gamma - posterior distribution&quot;, xlab=&quot;n_gamma&quot;) plot(density(hyp.params$pi), main=&quot;n_gamma - posterior distribution&quot;, xlab=&quot;n_gamma&quot;) # ------------------------------------------------------------------------------ # ============================================================================== Ideally, the MCMC traces should look like a caterpillar and distributions should be generally unimodal, indicating that mixing was good. Also it is highly recommended to run GEMMA multiple times (at least 3) and compare both the traces and the posterior distributions of the hyperparameters obtained from different runs and check that all are converging into the same values. However, we will not do this in this practical for time reasons. For instance, PVE should look like this: We can now look at the parameter estimates to see if there are any SNPs that are strongly and consistently associated with the trait. We will load a library to help speed up the loading of large tables and we will then load the parameter estimates: # library to speed up loading of big tables library(data.table) # Load parameters output # ============================================================================== params&lt;-fread(&quot;bslmm.param.txt&quot;,header=T,sep=&quot;\\t&quot;, data.table=F) # ============================================================================== Now we will get the SNPs that have a sparse effect (i.e. detectable large effect) on the phenotype. In the table, beta is actually β|γ == 1 for a given SNP, β being the small effect all variants have and γ the additional large effect only some variants have. Thus, the sparse effect size for each SNP is calculated multiplying β by γ. # Get variants with sparse effect size on phenotypes # ============================================================================== # add sparse effect size (= beta * gamma) to data frame params[&quot;eff&quot;]&lt;-abs(params$beta*params$gamma) Then we will extract the SNPs that have a detectable large effect size, sort them by effect size, and save the top SNPs using several thresholds (1%, 0.1%, 0.01%): # get variants with effect size &gt; 0 params.effects&lt;-params[params$eff&gt;0,] # show number of variants with measurable effect nrow(params.effects) # sort by descending effect size params.effects.sort&lt;-params.effects[order(-params.effects$eff),] # show top 10 variants with highest effect head(params.effects.sort, 10) # variants with the highest sparse effects # ------------------------------------------------------------------------------ # top 1% variants (above 99% quantile) top1&lt;-params.effects.sort[params.effects.sort$eff&gt;quantile(params.effects.sort$eff,0.99),] # top 0.1% variants (above 99.9% quantile) top01&lt;-params.effects.sort[params.effects.sort$eff&gt;quantile(params.effects.sort$eff,0.999),] # top 0.01% variants (above 99.99% quantile) top001&lt;-params.effects.sort[params.effects.sort$eff&gt;quantile(params.effects.sort$eff,0.9999),] # ------------------------------------------------------------------------------ # write tables write.table(top1, file=&quot;top1eff.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) write.table(top01, file=&quot;top0.1eff.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) write.table(top001, file=&quot;top0.01eff.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) There should be around 15595 SNPs with detectable sparse effects and the top SNPs should look like this (notice we didn’t specify the chromosomes or the positions and therefore there are only SNP IDs, i.e. rs): chr rs ps n_miss alpha beta gamma -9 lg8_ord45_scaf1036-131573 -9 0 -1.229901e-05 -1.2065980 0.5364 4929 0.64721917 -9 lg8_ord55_scaf1512-149001 -9 0 -6.640361e-05 -0.7905428 0.5635 4943 0.48103043 -9 lg8_ord66_scaf318-448145 -9 0 -1.133074e-04 -0.6136137 0.0566 341474 -9 lgNA_ordNA_scaf784-237602 -9 0 1.004314e-04 0.7119540 0.0378 330495 -9 lg3_ord81_scaf488-29866 -9 0 7.241678e-05 0.9273204 0.0232 315197 -9 lgNA_ordNA_scaf154-274617 -9 0 5.164412e-05 1.1116970 0.0157 5105 -9 lg6_ord32_scaf531-110990 -9 0 6.656697e-05 0.9619961 0.0169 183101 -9 lg6_ord32_scaf531-110990 -9 0 6.656697e-05 0.9619961 0.0169 138641 -9 lg6_ord32_scaf531-110989 -9 0 6.606357e-05 0.9763403 0.0144 138636 -9 lg3_ord28_scaf970-18573 -9 0 5.868428e-05 1.1262150 0.0109 138636 We are going to estimate the Posterior Inclusion Probability (PIP), that is the frequency a SNP is estimated to have a detectable large effect in the MCMC (i.e. γ). This can be used as a measure of the strength of the association of a SNP with a phenotype. As with the effect sizes we will sort them by PIP, and save the top SNPs using several thresholds (0.01, 0.1, 0.25, 0.5): # ============================================================================== # Get variants with high Posterior Inclusion Probability (PIP) == gamma # ============================================================================== # PIP is the frequency a variant is estimated to have a sparse effect in the MCMC # sort variants by descending PIP params.pipsort&lt;-params[order(-params$gamma),] # Show top 10 variants with highest PIP head(params.pipsort,10) # sets of variants above a certain threshold # variants with effect in 1% MCMC samples or more pip01&lt;-params.pipsort[params.pipsort$gamma&gt;=0.01,] # variants with effect in 10% MCMC samples or more pip10&lt;-params.pipsort[params.pipsort$gamma&gt;=0.10,] # variants with effect in 25% MCMC samples or more pip25&lt;-params.pipsort[params.pipsort$gamma&gt;=0.25,] # variants with effect in 50% MCMC samples or more pip50&lt;-params.pipsort[params.pipsort$gamma&gt;=0.50,] # write tables write.table(pip01, file=&quot;pip01.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) write.table(pip10, file=&quot;pip10.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) write.table(pip25, file=&quot;pip25.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) write.table(pip50, file=&quot;pip50.dsv&quot;, quote=F, row.names=F, sep=&quot;\\t&quot;) You can see the SNPs are very similar but not exactly the same ones than when sorted by effect size: chr rs ps n_miss alpha beta gamma -9 lg8_ord55_scaf1512-149001 -9 0 -6.640361e-05 -0.7905428 0.5635 0.445470868 -9 lg8_ord45_scaf1036-131573 -9 0 -1.229901e-05 -1.2065980 0.5364 0.647219167 -9 lg8_ord45_scaf1036-131605 -9 0 -1.096165e-05 -1.0375980 0.4636 0.481030433 -9 lg8_ord66_scaf318-448145 -9 0 -1.133074e-04 -0.6136137 0.0566 0.034730535 -9 lgNA_ordNA_scaf784-237602 -9 0 1.004314e-04 0.7119540 0.0378 0.026911861 -9 lg1_ord96_scaf190-725578 -9 0 -2.183769e-04 -0.3418733 0.0326 0.011145070 -9 lg3_ord81_scaf488-29866 -9 0 7.241678e-05 0.9273204 0.0232 0.021513833 -9 lg10_ord64_scaf380-30883 -9 0 -2.363161e-04 -0.3245005 0.0180 0.005841009 -9 lg8_ord60_scaf2482-78371 -9 0 1.514078e-04 0.4569336 0.0174 0.007950645 -9 lg6_ord32_scaf531-110990 -9 0 6.656697e-05 0.9619961 0.0169 0.016257734 Lastly we can generate a Manhattan plot of the PIPs to visualize the distribution of associated SNPs across the genome. We will highlight the top candidates in the genome and make the size of the dots reflect the effect size. # ============================================================================== # plot variants PIPs across linkage groups/chromosomes # ============================================================================== # Prepare data # ------------------------------------------------------------------------------ # add linkage group column (chr) chr&lt;-gsub(&quot;lg|_.+&quot;,&quot;&quot;,params$rs) params[&quot;chr&quot;]&lt;-chr # sort by linkage group and position params.sort&lt;-params[order(as.numeric(params$chr), params$rs),] # get list of linkage groups/chromosomes chrs&lt;-sort(as.numeric(unique(chr))) # ------------------------------------------------------------------------------ # Plot figure # ------------------------------------------------------------------------------ # ------------------------------------------------------------------------------ # set up empty plot plot(-1,-1,xlim=c(0,nrow(params.sort)),ylim=c(0,1),ylab=&quot;PIP&quot;,xlab=&quot;linkage group&quot;, xaxt=&quot;n&quot;) # plot grey bands for chromosome/linkage groups # ------------------------------------------------------------------------------ start&lt;-1 lab.pos&lt;-vector() for (ch in chrs){ size&lt;-nrow(params.sort[params.sort$chr==ch,]) cat (&quot;CH: &quot;, ch, &quot;\\n&quot;) colour&lt;-&quot;light grey&quot; if (ch%%2 &gt; 0){ polygon(c(start,start,start+size,start+size,start), c(0,1,1,0,0), col=colour, border=colour) } cat(&quot;CHR: &quot;, ch, &quot; variants: &quot;, size, &quot;(total: &quot;, (start+size), &quot;)\\n&quot;) txtpos&lt;-start+size/2 lab.pos&lt;-c(lab.pos, txtpos) start&lt;-start+size } # Add variants outside linkage groups chrs&lt;-c(chrs,&quot;NA&quot;) size&lt;-nrow(params.sort[params.sort$chr==&quot;NA&quot;,]) lab.pos&lt;-c(lab.pos, start+size/2) # ------------------------------------------------------------------------------ # Add x axis labels axis(side=1,at=lab.pos,labels=chrs,tick=F) # plot PIP for all variants # ------------------------------------------------------------------------------ # rank of variants across linkage groups x&lt;-seq(1,length(params.sort$gamma),1) # PIP y&lt;-params.sort$gamma # sparse effect size, used for dot size z&lt;-params.sort$eff # log-transform to enhance visibility z[z==0]&lt;-0.00000000001 z&lt;-1/abs(log(z)) # plot symbols(x,y,circles=z, bg=&quot;black&quot;,inches=1/5, fg=NULL,add=T) # ------------------------------------------------------------------------------ # highlight high PIP variants (PIP&gt;=0.25) # ------------------------------------------------------------------------------ # plot threshold line abline(h=0.25,lty=3,col=&quot;dark grey&quot;) # rank of high PIP variants across linkage groups x&lt;-match(params.sort$gamma[params.sort$gamma&gt;=0.25],params.sort$gamma) # PIP y&lt;-params.sort$gamma[params.sort$gamma&gt;=0.25] # sparse effect size, used for dot size z&lt;-params.sort$eff[params.sort$gamma&gt;=0.25] z&lt;-1/abs(log(z)) symbols(x,y,circles=z, bg=&quot;red&quot;,inches=1/5,fg=NULL,add=T) # ------------------------------------------------------------------------------ # add label high PIP variants text(x,y,labels=params.sort$rs[params.sort$gamma&gt;=0.25], adj=c(0,0), cex=0.8) # ------------------------------------------------------------------------------ # ------------------------------------------------------------------------------ # ============================================================================== The PIP plot shows there are three large effect size SNPs strongly associated with colour pattern in two non-contiguous scaffolds that belong to linkage group 8: The code to produce these plots is contained in two R scripts. If you were running these on your own data the scripts could be run in batch mode by executing: Rscript gemma_hyperparam.R. We will provide the scripts in the materials we send around after the course. "],["11-Appendix.html", "A Mamba installs A.1 Mamba installation and environment B Jupyter-notebook C Using loops to process multiple samples C.1 Simple loop example C.2 Trimmomatic loop example D An overview of Bayesian statistics E Additional approaches for detecting selection and for GWAS E.1 rehh E.2 GCTA", " A Mamba installs A.1 Mamba installation and environment Mamba is a reimplementation of conda. It is a great tool for installing bioinformatic packages including R packages. Mamba github: https://github.com/mamba-org/mamba Mamba installation: https://github.com/conda-forge/miniforge#mambaforge Mamba guide: https://mamba.readthedocs.io/en/latest/user_guide/mamba.html To create the mamba environment popgenomics run the below commands in your bash. You will need to have installed mamba first. #R community mamba create -n popgenomics mamba activate popgenomics #Install packages from bioconda channel #NGSadmix is part of angsd mamba install -c bioconda plink fastqc trimmomatic bwa samtools bcftools angsd r-popgenome gemma B Jupyter-notebook If you are running this on your own computer you can use RStudio for R. However, you can also use Jupyter-notebook if you are using an HPC or prefer it. If using bash you will need to create an environment with Jupyter-notebook. Ensure you are in the (base) mamba environment. mamba create -n jupyter mamba activate jupyter mamba install -c anaconda jupyter mamba deactivate To run Jupyter-notebook with your r_community environment you can run the following. #Activate you R_community env mamba activate r_community #Run jupyter-notebook (may be a slightly different path) ~/mamba/envs/jupyter/bin/jupyter-notebook C Using loops to process multiple samples In our main workflow we learnt how to run the QC and alignment steps using a single sample. You may be wondering how you would run these initial steps when you have many samples. Ideally you don't want to have to run the same command multiple times - this is a recipe for typos and errors creeping into your code as well as being tedious and time consuming. One approach to get around this is to run a loop which will cycle through each of your samples running the task you specify. Put simply the syntax for a for loop is as follows: for &lt;variable name&gt; in &lt;a list of items&gt;; do &lt;run a command&gt; $&lt;variable name&gt;; done The &lt;variable name&gt; is the name of the variable you will use in the do part of your loop. It contains the item in your list the loop is currently using at that point. I.e. in the simple loop example (below) we call this variable f but you could use any name as long as you use it consistently in your do section. The &lt;a list of items&gt; is anything that returns a list of items, e.g. a list of files in our case. C.1 Simple loop example Below is an example of a loop, which can be adjusted for your samples and the software you are running. This simplified loop will clearly demonstrate what the first part of the code is doing. Make sure you are in the directory ~/popgenomics/data/fastq/ and in a terminal window (not in Jupyter). for f in *_1_small.fastq.gz; #the following will loop through all files ending _1_small.fastq.gz do BASE=${f%_1_small.fastq.gz} #assign the file name &#39;$f&#39; to a variable called &#39;BASE&#39; but remove the extension &#39;_1_small.fastq.gz&#39; echo $f #print $f to screen echo ${BASE}; #print ${BASE} to screen done By using echo to print the following variables to the console we can see: echo $f: prints the full filename. echo ${BASE}: prints just the start of the filename (minus the _1_small.fastq.gz extension as we specified in our code) The loop then repeats the commands on the next file in the list (in this case we only have one file) and will keep going until it has run through all files ending with _1_small.fastq.gz in the given directory. done specifies the end of the commands in the loop. All commands between do and done in your loop will be executed. In this case we only have two files but this would work with any number of files. This is a useful bit of code as it means we can keep the filename but change the ending/extension of the file in our loops. C.2 Trimmomatic loop example Now we can write a loop to run trimmomatic starting with the code we used above as a base. for f in *_1_small.fastq.gz; #the following will loop through all files ending _1_small.fastq.gz do BASE=${f%_1_small.fastq.gz} #assign the file name &#39;$f&#39; to a variable called &#39;BASE&#39; but remove the extension &#39;_1_small.fastq.gz&#39; trimmomatic PE -threads 4 -phred33 \\ #call trimmomatic and specify parameters ${BASE}_1_small.fastq.gz \\ #specify input R1 extension ${BASE}_2_small.fastq.gz \\ #specify input R2 extension trimmed/${BASE}_1_out_paired.fastq \\ #specify directory and filename for output of paired R1 read trimmed/${BASE}_1_out_unpaired.fastq \\ #specify directory and filename for output of unpaired R1 read trimmed/${BASE}_2_out_paired.fastq \\ #specify directory and filename for output of paired R2 read trimmed/${BASE}_2_out_unpaired.fastq \\ #specify directory and filename for output of unpaired R2 read ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:12 SLIDINGWINDOW:4:30 MINLEN:80; #further paramenters for trimmomatic done #end of commands You can then edit this code to run other software in a loop. For a nice basic intro to Linux loops please click here. D An overview of Bayesian statistics As indicated in the presentations and used in some of the workshop analyses, many tools incorporate Bayesian approaches or statistics over other approaches. For example, population structure, admixture and ancestry can be examined using NGSadmix, which uses a maximum-likelihood approach, or STRUCTURE, which uses a Bayesian approach, and GEMMA for GWAS can implement a number of different models under a Bayesian framework. But, what does a Bayesian approach actually entail? This resource and this one are useful introductions - they are easy to follow and not too intimidating. (The latter includes some R exercises to demonstrate some principles). The core philosophy of Bayesian statistics is built around the concept of conditional probability, with analyses aiming to maximise this conditional probability or to minimise the uncertainty in parameter space around this. This conditional probability is the probability or likelihood of observing a particular outcome given (i.e. conditional on) the data, a model and some prior information (a prior). (Paradoxically, it is actually the probability of observing the data, given the model, the priors and the outcome). The ability to include prior information is key, and an advantage of the approach is that it evaluates potential outcomes and all the parameters that underlie these simultaneously. It also lends itself to being iterative; the priors and their influence on the conditional probabilities can be explored, refined or updated through repeated analyses or by incorporating additional data. In many cases, Bayesian approaches are favoured, not because of the ability to incorporate this prior, but because they are computationally efficient. If one can conceptualise a particular solution or result as lying at a given point in parameter space, a typical maximum likelihood approach may use an exhaustive or heuristic procedure to evaluate all points in that space, calculating a likelihood or probability given the model and data, and eventually compare these. In most cases, this is computationally restrictive and/or time-consuming. Bayesian approaches employ much more efficient sampling algorithms, such as MCMC (Markov chain Monte Carlo), as conducted when implementing the BSLMM model in GEMMA, to sample posterior probability distributions. Multiple chains will move around and selectively sample various points in parameter space, comparing the conditional probabilities among the chains themselves and to the prior sampled states, and retain those points (and the parameters underlying them) from the posterior probability distribution if there was an improvement in conditional probability. In doing so, eventually the chains and sampling will converge around the “true result”, as in the figure above, taken from this paper. While the chains might not necessarily hit on the exact result, there will be sufficient sampling around the “true result” that one can synthesise the information from multiple sampled “generations”, and obtain average values around the optimum. As such, it can provide useful information around uncertainty in our data, models and priors, allowing us to evaluate confidence intervals easily, and potentially change parameters, priors or add data. An unfortunate drawback of this approach is the possibility that the sampling gets trapped in local optima in parameter space (the points in the bottom right corner in the figure) rather than sampling near the “true result”. Thus, it is often advised to run multiple iterations of Bayesian analyses, modifying the starting parameters of the search, where the applications allow this. There are additional aspects to be aware of. There is a longstanding debate as to whether Bayesian statistics have frequentist properties (and may be interpreted as one would interpret more traditional frequentist statistics, especially when evaluating significance) or whether they are entirely probabilistic. Often this conflict results from subtle differences in philosophical perspective; see this example. Additionally, when designing an experiment or choosing an analytical approach, Bayesian and frequentist approaches can yield different outcomes, with frequentist approaches often being more sensitive than Bayesian counterparts to how the hypotheses to be tested are framed or approached. E Additional approaches for detecting selection and for GWAS E.1 rehh rehh is a R package which is used to explore SNP data and identify genomic regions that have been under recent positive selection (where advantageous variants are retained and increase in prevalence) in a single population or species, or to examine differential selection between two populations. A handy vignette (from which the above graph was taken) is available here. The approach is based on the concept of Extended Haplotype Homozygosity (EHH) (or a long-range haplotype test) proposed by Sabeti et al. 2002. Fundamentally, it considers the relationship between allele frequencies and linkage disequilibrium (LD). Under neutral evolution, mutations require time to establish in a population, by which time LD would have broken down around these mutations through recombination. Under recent positive selection, mutations increase in frequency rapidly, before LD has a chance to break down, and these extended haplotypes are then transmitted without recombination. EHH is the probability that a pair of randomly-chosen chromosomes are identical by descent and homozygous for those SNPs which we have genotyped and mapped at a given distance from a core region of interest (presented as haplotypes of increasing length) on that chromosome. Values range from 0, where all these extended haplotypes are different, to 1, where all are homozygous. EHH is then considered relatively, where the EHH of the tested core haplotype is compared with the EHHs of other core haplotypes in the same region (but excluding other core haplotypes). Through this, the approach detects the breakdown of linkage disequilibrium as one moves further away from the core region. Core haplotypes in regions that have high EHH and are at high frequency in the population indicate mutations that have likely been established through non-neutral evolution. The approach appears to be more powerful than many traditional metrics, but it can be complex to implement. To reconstruct the extended haplotypes, genotype data are phased, where each of the alleles of a given SNP genotype is assigned to a hypothetical haplotype. This is obviously straightforward when many loci are homozygous, but as one moves away from the SNP of interest (central in the core haplotype) and encounters heterozygous SNP genotypes the number of hypothetical haplotypes increases exponentially. There are a number of tools and approaches for phasing haplotypes, and a useful (albeit dated) review was published by Browning &amp; Browning 2011. As input, rehh can accept haplotype data in various formats, including vcf-formatted data, and those produced by phasing programmes like SHAPEIT2 and fastPHASE. E.2 GCTA GCTA Genome-wide Complex Trait Analysis is a package that runs within a Linux/Unix environment, although there are installs and executables for Mac and Windows too. Yang et al. 2011 originally developed it to determine the proportions of phenotypic variance of a complex trait that can be explained by genome-wide SNP variation. It was since expanded to and focused on many different and powerful GWAS applications, and it now supports a wide range of analyses, including relatedness and inbreeding, heritability and LD estimation, as well as more traditional population genetic statistics. The program takes binary PED files, generated by PLINK, as input files. As with many applications for GWAS, only biallelic SNPs are supported at the moment, and multiallelic SNPs are automatically ignored. "]]
