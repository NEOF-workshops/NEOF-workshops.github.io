[["01-Intro.html", "A taster session on bioinformatics for population genomics Chapter 1 Introduction Table of contents", " A taster session on bioinformatics for population genomics Helen Hipperson, Katy Maher, Victor Soria-Carrasco, Graeme Fox, Gavin Gouws 2024-12-17 Chapter 1 Introduction In this practical session we will introduce you to analysing SNP datasets for population genomic studies. Table of contents Background Cluster Sequence QC Mapping sequences to reference SNP and genotype calling Population structure This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Background.html", "Chapter 2 Background 2.1 Example data: Whole-genome resequencing of killer whales", " Chapter 2 Background In this workshop we will work through examples of analyses that can be performed using large datasets of genetic variation (single nucleotide polymorphisms - SNPs) obtained through next-generation sequencing. 2.1 Example data: Whole-genome resequencing of killer whales We will use whole-genome resequencing data from Foote et al. 2019. These data were generated to examine population genetic structure and admixture in killer whale populations from around the globe. A total of 56 individuals were analysed in the study - combining data sets of low-coverage whole-genome re-sequencing and RADseq. All the data were generated using Illumina sequencing. To allow us to run analyses in a timely manner for the workshop, we will initially perform QC and map a subset of sequencing reads from one individual, call and filter SNPs for three individuals, and perform some downstream analyses on 16 individuals. Here are some figures from the Foote et al. paper showing sampling locations and genetic structure using PCA plots and a plot of admixture proportions. We will generate similar plots in Chapter 7, although they will look different to these as we are using a subset of the data from this paper. "],["03-Cluster_Introduction.html", "Chapter 3 Cluster Introduction 3.1 Logon instructions 3.2 The Terminal Window 3.3 Accessing the example data", " Chapter 3 Cluster Introduction 3.1 Logon instructions For this workshop, we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent. You will now be in a logged-in Linux VNC desktop with two terminals. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance. If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal. These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Mint Linux The following link is a guide to install Mint Linux: https://linuxmint-installation-guide.readthedocs.io/en/latest/ 3.2 The Terminal Window In our case, the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences, it may look different. Already, there is useful information for us on the terminal window. nsc006: This is the login name, also known as the username. In this case, nsc006 is a demonstrator's account. Your screen should show a different account name, which will be your username for the Linux machine/cluster you are logged into. gauss03: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux OS and others '~' is a shortcut to the user's home directory. Everything after the '$' is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen and choose Terminal 3.3 Accessing the example data Before we can start, we first need to make a directory which will be used to contain all the files you generate throughout this workshop. To do this, type the following commands. #Change directory to home cd ~ #Make a directory in home called &quot;popgenomics&quot; mkdir popgenomics #Change directory to &quot;popgenomics&quot; cd popgenomics Then copy the example data for the workshop into your 'popgenomics' directory. These are the example files for all days of the workshop and will take several minutes to copy across. cp -r /pub14/tea/nsc206/NEOF/popgenomics/data/ . You will need to activate the popgenomics conda environment before continuing. Carry this out with the following command. Note: Ensure you include the dot and space (. ) at the start before usepopgenomics. . usepopgenomics You're now ready to start the analyses! "],["04-QC.html", "Chapter 4 QC Tutorials 4.1 Workshop data 4.2 Killer whale genomic dataset 4.3 Quality assessment 4.4 Quality control", " Chapter 4 QC Tutorials This tutorial will give hands on experience with quality control of genomic Illumina data. We will first look at the quality of the data 4.1 Workshop data Before we can carry on with the workshop, ensure you are in your working directory. cd ~/popgenomics Before running any examples, let us change directory to the raw fastq data directory. cd data/fastq/ 4.2 Killer whale genomic dataset This dataset contains an example fastq file from one individual (sample name SRR8861574) of a re-sequencing project on killer whales. The directory contains the forward (1) and reverse (2) read fastq files, and has been downsized to allow processing in a short amount of time for the workshop. List the contents of the current directory to confirm the two files are there. ls Have a look at the structure of the input fastq file: zcat SRR8861574_1_small.fastq.gz | head These fastq files are compressed (with the .gz file extension). zcat is a command for viewing the contents of compressed files, the pipe | symbol then passes this to head to display only the first few lines of the file. Fastq files contain a header line, the nucleotide sequence, and its corresponding quality scores. 4.3 Quality assessment We’ll run the raw sequence data through FastQC to summarise the data quality. First, make a directory for the output. mkdir raw_fastqc_output Now we can run FastQC. Note: The below command can be run over one line excluding the \\ Alternatively, you can type \\ during a command and then press the enter key. The next line on the command line will start with &gt;. This will allow you to continue typing the command on the line. This can be used to type one command over multiple lines. fastqc -o raw_fastqc_output \\ SRR8861574_1_small.fastq.gz SRR8861574_2_small.fastq.gz Normally, when you run a command it will run the process in the foreground. This means your command line will be locked whilst the process is running. You will only be able to type a new command in the terminal once the foreground process is finished. This is normally wanted but not always, for example when you run a process like firefox. To run a process in the background, so you can type and run new commands, you can add the symbol &amp; to the end of a command. We will use this for running firefox. Please follow this link for more info on foreground and background processing. Using firefox, have a look at the output html reports and answer the questions further below. To look at R1 fastQC output firefox raw_fastqc_output/SRR8861574_1_small_fastqc.html &amp; You may see a warning message on your terminal, but you can press enter to continue to type in the terminal. To look at R2 fastQC output firefox raw_fastqc_output/SRR8861574_2_small_fastqc.html &amp; How many total reads are there in both the files? 350000 355071 500000 565000 What is the length of the reads? 35-76 50-76 75-100 75-150 In the read 2 file, at what base position does the quality of all the reads go below 28 (i.e. the position where not all of the box plot is in the green)? 70-71 72-73 74-75 76 Are there any sequences with adapter content or overrepresented sequences? Yes No In this case the reads seem to be very high quality. This is likely as these reads are relatively short for an Illumina sequencing run. Longer sequences can be of poorer quality and may contain adapter sequence, especially towards the ends of the reads. Unfortunately, data from a sequencing machine will often not look so nice and will require quality trimming and filtering. For most datasets: The quality decreases towards the end of the reads The R2 reads have poorer quality than the R1 reads The read sizes have a range compared to all being one size. However, most of the reads are towards the long end of the range. Generally, even if data do look very nice we would carry out quality control to get rid of any poor data that is masked by the very good data and to remove any adapter sequences. 4.4 Quality control Quality control generally comes in two forms: Trimming: This is directly cutting off bits of sequence. This is typical in the form of trimming off low quality bases from the end of reads and trimming off adapters at the start of reads. Filtering: This occurs when entire reads are removed. A typical occurrence of this is when a read is too short as we do not want reads below a certain length. To carry this out, we are going to use Trimmomatic. To run Trimmomatic with the reads we will use the command below. trimmomatic \\ PE -phred33 \\ SRR8861574_1_small.fastq.gz SRR8861574_2_small.fastq.gz \\ SRR8861574_1_out_paired.fastq SRR8861574_1_out_unpaired.fastq \\ SRR8861574_2_out_paired.fastq SRR8861574_2_out_unpaired.fastq \\ ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 \\ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:30 MINLEN:50 The parameter meanings are: PE: Input data consists of paired-end reads. -phred33: Type of Phred quality encoding. In this case, it is phred33 as our fastq files use the Sanger encoding like most Illumina data. Next the raw input forward (1) and then reverse (2) read files for quality control are specified. On the next line, the output files for read 1 are specified, first for paired reads and second for unpaired reads. This comes about when one read from a pair is filtered out but the other one is not. We can normally ignore this file after the trimming. However, trimmomatic must have this value to run. Next the output files for read 2 are specified, first paired then unpaired. ILLUMINACLIP: These settings are used to find and remove Illumina adapters. First, a fasta file of known adapter sequences is given, followed by the number of mismatches allowed between the adapter and read sequence and then thresholds for how accurate the alignment is between the adapter and read sequence. LEADING: The minimum quality value required to keep a base at the start of the read. TRAILING: The minimum quality value required to keep a base at the end of the read. SLIDINGWINDOW: This specifies to scan the read quality over a 4 bp window, cutting when the average quality drops below 30. MINLEN: This specifies the minimum length of a read to keep; any reads shorter than 50 bp are discarded. Once the command is run, it will give you some useful output to the screen. Based on it, approximately what percentage of paired reads were kept? 95 83 71 11 4.4.1 Post Quality control check To see how successful the quality control has been, we need to run fastQC on the Trimmomatic output. First create a new directory for the FastQC output mkdir trim_fastqc_output Now run FastQC on the two paired read files fastqc -o trim_fastqc_output \\ SRR8861574_1_out_paired.fastq SRR8861574_2_out_paired.fastq We can open the FastQC output with the following commands firefox \\ trim_fastqc_output/SRR8861574_1_out_paired_fastqc.html &amp; firefox \\ trim_fastqc_output/SRR8861574_2_out_paired_fastqc.html &amp; To see how well the reads have improved, let’s answer the questions below and compare these to the answers of the raw read FastQC questions. How many total reads are there in both the files? 350000 355071 500000 565000 What is the length of the reads? 35-76 50-76 75-100 75-150 In the read 2 file, at what base position does the quality of all the reads go below 28 (i.e. the position where not all of the box plot is in the green)? 72-73 74-75 76 It does not What is the average GC content (%) of the reads? 40 43 45 50 Some things to note: The amount of reads has significantly decreased due to quality control. This is expected, but will need to be taken into account. If you do not have enough reads for the downstream analysis you will need to be less stringent on the trimming or filtering. What is the minimum number of reads required for downstream analysis? Unfortunately, this depends on the specific analysis at hand. For genome assemblies, the general rule of thumb is you want at least 20 times as many bases in the reads as would be in the final assembly (based on the genome size of the organisms). For calling SNPs, you need less coverage. R2 quality is still slightly worse than R1 quality towards the end of the reads. Again, this is normal. You could be more stringent but as previously mentioned you may be removing too many reads. "],["05-Mapping.html", "Chapter 5 Mapping sequences to the reference genome using BWA 5.1 Reference sequence preparation 5.2 Read mapping 5.3 Assessing mapping results", " Chapter 5 Mapping sequences to the reference genome using BWA BWA is a program for mapping sequences to a reference genome. Here we will use BWA to map our paired, trimmed sequences to the killer whale reference genome (GCF_000331955.2_Oorc_1.1_genomic.fna), and output a BAM (Binary sequence Alignment/Map format) file of the results. Further information on the reference genome, such as the sex of the sequenced individual, the technology and assembly programs used, the number of scaffolds/contigs and the sequence coverage can be found on the NCBI website page for this assembly. 5.1 Reference sequence preparation BWA uses indexing of the reference genome to speed up the mapping. The index needs to be prepared before we map our reads. As this takes some time, this has been run for you, but for information the command used to do this was: bwa index GCF_000331955.2_Oorc_1.1_genomic.fna This produces index files with the extensions '.sa', '.pac', '.ann', '.amb' and '.bwt' that will be automatically detected and used in the mapping step below. 5.2 Read mapping To map our paired-end, trimmed reads type the following: bwa mem \\ /pub14/tea/nsc206/NEOF/popgenomics/genome/GCF_000331955.2_Oorc_1.1_genomic.fna \\ SRR8861574_1_out_paired.fastq SRR8861574_2_out_paired.fastq | \\ samtools sort -o SRR8861574_paired.bam bwa mem is an alignment algorithm well suited to Illumina-length sequences. The default output is a SAM (Sequence Alignment Map format). However, here we pipe the output to samtools, a program for writing, viewing and manipulating alignment files, to sort and generate a BAM format, a binary, compressed version of SAM format. The mapping will take a few minutes to run, during which some output messages will be printed to the screen. When it has finished you will see the command prompt $ and flashing cursor again. We can view this file using samtools view and head to display only the first few lines: samtools view -h SRR8861574_paired.bam | head The header section consists of lines starting with '@'. In this case, we have a header line for each of the scaffolds in the killer whale reference genome with their name (SN) and length in bp (LN) given. To look at the information in the alignment section, we can leave out the -h option: samtools view SRR8861574_paired.bam | head Here is an example of one line, representing information from one sequence read. Each column contains information on where and how well the read aligns to the reference. After column 11 there can be several optional tags, often specific to the aligner used and can be useful for downstream analysis. The image above is from a different alignment file to the one you have generated here. On the first line of your alignment file what is the query sequence name? What is the reference contig name? In order to process the BAM files for further analysis, it is usually required that they are indexed. This can also be done with samtools: samtools index SRR8861574_paired.bam Use ls to view the contents of your directory - you should now have an index file with the extensions .bam.bai. 5.3 Assessing mapping results Lastly, we can use the samtools command flagstat to find information on how the reads mapped: samtools flagstat SRR8861574_paired.bam How many reads have mapped? 710696 710142 705371 687024 What percentage of reads have mapped? 90.32 96.74 99.25 100 This information is a summary of the FLAG values - the second field in the sam/bam file. These values can be used if you want to extract or exclude a specific set of reads for downstream analysis. For example, to exclude unmapped reads from our alignment file we can run: samtools view -F 4 -b SRR8861574_paired.bam &gt; SRR8861574_paired_mapped.bam The options used are: -F: only include reads without this flag -b: output in bam format Conversely, you can use -f to include the reads with the specified flag. The Broad Institute has a useful website to interpret the FLAG values, likewise they are listed in the samtools documentation. Run flagstat on the bam file from which we have excluded the unmapped reads. How many reads are in the filtered file (compare this to the unfiltered file)? 710696 705371 704817 687024 What proportion of the reads in this file are now mapped (compare this to the unfiltered file)? 90.32 96.74 99.25 100 In the next step, we will use alignment files from several samples to detect genetic variation. "],["06-SNPs.html", "Chapter 6 SNP and genotype calling using SAMtools and BCFtools 6.1 File preparation 6.2 Variant calling 6.3 Variant filtering", " Chapter 6 SNP and genotype calling using SAMtools and BCFtools BCFtools is a very popular program to call SNPs and genotypes (and also to manipulate and filter vcf/bcf files, as we will see afterwards). SNP calling is a relatively intensive process, so to speed things up we will be restricting variant calling to one scaffold from the killer whale genome (NW_004438415.1). We will call SNPs for three samples (SRR8861574, SRR8861575 and SRR8861576). The alignments have been provided for you in '.bam' format - this is a binary, compressed version of a sam file. First navigate to the 'aligned' directory: cd ~/popgenomics/data/aligned/ Check that you have bam files for the three samples and their corresponding index files using ls. 6.1 File preparation Before we start SNP calling, we will copy over the genome assembly file and index it using samtools: cp /pub14/tea/nsc206/NEOF/popgenomics/genome/GCF_000331955.2_Oorc_1.1_genomic.fna . samtools faidx GCF_000331955.2_Oorc_1.1_genomic.fna This will produce a .fai index file. Also, we need to make a list of the BAM files before we start: ls *.bam &gt; bamFiles.txt We will use this list file to call SNPs from all three samples rather than having to specify each sample name in the command. 6.2 Variant calling We will use BCFtools mpileup to produce genotype likelihoods for the three samples/BAM files, and pipe this output to BCFtools call to generate a file of genetic variants. Here is the BCFtools command: bcftools mpileup -Ou \\ --max-depth 10000 -q 20 -Q 20 \\ -P ILLUMINA -a FORMAT/DP,FORMAT/AD \\ -f GCF_000331955.2_Oorc_1.1_genomic.fna \\ -r NW_004438415.1 \\ -b bamFiles.txt | \\ bcftools call -m -v -P 1e-6 -f GQ \\ -O b -o orca_3samp_1scaf.bcf The options we are using are: mpileup -Ou: ouput an uncompressed bam file. This is the option to use when piping the output to another command for optimum performance/speed. --max-depth 10000: the maximum number of sequences considered per position -q 20: filter out alignments with mapping quality &lt;20 -Q 20: filter out bases with QS &lt; 20 -P ILLUMINA: use Illumina platform for indels -a FORMAT/DP,FORMAT/AD: output depth and allelic depth -f: specify the genome reference file, which must be faidx-indexed -r: restrict mpileup output to this specific region -b: list of input bam alignment files call -m: use the multiallelic caller -v: output variants only -P 1e-6: prior for expected substitution rate -f GQ: output genotype quality -O b: output in BCF format This command generates a '.bcf' file, which is a binary version of a '.vcf'. One of the most popular text-file formats for storing genetic variation information is the Variant Call Format (VCF). There are different versions of the format, but the core elements are the same. You can find a full description of the latest iteration here. Due to the large amount of data usually involved, files tend to be stored compressed as a BCF file. BCF is designed to be much more efficient to process and store large amounts of data. The practical downside is that the contents can only be accessed with bcftools view. The VCF format is composed of meta-information lines (prefixed wih ##), a header line (prefixed with #), and data lines each containing information about a position in the genome and genotype information on samples for each position (text fields separated by tabs). There are 8 fixed field lines, usually followed by an additional FORMAT field and an arbitrary number of sample fields when genotypes are included. Description of VCF fields Some notes about genotype fields: GT: Allele values are 0 for the reference and 1+ for alternate alleles. Alleles are separated with / if unphased and with | if phased. There are as many alleles separated by / or | as the ploidy. A . is used for missing data, i.e. when a genotype could not be called. More information on phasing is provided in the Appendix. GL or PL: Genotype likelihoods as floating-point log10-scaled or phred-scaled format. A diploid biallelic site has three values: homozygote for reference, heterozygote, and homozygote for alternate. Triallelic sites have 6 values: homozygote for reference, heterozygote of reference and alternate allele 1, homozygote for alternate allele 1, heterozygote of reference and alternate allele 2, heterozygote of alternate alleles 1 and 2, and homozygote for alternate allele 2. A graphical explanation of the different parts and fields can be found here. Let's have a look at our BCF file: bcftools view orca_3samp_1scaf.bcf | less -S The file starts with a few lines specifying the VCF and bcftools versions, the command used with bcftools mpileup, followed by a long list of the &gt;1,000 scaffolds that comprise the genome assembly (starting with ##contig), the descriptions of the fields for ALT, INFO and FORMAT, and any additional commands that were executed to produce this file (bcftools call in this case). To focus only on the header or the SNPs, the flags -h and -H can be used: bcftools view -h orca_3samp_1scaf.bcf | less -S bcftools view -H orca_3samp_1scaf.bcf | less -S Are all of the variants SNPs or are there also INDELs in the file? Only SNPs A good mix of SNPs and indels A few indels Are the variant quality scores (QUAL field) all high (&gt; 20)? Yes No Information in the BCF file can be used to filter to retain, for example, only biallelic SNPs, or to filter out variants with a low quality score or read depth. A list of the samples contained in the file can be obtained using bcftools query, and can be counted with wc: bcftools query -l orca_3samp_1scaf.bcf bcftools query -l orca_3samp_1scaf.bcf | wc -l Lastly, let's index the bcf file in order to be able to process it below: bcftools index orca_3samp_1scaf.bcf 6.3 Variant filtering The number of variants can be counted using bcftools view -H and wc, but it includes all kinds of variants. To check particular ones, we need to use the flags -v/-V (or --types/--exclude-types) to include or exclude certain variant: # all variants bcftools view -H orca_3samp_1scaf.bcf | wc -l # only SNPs bcftools view -H -v snps orca_3samp_1scaf.bcf | wc -l # only indels bcftools view -H -v indels orca_3samp_1scaf.bcf | wc -l Biallelic SNPs (with just 1 alternate allele) can be extracted with the -m and -M flags: bcftools view -H -v snps -m 2 -M 2 orca_3samp_1scaf.bcf | less -S -m 2 is the bcftools command that means “-m, –min-alleles INT: print sites with at least INT alleles listed in REF and ALT columns”. So here we are asking for all lines where there at least 2 REF and ALT alleles in total. -M 2 is the bcftools command that means “-M, –max-alleles INT: print sites with at most INT alleles listed in REF and ALT columns”. So here we are asking for all lines where there at most 2 REF and ALT alleles in total. 6.3.1 Extracting information Partial information can be extracted using bcftools query. In the example above, we saw how to get the list of samples using the -l option, but it can also be used to extract any fields using the -f option. For example, you can simply extract the list of positions with: bcftools query -f &#39;%POS\\n&#39; orca_3samp_1scaf.bcf | less -S Or combine multiple fields in the output, for example, scaffold/chromosome, position, reference and alternate alleles: bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT\\n&#39; orca_3samp_1scaf.bcf | less -S You can also extract genotypes (notice the position of the tab to avoid adding an extra tab at the end of the lines): bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT[\\t%GT]\\n&#39; \\ orca_3samp_1scaf.bcf | less -S Or genotype likelihoods: bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT[\\t%PL]\\n&#39; \\ orca_3samp_1scaf.bcf | less -S You can even combine this with awk, Perl or other tools to do simple calculations. For example, you can calculate allele frequencies from the AC and AN counts: bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT\\t%AC\\t%AN\\n&#39; \\ orca_3samp_1scaf.bcf | less -S bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT\\t%AC\\t%AN\\n&#39; \\ orca_3samp_1scaf.bcf | awk &#39;{print $1,$2,$3,$4,$5,$6,$5/$6}&#39; | less -S Or the mean genotype quality: # For all genotypes (uncalled ones = 0) bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT[\\t%GQ]\\n&#39; \\ orca_3samp_1scaf.bcf | \\ awk &#39;{SUM=0; N=0; for(i=5; i&lt;=NF; i++){N+=1; SUM+=$i}; \\ AVG=SUM/N; print $1,$2,$3,$4,AVG}&#39; | \\ less -S # Only for called genotypes: bcftools query -f &#39;%CHROM\\t%POS\\t%REF\\t%ALT[\\t%GQ]\\n&#39; \\ orca_3samp_1scaf.bcf | \\ awk &#39;{SUM=0; N=0; for(i=5; i&lt;=NF; i++){SUM+=$i; if ($i&gt;0) N+=1}; \\ AVG=SUM/N; print $1,$2,$3,$4,AVG}&#39; | less -S 6.3.2 Subsetting files We can subset SNPs within particular regions. For example, we can extract all the SNPs within a region on a particular chromosome/scaffold: bcftools view -H orca_3samp_1scaf.bcf NW_004438415.1:50000-100000 | wc -l bcftools view -H orca_3samp_1scaf.bcf NW_004438415.1:50000-100000 | less -S Subsetting a number of samples is also possible, for example to get only the first 2 samples: SAMPLES=$(bcftools query -l orca_3samp_1scaf.bcf | head -n 2 | tr &#39;\\n/&#39; &#39;,&#39;| sed &#39;s/,$//&#39;) bcftools view -s $SAMPLES orca_3samp_1scaf.bcf -O b -o orca_3samp_1scaf_first2.bcf bcftools index orca_3samp_1scaf_first2.bcf 6.3.3 Filtering Usually, we want to apply some filters depending on the sort of downstream analyses that we intend to do. Filtering usually consists of establishing some sort of threshold for the SNP quality in the QUAL field or some of the variables encoded in the INFO field (e.g. depth, mapping quality, etc.), but more sophisticated filtering is possible based on genotype-specific (i.e. per-sample) variables (e.g. discarding genotypes below a certain quality or with too few reads). Although it is possible to do soft-filtering of SNPs and simply tag them as PASSED or FAILED in the VCF/BCF file, it is more useful in practice to do hard-filtering and simply remove all those SNPs (and genotypes) that did not meet our thresholds. We are going to focus on the SNPs only, so let’s extract only the SNPs from all the variants: bcftools view -v snps orca_3samp_1scaf.bcf -O b &gt; orca_3samp_1scaf_snps.bcf bcftools index orca_3samp_1scaf_snps.bcf bcftools view -H orca_3samp_1scaf_snps.bcf | wc -l bcftools allows applying filters on many of its commands, but usually they are used with bcftools view or with bcftools filter. Filtering can be done using information encoded in the QUAL or INFO fields, also allowing expression with multiple conditions and basic arithmetics more details here. Here are some examples: SNP quality An obvious filter is to exclude (-e) SNPs below a quality threshold: bcftools view -e &#39;QUAL&lt;20&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.QS20.bcf Filters can also be specified as include (-i), the equivalent of the one above is: bcftools view -i &#39;QUAL&gt;=20&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.QS20i.bcf You can see they have the same number of SNPs: bcftools view -H orca_3samp_1scaf_snps.QS20.bcf | wc -l bcftools view -H orca_3samp_1scaf_snps.QS20i.bcf | wc -l SNP depth An example to exclude SNPs with depth &lt;10: bcftools view -e &#39;INFO/DP&lt;10&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.DP10.bcf # You can see this is quite a stringent filter for this subset of data: bcftools view -H orca_3samp_1scaf_snps.DP10.bcf | wc -l How many of the 267 SNPs have a sequencing depth of 10 or more reads? 2 3 8 29 An important note here is to be aware that if the file is further processed so that only part of the individuals are used, the fields in INFO may not be updated and it would be then unreliable to filter out using any information from that field. Mapping quality Another typical filter is to remove SNPs with low mapping quality (RMS MQ: root mean square of the mapping quality of reads across all samples). For examples, all SNPs with RMS MQ &lt; 40 can be discarded with the following command: bcftools view -e &#39;MQ&lt;40&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.MQ40.bcf How many SNPs has this removed? bcftools view -H orca_3samp_1scaf_snps.bcf | wc -l bcftools view -H orca_3samp_1scaf_snps.MQ40.bcf | wc -l Eleven of the SNPs had MQ&gt;40, and most have MQ=60: bcftools query -f &#39;%MQ\\n&#39; orca_3samp_1scaf_snps.bcf | sort | uniq -c Allele frequency For population genetic analyses, it is frequently advised to remove variants below a certain allele frequency, as these ones are difficult to tell apart from sequencing errors. For example, to exclude all SNPs with a minor allele frequency (MAF) below 5% we would run: bcftools view -e &#39;MAF&lt;0.05&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.MAF005.bcf How many SNPs have a minor allele frequency less than 5% (i.e. 0.05)? 8 29 257 259 Sample coverage Another typical situation is to want to exclude all SNPs for which only a small fracion of all the individuals have sequence data. That requires using the total number of alleles in the called genotypes (AN). In the case of a diploid species, this can be used to filter SNPs by number of individuals genotyped. For example, we can remove all SNPs genotyped for less than 3 individuals: bcftools view -e &#39;AN/2&lt;3&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.SAMP3.bcf How many SNPs have been called and genotyped all three of the samples/individuals? 3 8 10 29 Genotype-based filtering Filtering using the genotype fields can allow for some more precise filtering. For example, in cases of high depth heterogeneity among samples, it may be better to filter out by median genotype depth than by total depth across all samples. An example removing all SNPs where the mean genotype depth is below 3: bcftools view -e &#39;AVG(FMT/DP)&lt;3&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.MEANGTDP3.bcf How many SNPs have a mean genotyping depth of 3 or more? 3 8 10 29 Sometimes it is reasonable to ignore genotype calls based on few reads. The following command remove all genotype calls (i.e. genotypes are substituted by ./.) informed by less than 5 reads: bcftools filter -S . -e &#39;FMT/DP&lt;5&#39; -O b orca_3samp_1scaf_snps.bcf &gt; orca_3samp_1scaf_snps.NOGTDP5.bcf Combining multiple filters Multiple filters can be combined in a single command using or piping several ones. For example, we can combine a few of the filters we have used above: bcftools filter -S . -e &#39;FMT/DP&lt;3&#39; orca_3samp_1scaf_snps.bcf | \\ bcftools view -e &#39;AVG(FMT/DP)&lt;3 || QUAL&lt;20 || AN/2&lt;2&#39; -O b &gt; \\ orca_3samp_1scaf_snps.NOGTDP3.MEANGTDP3.Q20.SAMP2.bcf # This results in quite a dramatic reduction in the number of SNPs: bcftools view -H orca_3samp_1scaf_snps.NOGTDP3.MEANGTDP3.Q20.SAMP2.bcf | wc -l Final note: It is important to be careful with the order of the filters, as different combinations can result in different end results, especially if we subsample the individuals at some point. Also, some positions may end up being invariant or only variant with respect to the reference (i.e. private), or not having any individuals genotyped. We have also worked on a small bcf file with these examples - just one scaffold for three individuals. With larger amounts of data you will likely want to change some of the threshold values from those used here. "],["07-NGSAdmix.html", "Chapter 7 Population structure with NGSadmix and PCA 7.1 NGSadmix 7.2 Jupyter 7.3 Admixture plots 7.4 PCA", " Chapter 7 Population structure with NGSadmix and PCA 7.1 NGSadmix NGSadmix is a program (part of the package ANGSD) for inferring admixture from genotype likelihoods, thus taking into account the uncertainty inherent to NGS data. It is similar to the famous program STRUCTURE, but better suited for low-coverage NGS data. For those not familiar with these kind of approaches, these programs allow investigating population structure by inferring individual admixture proportions given a number of populations (K). This allows us to assign individuals to populations and identify migrants or admixed individuals. For this step, we will use an already prepared BCF file containing filtered genome-wide variants for 16 individual whales. First navigate to the 'structure' directory: cd ~/popgenomics/data/structure/ Check that you have the bcf file plus a text file of sample location information using ls. We then need to convert the bcf file to beagle format with angsd: mkdir ngsadmix angsd -vcf-pl \\ orca16_filtered.bcf \\ -fai ../aligned/GCF_000331955.2_Oorc_1.1_genomic.fna.fai \\ -doMaf 3 -nInd 16 \\ -domajorminor 1 -doglf 2 \\ -out ngsadmix/snps Now we need to run NGSadmix three times for K=2 to K=4. Each run will take a few minutes to complete. We will first run K=2: NGSadmix \\ -likes ngsadmix/snps.beagle.gz \\ -K 2 -seed 1 -P 4 \\ -o ngsadmix/snps_K2 Repeat for K=3 and K=4. We can now look at the '.qopt' files that contain the admixture proportions: head ngsadmix/snps_K2.qopt You should get something that looks like this, which represents the admixture proportions for each of the K populations (here K=2) for the first 10 samples: 0.99999982830453515437 0.00000017169546480430 0.74766154471069756760 0.25233845528930237689 0.99999999900000002828 0.00000000100000000000 0.99999999900000002828 0.00000000100000000000 0.76116119088545863836 0.23883880911454133389 0.53927021410032682791 0.46072978589967317209 0.57937878456323999021 0.42062121543676006530 0.00000000100000000000 0.99999999900000002828 0.00000000100000000000 0.99999999900000002828 0.99999999900000002828 0.00000000100000000000 Now we are going to visualize the admixture estimates using 'R'. We will need some more information about the samples to produce meaningful plots. A file with information about the sampling location of each individual can be found in your \"structure\" directory. Both the population and ocean each sample is from is listed. head sample_location.tsv We will be using Jupyter-notebook to run our R code and generate some plots for the admixture inferences when K=2. 7.2 Jupyter Jupyter-notebook is a nice browser-based method to write, edit, and run code. It was initally created for Python coding, but has since branched out to many other languages, such as R. We are using it in this workshop for a variety of its properties: It is popular and well maintained. It is lightweight. Other heavier weight programs, such as RStudio, would struggle in our HPC due to the graphical and CPU load. It is interactive and displays code output. It allows for easier annotation, editing, and debugging than the command line. It provides a graphical interface for changing directories and choosing files. Before carrying out any analysis, we will go through a quick tutorial of jupyter-notebook. 7.2.1 Open Jupyter-notebook The first step is to open jupyter-notebook. Run the below command in your (popgenomics) environment. jupyter-notebook This will open jupyter-notebook in firefox. We won't need to access the linux terminal anymore. Leave the terminal running jupyter-notebook and full screen your firefox. You should see something like below. 7.2.2 Create R notebook The next step is to create a R notebook. Click on the \"New\" button towards the top right, right of the \"Upload\" button. From the dropdown click \"R\" below \"Python 3 (ipykernel)\". This will open up a new R notebook like below. 7.2.3 Cells and code Jupyter-notebook uses cells (the gray boxes) to separate code. This is very useful to compartmentalise our code. There will already be one cell. Within the cell, type in the below commands. 1+1 2-3 When pressing enter in cells it will create a new line. To run all commands in a cell press CTRL + enter. Run your current cell and you should see something like below. 7.2.4 Create new cells You can create new cells by 2 different means. Press the + button on the tool bar (between the floppy disk and scissors ). This will add a cell below your currently selected cell. Click on the Insert button and use the dropdown to add a cell above or below your currently selected cell. Tip: Hover over the toolbar icons to display a text based description of its function. With that knowledge add a second cell below the first cell. Add the following code to your second cell but do not run it. num_1 &lt;- 3 num_2 &lt;- 10 Tip: Notice there are green lines around your selected cell. Insert a third cell and add the following code to it. Do not run the code. num_1 * num_2 7.2.5 Running code Try to run the code in the third cell. There should be an error as we have not created the objects num_1 &amp; num_2. We have only written the code for these objects but not run them. We can run all the code in a notebook starting from the first cell to the last cell. To run all cells from the start: Click on the \"Cell\" button. Click \"Run All\" from the drop-down options. You should then see something like the below in your notebook. There is no output printed for cell 2 because we are assigning variables. However, the correct output for Cell 3 is below it. This is because the variables were assigned in cell 2 before cell 3 was run. 7.2.6 Saving the file As with RStudio and other good coding interfaces, we can save our notebook. First, we should rename the file. Rename the notebook to \"jupyter_tut\": Click on the name of the notebook, currently called \"Untitled\". This is at the very top of the notebook, right of the Jupyter logo. A pop-up called \"Rename Notebook\" will appear. Change the Name to \"jupyter_tut\". Click \"Rename\". Now we can save the file. Two methods to save are: Click the floppy disk on the toolbar. Click on the \"File\" button. Click \"Save and Checkpoint\" from the dropdown options. 7.2.7 Title cells with markdown We will be using multiple notebooks in this workshop. We will also have multiple sections per notebook. It will be useful to create header cells with markdown to create visual separation of the different sections. To add a header cell to the top of our notebook: Create a new cell at the top of the notebook. Click on the \"Code\" drop down and select \"Markdown\". The \"Heading\" option no longer works. Add the following to the \"Markdown\" cell to create a first level header. Ensure you have a space between the # and header text (\"Tutorial\"). # Tutorial Great, we can now add nice headers in our notebooks. Save the notebook once more before carrying on to the next section. Markdown You won't need to know more about Markdown but if you are interested please see the Markdown guide. 7.2.8 Close the notebook To close the notebook: Click on \"File\". From the dropdown options click \"Close and Halt\". When you are back in the file explorer page, you may not yet see the new file you saved. If so, you will need to refresh the page with the Refresh button towards the top right. With that quick tutorial of jupyter-notebook, we can start visualizing our admixture plots. 7.2.9 Video tutorial 7.3 Admixture plots As mentioned above, we will be using R to generate some plots for the admixture inferences when K=2 within Jupyter Notebooks. Create a new notebook called \"Chp07a-Admixture\". - Add a markdown cell with the first level header: # Admixture analysis Now let’s load the admixture proportions, the information about the individuals, and let’s do some plotting. Remember this is typed into a code cell and then run. Please create your own coding and markdown cells where you think appropriate. # Load admixture proportions admix&lt;-t(as.matrix(read.table(&quot;ngsadmix/snps_K2.qopt&quot;))) # Load info about samples id.info&lt;-read.table(&quot;sample_location.tsv&quot;, sep=&quot;\\t&quot;, header=T) # Palette for plotting mypal&lt;-c(&quot;#E41A1C&quot;,&quot;#377EB8&quot;,&quot;#4DAF4A&quot;,&quot;#984EA3&quot;) # sort by sampling area and plot admix&lt;-admix[,order(id.info$area)] id.info&lt;-id.info[order(id.info$area),] barplot(admix,col=mypal,space=0,border=NA,xlab=&quot;Samples&quot;,ylab=&quot;admixture&quot;) text(tapply(1:nrow(id.info),id.info$area,min),-0.05,unique(id.info$area),xpd=T, srt=-45, adj=0.2) This generates a plot for K=2 which should look something like this: Do individuals appear to be genetically structured in relation to the ocean they are sampled in? Plot the admixture proportions for K=3. Can you infer anything further than with the K=2 plot? There appears to be some similarities between, for example, Australasian and Pacific samples. However, without knowing the exact sampling locations, as you hopefully would for your own study!, it is hard to interpret these plots robustly, but hopefully this code will be useful for making your own structure plots from your SNP datasets. Important: You may notice the results vary across runs. This happens when the maximum-likelihood searches gets stuck in different local optima. In production runs, it is recommended to run multiple searches for each K and check the likelihoods. 7.4 PCA Next we are going to carry out PCA using R. For that, we have converted our BCF file to mean genotype format (based on the BIMBAM format), where genotypes are encoded as mean genotypes. A mean genotype is a value between 0 and 2 that can be interpreted as the minor allele dosage: 0 is homozygous for the major allele, 1 is a heterozygote, and 2 is a homozygote for the minor allele. Although genotype calls can be used, it is also possible to use genotype likelihoods to get intermediate values that incorporate uncertainty in genotype calling. We have already done this for you for the purposes of the course. First navigate to the 'pca' directory within Jupyter. We will use R within Jupyter to perform the PCA with the mean genotypes. Create a new Jupyter notebook called \"Chp07b-PCA\". - Add a markdown cell with the first level header: # PCA analysis First, let’s load the genotypes in a new markdown cell: ## load genotypes genotypes&lt;-read.table(&quot;orca16_filtered.bbgeno&quot;,header=T, check.names=F) head(genotypes) We can now do a the PCA with the genotype matrix, excluding the first 3 columns that contain information for SNP id, reference allele and alternate allele: pca.genotypes&lt;-prcomp(t(genotypes[,-(1:3)]), center=TRUE, scale=FALSE) Then we can have a look at the proportion of variance explained by each PC: summary(pca.genotypes) As you can see, ~30% of the variance is explained by the first 4 PCs. We will extract the PCs vectors and plot them by pairs (i.e. PC1xPC2, PC2xPC3, PC3xPC4): # Get PCs pcs&lt;-pca.genotypes$x # Load info about samples id.info&lt;-read.table(&quot;sample_location.tsv&quot;, sep=&quot;\\t&quot;, header=T) # sort as in the pcs id.info&lt;-id.info[match(rownames(pcs), id.info$run_accession),] # colours id.colours&lt;-as.character(id.info$area) id.colours[id.colours==&quot;Atlantic_Ocean&quot;]&lt;-&quot;red&quot; # Atlantic as red id.colours[id.colours==&quot;Australasia&quot;]&lt;-&quot;blue&quot; # Australasia as blue id.colours[id.colours==&quot;Indian_Ocean&quot;]&lt;-&quot;green&quot; # Indian as green id.colours[id.colours==&quot;Pacific_Ocean&quot;]&lt;-&quot;orange&quot; # Pacific as orange id.colours[id.colours==&quot;Southern_Ocean&quot;]&lt;-&quot;purple&quot; # Southern as purple plot(pcs[,1], pcs[,2], main = &quot;PCA using genotype matrix&quot;, xlab = &quot;PC1&quot;, ylab = &quot;PC2&quot;, col=id.colours, pch=16) plot(pcs[,2], pcs[,3], main = &quot;PCA using genotype matrix&quot;, xlab = &quot;PC2&quot;, ylab = &quot;PC3&quot;, col=id.colours, pch=16) plot(pcs[,3], pcs[,4], main = &quot;PCA using genotype matrix&quot;, xlab = &quot;PC3&quot;, ylab = &quot;PC4&quot;, col=id.colours, pch=16) The first plot should look like this: There is some structure evident: PC1 separates out the Australasian and Pacific Ocean samples, the Atlantic Ocean samples are split, and the Indian Ocean sample does not cluster closely with any other. In reality, you would want more specific information about the sampling locations, and ideally more than 16 samples! However, hopefully this code will be useful in generating your own PCA plots. "],["08-Appendix.html", "A Mamba installs A.1 Mamba installation and environment B Jupyter-notebook C Using loops to process multiple samples C.1 Simple loop example C.2 Trimmomatic loop example", " A Mamba installs A.1 Mamba installation and environment Mamba is a reimplementation of conda. It is a great tool for installing bioinformatic packages including R packages. Mamba github: https://github.com/mamba-org/mamba The best way to use Mamba is to install Miniforge. It has both Conda and Mamba commands. Miniforge installation: https://github.com/conda-forge/miniforge Mamba guide: https://mamba.readthedocs.io/en/latest/user_guide/mamba.html To create the mamba environment popgenomics run the below commands in your bash. You will need to have installed mamba first. #R community mamba create -n popgenomics mamba activate popgenomics #Install packages from bioconda channel #NGSadmix is part of angsd mamba install -c bioconda plink fastqc trimmomatic bwa samtools bcftools angsd r-popgenome gemma B Jupyter-notebook If you are running this on your own computer you can use RStudio for R. However, you can also use Jupyter-notebook if you are using an HPC or prefer it. If using bash you will need to create an environment with Jupyter-notebook. Ensure you are in the (base) mamba environment. mamba create -n jupyter mamba activate jupyter mamba install -c anaconda jupyter mamba deactivate To run Jupyter-notebook with your r_community environment you can run the following. #Activate you R_community env mamba activate r_community #Run jupyter-notebook (may be a slightly different path) ~/mamba/envs/jupyter/bin/jupyter-notebook C Using loops to process multiple samples In our main workflow we learnt how to run the QC and alignment steps using a single sample. You may be wondering how you would run these initial steps when you have many samples. Ideally you don't want to have to run the same command multiple times - this is a recipe for typos and errors creeping into your code as well as being tedious and time consuming. One approach to get around this is to run a loop which will cycle through each of your samples running the task you specify. Put simply the syntax for a for loop is as follows: for &lt;variable name&gt; in &lt;a list of items&gt;; do &lt;run a command&gt; $&lt;variable name&gt;; done The &lt;variable name&gt; is the name of the variable you will use in the do part of your loop. It contains the item in your list the loop is currently using at that point. I.e. in the simple loop example (below) we call this variable f but you could use any name as long as you use it consistently in your do section. The &lt;a list of items&gt; is anything that returns a list of items, e.g. a list of files in our case. C.1 Simple loop example Below is an example of a loop, which can be adjusted for your samples and the software you are running. This simplified loop will clearly demonstrate what the first part of the code is doing. Make sure you are in the directory ~/popgenomics/data/fastq/ and in a terminal window (not in Jupyter). for f in *_1_small.fastq.gz; #the following will loop through all files ending _1_small.fastq.gz do BASE=${f%_1_small.fastq.gz} #assign the file name &#39;$f&#39; to a variable called &#39;BASE&#39; but remove the extension &#39;_1_small.fastq.gz&#39; echo $f #print $f to screen echo ${BASE}; #print ${BASE} to screen done By using echo to print the following variables to the console we can see: echo $f: prints the full filename. echo ${BASE}: prints just the start of the filename (minus the _1_small.fastq.gz extension as we specified in our code) The loop then repeats the commands on the next file in the list (in this case we only have one file) and will keep going until it has run through all files ending with _1_small.fastq.gz in the given directory. done specifies the end of the commands in the loop. All commands between do and done in your loop will be executed. In this case we only have two files but this would work with any number of files. This is a useful bit of code as it means we can keep the filename but change the ending/extension of the file in our loops. C.2 Trimmomatic loop example Now we can write a loop to run trimmomatic starting with the code we used above as a base. for f in *_1_small.fastq.gz; #the following will loop through all files ending _1_small.fastq.gz do BASE=${f%_1_small.fastq.gz} #assign the file name &#39;$f&#39; to a variable called &#39;BASE&#39; but remove the extension &#39;_1_small.fastq.gz&#39; trimmomatic PE -threads 4 -phred33 \\ #call trimmomatic and specify parameters ${BASE}_1_small.fastq.gz \\ #specify input R1 extension ${BASE}_2_small.fastq.gz \\ #specify input R2 extension trimmed/${BASE}_1_out_paired.fastq \\ #specify directory and filename for output of paired R1 read trimmed/${BASE}_1_out_unpaired.fastq \\ #specify directory and filename for output of unpaired R1 read trimmed/${BASE}_2_out_paired.fastq \\ #specify directory and filename for output of paired R2 read trimmed/${BASE}_2_out_unpaired.fastq \\ #specify directory and filename for output of unpaired R2 read ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:12 SLIDINGWINDOW:4:30 MINLEN:80; #further paramenters for trimmomatic done #end of commands You can then edit this code to run other software in a loop. For a nice basic intro to Linux loops please click here. "]]
