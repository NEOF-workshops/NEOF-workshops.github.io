[["01-Shotgun_metagenomics.html", "Shotgun Metagenomics Chapter 1 Introduction Table of contents", " Shotgun Metagenomics Sam Haldenby and Matthew R. Gemmell 2026-02-27 Chapter 1 Introduction There are many unknown and unculturable microbes found in a vast array of different environments. Shotgun metagenomics is an approach to capture all the genetic information in a sample, providing the taxonomic and metabolic information of all present organisms. In this course we will analyse shotgun metagenomic sequencing data from stool samples to compare Western and Korean diets. Sessions will start with a brief presentation followed by self-paced computer practicals guided by this online interactive book. This book will contain theory and practice code. Multiple choice questions will guide the interpretation of results. At the end of the course learners will be able to: Carry out quality control of sequencing data, including host removal. Explain how Kraken2 utilises k-mers to taxonomically classify sequence data. Quantify taxonomic composition of shotgun metagenomic data with Kraken2 &amp; Bracken. View taxonomic abundances with Krona. Utilise LEfSe for biomarker detection. Profile the microbial pathways in metagenomic sequencing data with the bioBakery suite of tools, including HUMAnN. Perform a metagenome assembly with MEGAHIT, and assess it with QUAST. Carry out genomic binning of the metagenome assembly with MetaBAT2 to try to separate the different species present into Metagenome Assembled Genomes (MAGs). Assess the quality, completeness, and contamination levels of MAGs with CoCoPyE. Functional annotation of the bins with Bakta. Table of contents Overview Raw data Trimming data Host removal Taxonomic profiling Functional profiling Metagenome assembly Binning Functional annotation Appendix This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Overview.html", "Chapter 2 Overview 2.1 What is metagenomics? 2.2 Why metagenomics? 2.3 Metagenomics vs Metagenetics 2.4 Tutorial overview", " Chapter 2 Overview 2.1 What is metagenomics? Meta /ˈmɛtə/ : prefix meaning “higher” or “beyond” Metagenomics is the study of genes and genetic material recovered from environmental samples (whether from the sea, soil, human gut, or anywhere else you can imagine). Unlike genomics, metagenomics deals with a multitude of usually diverse species rather than focussing on a single species/genome. 2.2 Why metagenomics? Microbes exist virtually everywhere on Earth, even in some of the most seemingly hostile environments. Every process on our planet is influenced in some way by the actions of microbes, and all higher organisms are intrinsically associated with microbial communities. While much can be learned from studying the genome of a single microbial species in isolation, it does not provide us with any information regarding that species' neighbours, i.e. what else is in its natural environment? Metagenomics offers a top-down approach which allows researchers to investigate and understand interactions between species in different environments, thus providing a much broader and complete picture. 2.3 Metagenomics vs Metagenetics Broadly speaking, there are two families of metagenomic analysis: Amplicon-based: This utilises sequencing data generated from amplified marker sequences, for example, regions of the 16S rRNA. Sequences are clustered together and taxonomically assigned to estimate the species abundance in a sample. This is sometimes referred to as metagenetics, as it does not consist of any genomic analysis beyond the marker gene regions. Shotgun: This utilises sequencing data generated from random fragments from total genomic DNA from environmental samples, rather than targeting specific genes. This approach allows for not only species abundance determination but direct functional analysis, too, due to having information on a wide range of genetic data sampled from the population. This is sometimes referred to as metagenomics, as it involves genome-wide analyses. Shotgun metagenomics is the focus of this practical session. 2.4 Tutorial overview 2.4.1 Basics This tutorial and practical session focuses on performing a range of metagenomic analyses using shotgun sequence data from the Illumina platforms. The analyses discussed here are by no means exhaustive and are instead intended to provide a sample of what can be done with a metagenomic dataset. 2.4.2 Structure We prefer to allow people to work at a pace that they are comfortable with rather than ensuring that everyone is at the same point of the tutorial at the same time. There will be no instructor telling you what to type and click. Instead, everything you require to carry out the practical is written in this document. Take your time; it's important to spend some time understanding why you are running the commands, rather than simply typing them out. If at any point you are having trouble or have a question, let one of us know and we'll provide 1-to-1 assistance. 2.4.3 Content This practical is broken up into the following broad sections. Raw data: We will first link to a dataset that we have downloaded for this tutorial. We will take a quick look at what the sequence files look like and briefly discuss the origin of the samples. Trimming data: This entails preprocessing our data to ensure that it is of good quality. Host removal: When sequencing the genomic content of a host's microbiota (bacteriome, archaeome, mycobiome, and more) it is likely you will also sequence the host's genome. This step shows a method of removing possible host contamination. Taxonomic profiling: We will analyse the dataset to determine the species abundance in each sample. Following this, we will visualise the data and compare the samples. Functional profiling: We will analyse the dataset to determine the pathway abundance and completeness in each sample. Following this, we will visualise the data and compare the samples. Metagenome assembly: Here, we will move away from just analysing the reads directly and will assemble the metagenome into contigs. Prior to this, we will 'stitch' the reads together to ensure we get the best assembly possible. Binning: This step attempts to seperate each assembled genomes into bins. These genome assemblies are called Metagenome-assembled Genomes (MAGs). Functional annotation: We will take our MAGs, predict genes and then functionally annotate them with MetaCyc. "],["03-Cluster_Introduction.html", "Chapter 3 Cluster Introduction 3.1 Logon instructions 3.2 The Terminal Window", " Chapter 3 Cluster Introduction 3.1 Logon instructions For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent. You will now be in a logged-in Linux VNC desktop with two terminals. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance. If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal. How to zoom with your browser You may need to zoom out with your browser so you can see the full webVNC window. Chrome: Click on the three dots in vertical line ( ) on the top left for a dropdown menu which includes zoom options. Edge: Click on the three horizontal lines ( ) on the top left for a dropdown menu which includes zoom options. Firefox: Click on the three dots in horizontal line ( ) on the top left for a dropdown menu which includes zoom options. These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Mint Linux The following link is a guide to install Mint Linux: https://linuxmint-installation-guide.readthedocs.io/en/latest/ If you have a windows machine and would like to install linux on it you can install Windows Subsystem for Linux (WSL). The following link is a guide to install linux via (WSL): https://learn.microsoft.com/en-us/windows/wsl/install 3.2 The Terminal Window In our case the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences it may look different. Already there is useful information for us on the terminal window. nsc206: This is the login name, also known as the username. In this case nsc206 is a demonstrator's account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into. ada02: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux (OS) and others '~' is a shortcut to the user's home directory. Everything after the '$' is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen, choose Terminal. "],["04-Start_conda.html", "Chapter 4 Startup &amp; mamba", " Chapter 4 Startup &amp; mamba During this practical you will use a number of installed programs and scripts. To ensure that the system knows where to look for the scripts, run the following command (ensure this starts with a full stop and a space .): . useshotgun The use scripts in this workshop are custom scripts that set up mamba environments. You can look at the above script with less /usr/local/bin/useshotgun if you are interested in its contents. Also, there’s a chance you’re currently not in your home directory, so let’s make sure you are with the following command: cd ~ "],["05-Raw_data.html", "Chapter 5 Raw data 5.1 Obtaining the data 5.2 Checking quality control", " Chapter 5 Raw data The very first thing we need to do is to obtain a dataset to work with. The European Bioinformatics Institute (EBI) provides an excellent metagenomics resource (https://www.ebi.ac.uk/metagenomics/) which allows users to download publicly available metagenomic and metagenetic datasets. Have a browse of some of the projects by selecting one of the biomes on the website. We have selected a dataset from this site that consists of DNA shotgun data generated from 24 human faecal samples. 12 of these samples are from subjects who were fed a Western diet and 12 are from subjects who were fed a Korean diet. This dataset comes from the EBI metagenomics resource (https://www.ebi.ac.uk/metagenomics/projects/ERP005558). 5.1 Obtaining the data First, we need to create a directory to put the data in and then change directory to it. mkdir 1-Raw cd 1-Raw Now we can generate a symbolic links (i.e. shortcut) to the raw sequence data files, which will appear in the current directory: ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/raw_fastq/* . If you would like to know more about the ln command please check out: https://linuxize.com/post/how-to-create-symbolic-links-in-linux-using-the-ln-command/. Check the symbolic links are in your current directory: ls There should be six files in the directory, two for each sample in the dataset. e.g. K1_R1.fastq.gz The file ID has three components: K1 is the sample ID. R1 is for the forward reads in the Illumina reads pair (R2 is for the set corresponding to the other end of the reads). fastq.gz tells us that this is a gzipped FASTQ file. The three samples are: K1: Fecal sample of individual of Korean diets K2: Fecal sample of individual of Korean diets W1: Fecal sample of individual of Western diets So, what do the R1 and R2 actually mean? With Illumina sequencing the vast majority of sequencing is paired end. i.e. DNA is first fragmented and both ends of each fragment are sequenced as shown here: This results in two sequences generated for each sequenced fragment: One reading in from the 3' end (R1) and the other reading in from the 5' end (R2). FASTQ is a sequence format much like FASTA, with the addition of quality scores. To see what a FASTQ file looks like, we can inspect the first few lines on one of our sequence files: zcat K1_R1.fastq.gz | head -n 4 | less -S The pipe symbol ( | ) is used to pass the output of one command as input to the next command. So, this command: Shows the unzipped contents of the FASTQ file Displays only the first 4 lines Displays them without wrapping lines (with –S, for easy viewing) The lines displayed represent one FASTQ sequence entry, or one read of a read pair: The corresponding second read can be viewed by running the same command on K1_R2.fastq.gz. The first line is the read identifier, the second line is the sequence itself, the third line is a secondary header (which is usually left blank except for '+') and the fourth line is the sequence quality score: For each base in the sequence, there is a corresponding quality encoded in this string of characters. To return to the command prompt, press q. Due to computational constraints, the files you have linked to are a subset of the original data (i.e. 1 million read pairs from each sample). 5.2 Checking quality control We can generate and visualise various sequence data metrics for quality control purposes using FastQC. We will run FastQC on the R1 and R2 reads separately as it is good to visualise them in two different reports. This is because R1 and R2 reads have different quality patterns, generally due to the poorer quality of R2. Run FastQC on the files: #R1 fastqc #Make an output directory mkdir R1_fastqc #Run fastqc on all the R1.fastq.gz files #* matches any pattern #*R1.fastq.gz matches any file that ends R1.fastq.gz in the current directory #-t 3 indicates to use 3 threads, chosen as there are three R1 files fastqc -t 3 -o R1_fastqc *R1.fastq.gz #R2 fastqc #Make output directory mkdir R2_fastqc #Run fastqc fastqc -t 3 -o R2_fastqc *R2.fastq.gz Once the FastQC commands are run we can run MultiQC to create interactive html reports for the outputs. #R1 multiqc fastqc report #Create output directory mkdir R1_fastqc/multiqc #Create multiqc output multiqc -o R1_fastqc/multiqc R1_fastqc #R2 multiqc fastqc report #Create output directory mkdir R2_fastqc/multiqc #Create multiqc report multiqc -o R2_fastqc/multiqc R2_fastqc Once completed, view the MultiQC reports (NB: The &amp; runs the command in the background, therefore allowing you to continue to run commands while Firefox is still open). This is a longer command so we've split it across multiple lines with bash escape. A \\ at the end of a line allows you to press return/enter without running the command, meaning you can continue to add to that command. When this happens, the $ changes to a &gt;. For more information please see our Intro to Unix materials Note if you do use the \\ character, the next key you press must be return/enter. If you use \\ in the middle of a line without pressing return afterwards, it will break the command! firefox R1_fastqc/multiqc/multiqc_report.html \\ R2_fastqc/multiqc/multiqc_report.html &amp; The FastQC report (via MultiQC) contains a number of metrics. The \"Sequence Quality Histograms\" shows the sequence quality across the length of the reads, you can hover over each line to show which sample it belongs to. Note how quality decreases as the length of the read increases. While this is normal with Illumina sequencing, we will improve the situation a bit in the next chapter. For more information on the plots of FactQC please see this online resource. Once you have finished inspecting, minimise the Firefox window. "],["06-Trimming_data.html", "Chapter 6 Quality control 6.1 Removing adapters and low quality bases 6.2 Rename the files 6.3 Inspect the trimmed data", " Chapter 6 Quality control Now that we've obtained the raw data and had a look at it, we should clean it up. With any sequencing data, it is very important to ensure that you use the highest quality data possible: Rubbish goes in, rubbish comes out. There are two main methods employed to clean sequence data, and a third method specific to some metagenomic datasets. Remove low quality bases from the end of the reads: These are more likely to be incorrect, so are best trimmed off. Remove adapters: Sometimes sequencing adapters can be sequenced if the sequencing runs off the end of a fragment. Host removal: If a metagenomic sample derives from a host species then it may be advisable to remove any reads associated with the host genome. 6.1 Removing adapters and low quality bases Go back to your home directory and create a new directory where we will clean the sequences up: cd .. mkdir 2-Trimmed cd 2-Trimmed You are now in your newly created directory. Here we will run Trim Galore! which removes low quality bases and adapters. trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/K1_R1.fastq.gz ../1-Raw/K1_R2.fastq.gz This command will remove any low quality regions from the end of both reads in each read pair (quality score &lt; 20). Additionally, if it detects four or more bases of a sequencing adapter, it will trim that off too. Task: Rerun this command for the other two samples (K2 and W1). Try to run these without looking at the help box below. trim_galore commands #K2 trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/K2_R1.fastq.gz ../1-Raw/K2_R2.fastq.gz #W1 trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/W1_R1.fastq.gz ../1-Raw/W1_R2.fastq.gz 6.2 Rename the files Once that is complete list the contents of your directory: ls You will notice that we have a new bunch of files created: 2 new read files for each sample along with a trimming report for each file trimmed. However, the new names are needlessly long. For example K1_R1_val_1.fq.gz could be shortened to K1_R1.fq.gz. So, we'll rename all of the files with the mv command: mv K1_R1_val_1.fq.gz K1_R1.fq.gz mv K1_R2_val_2.fq.gz K1_R2.fq.gz mv K2_R1_val_1.fq.gz K2_R1.fq.gz mv K2_R2_val_2.fq.gz K2_R2.fq.gz mv W1_R1_val_1.fq.gz W1_R1.fq.gz mv W1_R2_val_2.fq.gz W1_R2.fq.gz Tip: If you want to edit and reuse previous commands, press the up arrow key. Task: Briefly inspect the log files to see how the trimming went (e.g. K1_R1.fastq.gz_trimming_report.txt). 6.3 Inspect the trimmed data To see what difference the trimming made, run FastQC and MultiQC again on the trimmed output files and view it. #R1 fastqc and multiqc mkdir R1_fastqc fastqc -t 3 -o R1_fastqc *R1.fq.gz mkdir R1_fastqc/multiqc multiqc -o R1_fastqc/multiqc R1_fastqc Task Run FastQC and MultiQC for the R2 files and then view the R1 and R2 MultiQC reports with firefox. Try to run the commands without looking at the help box below. How does the quality compare to the untrimmed data? R2 commands #R2 fastqc and multiqc mkdir R2_fastqc fastqc -t 3 -o R2_fastqc *R2.fq.gz mkdir R2_fastqc/multiqc multiqc -o R2_fastqc/multiqc R2_fastqc "],["07-Host_removal.html", "Chapter 7 Host removal 7.1 Index reference 7.2 Alignment 7.3 Unmapped read extraction 7.4 Re-pair 7.5 Host removal summary", " Chapter 7 Host removal It is good practice to remove any host sequences from your data before further analysis. A good method for this is to align/map your reads to a reference of your host genome and remove the mapped sequences (i.e sequences we believe belong to the host). If there is no host genome available before you start your sample collections and sequencing it may be a good idea to attempt to sequence and assemble the host genome. We would recommend long read technologies for single genome assembly projects. This chapter contains a small example on how to carry out host removal. It uses only a section of a human (host of our samples) reference genome assembly. In real life you should use the entire reference. The first step is to copy over the reference fasta file we will use. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/GRCh38_slice.fasta . 7.1 Index reference We will use the Bowtie2 aligner for mapping/aligning. Prior to alignment/mapping we need to index our reference. bowtie2-build GRCh38_slice.fasta GRCh38_slice.fasta If you use ls you will now see a bunch of files starting with GRCh38_slice.fasta and ending with various suffixes that contain bt2. These are the index files which allow us to use the reference with Bowtie2. 7.2 Alignment With the indexed reference we will align the K1 reads to the reference. This creates a BAM file that contains alignment and read information (K1_mapped.bam). bowtie2 -x GRCh38_slice.fasta -1 K1_R1.fq.gz -2 K1_R2.fq.gz \\ -p12 2&gt; K1_bowtie2_out.log | samtools view -b -S -h &gt; K1_mapped.bam Parameters This command is split into two commands. The first is bowtie2 that creates the alignment. The parameters for the command are: -x: Indexed reference the reads will be aligned to. -1: The forward reads. -2: The reverse reads. -p: Number of threads to be used. 12 in this case. 2&gt;: This will cause the standard error to be redirected to the chosen file. In this case K1_bowtie2_out.log. This is useful for commands that may produce a lot of output to screen. If an error occurs you can view this file to see the error messages. The alignment is then piped (|) to the command samtools view. For more information on pipes please see our Intro to Unix course book. The parameters for samtools view are: -b: Output the alignment as a BAM file. BAM files are a binary form of SAM files so they are smaller in memory size. If you are interested in the SAM format please see its specification file. -S: Auto detect input format. -h: Include header. The binary alignment is redirected to a new file called K1_mapped.bam. For more information on redirection please see our Intro to Unix course book. 7.3 Unmapped read extraction Next step is to extract the reads that did not map to the host reference from the K1_mapped.bam file with the samtools fastq command (unmapped reads). samtools fastq -f 4 -1 K1_R1.u.fastq -2 K1_R2.u.fastq K1_mapped.bam Parameters -f: Output reads that only include the SAM flag. In this case 4 stands for unmapped reads. Therefore, our resulting fastq files will only contain unmapped reads. The following link is very useful to create a SAM flag you may need: https://broadinstitute.github.io/picard/explain-flags.html. -1: The output R1 fastq file of unmapped reads. -2: The output R2 fastq file of unmapped reads. Last file: The last file name is the input file for the command, K1_mapped.bam in this case. This step may make unmatched paired files (why we have .u. in the output file names). This occurs when a read from R2 is removed but the matching read in R1 is not removed, or vice versa. This will cause issues for further analysis. 7.4 Re-pair The below BBTools command will re-pair the reads by removing reads with a missing pair. The command ensures the order of the reads are identical in the 2 output paired files. repair.sh in1=K1_R1.u.fastq in2=K1_R2.u.fastq \\ out1=K1_R1.final.fastq out2=K1_R2.final.fastq \\ outs=K1_singletons.fastq Parameters in1=: The input R1 fastq file of unmapped unpaired reads. in2=: The input R2 fastq file of unmapped unpaired reads. out1=: The output R1 fastq file of unmapped paired reads. out2=: The output R2 fastq file of unmapped paired reads. outs=: The output fastq file containing the left over singletons (a sequence missing a pair). This file can normally be ignored. 7.5 Host removal summary We have run through quality control including host removal for our K1 sample. As our data has pretty much no human data we will skip this step for the other samples and use the trimmed data for the downstream analysis. In a real analysis project you would use a whole genome reference for your host. However, that would have taken too long for this practical. The most current Human reference (when this was written) is GRCh38. We used a random 10kb section to align our reads to. For more resources on the Human reference please see: https://www.ncbi.nlm.nih.gov/genome/guide/human/ The assembly we used was: https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz "],["08-Taxonomic_profiling.html", "Chapter 8 Taxonomic profiling", " Chapter 8 Taxonomic profiling There are a number of methods for determining the species composition of a metagenomic dataset, but for the purposes of this practical we will use Kraken2 &amp; Bracken (Bayesian Reestimation of Abundance with KrakEN). Kraken2 classifies short DNA with taxonomic labels and is frequently used for metagenomic studies. Bracken uses the taxonomic labels assigned by Kraken2 to compute the abundance of species in a set of DNA sequences. We will utilise Krona to create interactive tiered pie charts from the Kraken2 output. Finally, we will carry out biomarker detection of the Braken2 restimated baundances with LEfSe. First, we'll make a new directory for it and move into it, after returning home: cd .. mkdir 3-Taxonomy cd 3-Taxonomy "],["09-Kraken2.html", "Chapter 9 Kraken2 9.1 Set Kraken2 database 9.2 Classify reads 9.3 Output and report files 9.4 Confidence threshold 9.5 Kraken2 task 9.6 MCQs", " Chapter 9 Kraken2 Kraken2 assigns taxonomic sequence classifications to DNA sequences. It carries this out by comparing the k-mers of each DNA sequence to the k-mers in a genomic library. Each k-mer in the genomic library has a taxonomy assigned to it based on the LCA (lowest common ancestor) of all the known lineages that have the k-mer in their genome. One DNA sequence will have many k-mers, each of those k-mers having a classification. A single consensus taxonomic classification will be assigned to the DNA sequence. This could be at species level or higher. If the classification is at genus level then the species will be unknown. We use and suggest Kraken2 as a method to assign taxonomies to shotgun metagenomic reads for the following reasons: It is computationally efficient, being incredibly quick. It can be used for DNA sequences of any size including short illumina read, longer PacBio or ONT reads, and even assembled contigs. Custom databases can be used so you can in theory classify any organism whose genome has been sufficiently categorised by genomic sequencing. Bracken can be used downstream to produce taxonomy abundances that can be used for community analysis. For more information please see the Kraken2 paper: Improved metagenomic analysis with Kraken 2 9.1 Set Kraken2 database Prior to running Kraken2 we need to set a variable so Kraken2 knows where to look for the databases it will use. export KRAKEN2_DB_PATH=/pub14/tea/nsc206/NEOF/Shotgun_metagenomics/kraken2_db Note: You can look at the contents of the above directory to see it currently contains the MiniKraken database. This database contains only a subset of the bacteria, archaea, and viral Kraken2 libraries. This is used in this practical due to restrictions on time and computational resources. For your own analyses we would recommend the full Kraken2 database which uses all the bacteria, achaeal and viral complete genomes that are in Refseq at the time of building. See the following links for info on installing the databases. Standard Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database Custom Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases 9.2 Classify reads Now, run Kraken2 on sample K1 by running the following command. Note: In this tutorial we are not using the host removed data. This is to save time during the workshop. In your own analysis ensure you are using host removed data if your data is host derived. kraken2 --paired --db minikraken2_v1_8GB \\ --output K1.kraken --report K1.kreport2 \\ ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz Parameters --paired: Indicates that we are providing paired reads to Kraken2. Internally, Kraken2 will concatenate the R1 and R2 reads into one sequence with an N between them. --db: Specify the Kraken2 database to be used for taxonomic classification. Previous to the command we set the KRAKEN2_DB_PATH so in this case the command will look for the directory called minikraken2_v1_8GB within KRAKEN2_DB_PATH. Alternatively the full path of the required database could be provided (/pub14/tea/nsc206/NEOF/Shotgun_metagenomics/kraken2_db/minikraken2_v1_8GB). --output: The output file. More info below. --report: The output report file. More info below. ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz: The trimmed read pairs for K1, which we will use as input. 9.3 Output and report files There are two major output formats from Kraken2. 9.3.1 Output file The --output parameter creates a .kraken file. Each sequence (or sequence pair, in the case of paired reads) classified by Kraken2 results in a single line of output. Kraken2's output lines contain five tab-delimited fields; from left to right, they are \"C\"/\"U\": a one letter code indicating that the sequence was either classified or unclassified. Sequence ID: Obtained from the FASTA/FASTQ header. Taxonomy ID: Assigned by Kraken2. This is 0 if the sequence is unclassified. Length of the sequence in bp: In the case of paired read data, this will be a string containing the lengths of the two sequences in bp, separated by a pipe character, e.g. \"98|94\". LCA mapping: A space-delimited list indicating the LCA (also known as MRCA) mapping of each k-mer in the sequence(s). For example, \"562:13 561:4 A:31 0:1 562:3\" would indicate that: 562:13 - The first 13 k-mers mapped to taxonomy ID #562 561:4 - The next 4 k-mers mapped to taxonomy ID #561 A:31 - The next 31 k-mers contained an ambiguous nucleotide 0:1 - The next k-mer was not in the database 562:3 - The last 3 k-mers mapped to taxonomy ID #562 Note: that paired read data will contain a \"|:|\" token in this list to indicate the end of one read and the beginning of another. 9.3.2 Report file The --report parameter creates a .kreport2 file. This is the report output format and is required for bracken. It is tab-delimited with one line per taxon. The fields of the output, from left-to-right, are as follows: Percentage of paired reads covered by the clade rooted at this taxon. Number of paired reads covered by the clade rooted at this taxon. Number of paired reads assigned directly to this taxon. A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., \"G2\" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank. NCBI taxonomic ID number Indented scientific name 9.3.3 Screen output The output to screen will show how many sequences are classified. This will be lower than normal as we are using a mini Kraken2 database. 9.4 Confidence threshold In a real analysis you may use the option --confidence which represents the \"Confidence score threshold\". The default is 0.0, which is the lowest, with the maximum value being 1. A good place to start may be 0.1. Too many classifications are removed if you attempt it with this dataset, due to the mini Kraken2 database used. More info on the confidence scoring can be found at: https://github.com/DerrickWood/kraken2/wiki/Manual#confidence-scoring 9.5 Kraken2 task Once the Kraken2 command has finished running, run it on the other two samples. Attempt the commands without looking at the help box. Hint: You will need to change all instances of K1 to K2 or W1 in the above command K2 &amp; W1 Kraken 2 commands #K2 kraken2 --paired --db minikraken2_v1_8GB \\ --output K2.kraken --report K2.kreport2 \\ ~/2-Trimmed/K2_R1.fq.gz ~/2-Trimmed/K2_R2.fq.gz #W1 kraken2 --paired --db minikraken2_v1_8GB \\ --output W1.kraken --report W1.kreport2 \\ ~/2-Trimmed/W1_R1.fq.gz ~/2-Trimmed/W1_R2.fq.gz 9.6 MCQs Viewing the Kraken2 output files with your favourite text viewer (less, nano, vim, etc.), attempt the below MCQs. How many paired reads were unclassified for K1? 2 486,723 555,162 How many paired reads were classified for K2 (i.e. number of reads classified at root level and below)? 2 486,723 555,162 How many paired reads were assigned directly to root level for W1? 2 486,723 555,162 What percentage of W1's paired reads were assigned to the clade of Bacteroidetes (Phylum)? 0.12 0.59 14.89 What percentage of K2's paired reads were assigned to the clade of Rikenellaceae (Family)? 0.12 0.59 14.89 What percentage of K1's paired reads were assigned to the clade of Bacteroides helcogenes (Species)? 0.12 0.59 14.89 "],["10-Krona.html", "Chapter 10 Krona 10.1 Import taxonomy 10.2 Tiered piecharts 10.3 MCQs", " Chapter 10 Krona Krona is an interactive metagenome species abundance visualisation tool. We will use it to produce interactive tiered pie charts from our Kraken2 output. 10.1 Import taxonomy We can use the Kraken2 report files to create our Krona plots. With the below command we can import our Kraken2 taxonomy (within the report file) into a Krona html. ktImportTaxonomy -o kraken2.krona.html *.kreport2 -o is our output html file The final argument *.kreport2 represents all of our .kreport2 files in the current directory. The * is a wild-card, meaning any characters any number of times. Therefore *.kreport2 identifies the files K1.kreport2 K2.kreport2 and W1.kreport2. You will get a warning that not all taxonomy IDs were found. We will ignore this but in your own future installations this should be addressed with Krona's updateTaxonomy.sh command. 10.2 Tiered piecharts We can view our interactive chart in a web browser. This shows the percentage of reads that were classified to various taxonomies at different levels. firefox kraken2.krona.html &amp; This is an interactive multi-tiered pie chart with many options. Some of the interactivity is described below: To choose a sample to view, click on the sample name in the top left. To zoom into a lower taxonomy, double click on the taxonomy's name on the pie chart. E.g. double click the word \"Bacteria\" on the pie chart to zoom into Bacteria and therefore ignore Eukaryota, and \"[other Root]\". To zoom out to a higher taxonomy, click on the taxonomy's name in the middle of the pie chart. E.g. click the word \"root\" in the middle of the pie chart to zoom back out from any lower level of taxonomy. To view percentage stats of a taxonomy, click on the taxonomy name on the pie chart. This will display the percentage this taxonomy covers of all the different taxonomies above it that it belongs to. This is displayed on the right side of the screen. E.g. Click on Pseudomonadota to see the percentage of the Root it accounts for and to see the percentage of Bacteria it accounts for in the sample. 10.3 MCQs Viewing the Krona, attempt the below MCQs. What percentage of the root was classified to Bacteria for K1? 2% 34% 35% What percentage of bacteria was classified to Pseudomonadota for K2? 2% 34% 35% What percentage of the root was classified to Myxococcota for W1? 2% 34% 35% What percentage of the Terrabacteria groups was classified to Cellulomonas gilvus for K2? 39% 8% 6% What percentage of Alphaproteobacteria was classified to Phenylobacterium immobile for K1? 39% 8% 6% What percentage of Viridiplantae (in Eukaryota) was classified to Parasponia for W1? 39% 8% 6% "],["11-Bracken.html", "Chapter 11 Bracken 11.1 Bracken abundance reestimation 11.2 Output and report file 11.3 Bracken task 11.4 MCQs 11.5 Merging output 11.6 Extracting output", " Chapter 11 Bracken Bracken (Bayesian Reestimation of Abundance with KrakEN) uses taxonomy labels assigned by Kraken2 to compute estimated abundances of species in a metagenomic sample. 11.1 Bracken abundance reestimation Just like with Krona we can use the Kraken2 report files to run bracken. bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \\ -i K1.kreport2 -o K1.bracken -w K1.breport2 -r 100 -l S -t 5 Parameters -d : Specifies the Kraken2 database that was used for taxonomic classification. In this case bracken requires the variable $KRAKEN2_DB_PATH so the option is provided the full path to the kraken database. For clarity try the command ls $KRAKEN2_DB_PATH/minikraken2_v1_8GB. -i : The Kraken2 report file, this will be used as the input. -o : The output Bracken file. Information about its contents is below. -w: Output report file. This contains the Bracken read counts in a kraken-style report. This is an essential file if you want to use the Bracken output in R using the phyloseq object. This is covered in our R community analysis workshop. We won't cover it more here. -r 100: This is the ideal length of the reads that were used in the Kraken2 classification. It is recommended that the initial read length of the sequencing data is used. We are using 100 here as we used a paired library of 100bp*2 reads. -l S: This specifies the taxonomic level/rank of the Bracken output. In this case S is equal to species with the other options being D, P, C, O, F and G. -t 5: This specifies the minimum number of reads required for a classification at the specified rank. Any classifications with fewer reads than the specified threshold will not receive additional reads from higher taxonomy levels when distributing reads for abundance estimation. Five has been chosen here for this example data but in real datasets you may want to increase this number (default is 10). 11.2 Output and report file With our command we created an output and a report file for bracken. 11.2.1 Output The output file of Bracken (K1.bracken) contains the following columns: Name: Name of taxonomy at the specified taxonomic level. Taxonomy ID: NCBI taxonomy id Level ID: Letter signifying the taxonomic level of the classification Kraken assigned read: Number of reads assigned to the taxonomy by Kraken2 Added reads with abundance reestimation: Number of reads added to the taxonomy by Bracken abundance reestimation. Total reads after abundance reestimation: Number from field 4 and 5 summed. This is the field that will be used for downstream analysis Fraction of total reads: Relative abundance of the taxonomy 11.2.2 Report file The --report created the report file (K1.breport2). It contains the Bracken read counts in the same format as the report file for kraken2 This is an essential file if you want to use the Bracken output in R using the phyloseq object. This is covered in our R community analysis workshop and won't be covered here. The fields of the output, from left-to-right, are as follows: Percentage of paired reads covered by the clade rooted at this taxon. Number of paired reads covered by the clade rooted at this taxon. Number of paired reads assigned directly to this taxon. A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., \"G2\" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank. NCBI taxonomic ID number Indented scientific name 11.3 Bracken task Repeat the above commands for K2 and W1 K2 &amp; W1 Bracken commands #K2 bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \\ -i K2.kreport2 -o K2.bracken -w K2.breport2 -r 100 -l S -t 5 #W1 bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \\ -i W1.kreport2 -o W1.bracken -w W1.breport2 -r 100 -l S -t 5 11.4 MCQs Viewing the Bracken output files (.bracken) with your favourite text viewer (less, nano, vim, etc.), attempt the below MCQs. In K1, how many total reads after abundance reestimation are there for Prevotella fusca? 0.00011 16 702 In K2, how many reads after abundance reestimation were added for Bacteroides caccae? 0.00011 16 702 In W1, what is the fraction of total reads (after abundance reestimation) for Tannerella forsythia? 0.00011 16 702 11.5 Merging output To make full use of Bracken output, it is best to merge the output into one table. Before we do this we’ll copy the Bracken output of other samples that have been generated prior to the workshop. These are all either Korean or Western Diet samples. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/bracken/* . Now to merge all the K and W Bracken files. combine_bracken_outputs.py --files [KW]*.bracken -o all.bracken This output file contains the first three columns: name = Organism group name. This will be based on the TAX_LVL chosen in the Bracken command and will only show the one level. taxonomy_id = Taxonomy id number. taxonomy_lvl = A single string indicating the taxonomy level of the group. ('D','P','C','O','F','G','S'). After these columns are the following two columns for each sample. ${SampleName}.bracken_num: The number of reads after abundance reestimation ${SampleName}.bracken_frac: Relative abundance of the group in the sample 11.6 Extracting output We want a file with only the first column (organism name) and the bracken_num columns for each sample. To carry this out we first create a sequence of numbers that will match the bracken_num column numbers. These start at column 4 and are every even numbered column after this. We will use seq to create a sequence of numbers starting at 4 and including every second (2) number up to and including 50 with commas (,) as separators (-s). Note: The number 50 is chosen as 3 (first three info columns) + 24*2 (24 samples with 2 columns each) = 50. #Try out the seq command to see its output seq -s , 4 2 50 #Create variable bracken_num_columns=$(seq -s , 4 2 50) echo $bracken_num_columns Now to use the variable to extract the bracken_num columns plus the first column (species names). cat all.bracken | cut -f 1,$bracken_num_columns &gt; all_num.bracken "],["12-Lefse.html", "Chapter 12 LEfSe 12.1 Add metadata 12.2 Format file 12.3 LEfSe run 12.4 Out file 12.5 Biomarker LDA barchart 12.6 MCQs 12.7 Kraken2 and Bracken databases", " Chapter 12 LEfSe We will use LEfSe (Linear discriminant analysis Effect Size) to determine which taxa can most likely explain the differences between the Western and Korean diet. LEfSe couples standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. It can be used with other features such as organisms, clades, operational taxonomic units, genes, or functions. In essence it allows for the detection of biomarkers when comparing sample groups. In the LEfSe terminology the sample groups are called the class. Although LEfSe is a nice tool to use for a tutorial there are more robust tools. Two recommended tools are: ANCOM-BC2 MaAsLin3 They both require R and so are not included in this tutorial. However, our R community analysis workshop does include how to use ANCOM-BC2. 12.1 Add metadata We need to add metadata to our Bracken file to be ready for LEfSe. First we will copy the file so we have a backup in case we do anything wrong. cp all_num.bracken all_num.lefse.bracken Using your favourite text editor (e.g. nano, vim, etc.) add the following line to the top of your all_num.lefse.bracken file. The words are separated by tabs. If you are not sure how to carry out this task please ask a demonstrator. diet K K K K K K K K K K K K W W W W W W W W W W W W Note: The above is diet followed by 12 K and 12 W. The singular line should match the order of your samples within the file. This is the metadata line that LEfSe will use to determine which samples belong to each sample group, and therefore which to compare. In this case it is Korean diet samples versus Western diet samples. Issues with creating file? If you are having issues with creating and editing the file all_num.lefse.bracken you can copy a pre-made version. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/lefse/all_num.lefse.bracken . 12.2 Format file We need to further format and preprocess our file with a LEfSe script. lefse_format_input.py all_num.lefse.bracken all_num.lefse -c 1 -u 2 -o 1000000 Parameters all_num.lefse.bracken : Input Bracken file. all_num.lefse : Output file formatted for the run_lefse command, which we will soon run. -c 1 : Specifies the row with the class info. This is used to determine which samples will be compared against which samples. In this case it is the first row with the Ks and Ws. -u 2 : Specifies the row with the sample names. This is the second row in this case. -o 1000000 : An integer can be indicated to determine to what size (count sum value) each sample should be normalised to. LEfSe developers recommend 1000000 (1 million) when very low values are present. We generally always use 1 million for consistency. 12.3 LEfSe run Now to run LEfSe. All we need to do is run the command with the formatted input and provide an output file name. lefse_run.py all_num.lefse all_num.lefse.out 12.4 Out file The output file (all_num.lefse.out) is a tab-delimited file which contains a row for each species. Biomarkers will have the five columns below whilst non-biomarkers will have the first two followed by a \"-\". Biomarker name Log of highest class average: I.e. get the class with the greater amounts of the biomarker, average the counts and then get the log of this value. Class with the greater amounts of biomarker LDA effect size: A statistical figure for LEfSe. p-value: Biomarkers must have a p-value of &lt;0.05 to be considered significant. The LDA effect size indicates how much of an effect each biomarker has. The default is to only count a species with an LDA effect size of greater than 2 or less than -2 as a biomarker. The further the LDA effect size is from 0 the greater the effect the species causes. Generally, it can be thought of as the order of magnitude difference in the abundance of the biomarker between the sample groups. 12.5 Biomarker LDA barchart Next we can visualise the output. lefse_plot_res.py --dpi 200 --format png all_num.lefse.out biomarkers.png --dpi 200 : Dots per inch. This refers to the resolution of the output image. Normally publications want 300 dpi. We’ve chosen 200 as it is good quality and we will not be publishing these results. --format png : Format of output file. png is a commonly used file format for images. all_num.lefse.out : LEfSe output to visualise. biomarkers.png : Plot showing the LDA scores of the species detected as biomarkers. Colouring shows which class (K or W) the species is found in higher abundance. Look at the figure with firefox: firefox biomarkers.png 12.6 MCQs Interrogating the biomarkers.png plot and the all_num.lefse.out file, attempt the below MCQs. Note: In this instance green bars represent biomarkers in higher abundance in the W samples whilst the red bars represent biomarkers in higher abundance in the K samples. Which species biomarker causes the biggest effect in the W class? Adlercreutziaequolifaciens Alistipesshahii Methanosphaerastadtmanae Which species biomarker causes the biggest effect in the K class? Adlercreutziaequolifaciens Alistipesshahii Methanosphaerastadtmanae Which species biomarker (i.e. present in the plot) causes the lowest effect in the W class? Adlercreutziaequolifaciens Alistipesshahii Methanosphaerastadtmanae Which class has more biomarkers associated with it? Korean Western What is the LEfSe p-value for Campylobactercoli? 0.0022136919545913936 3.0266165524513937 3.314396201978439 What is the Log of highest class average for Streptococcussuis? 0.0022136919545913936 3.0266165524513937 3.314396201978439 What is the LDA effect size for Bifidobacteriumlongum? 0.0022136919545913936 3.0266165524513937 3.314396201978439 12.7 Kraken2 and Bracken databases In your own future analysis you will need to create your own Kraken2 and Bracken databases. Please see the following links on information for this: Kraken2 Standard Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database Custom Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases Bracken https://ccb.jhu.edu/software/bracken/index.shtml?t=manual#step1 This requires a Kraken2 database to be built first. "],["13-Functional_profiling.html", "Chapter 13 Functional profiling", " Chapter 13 Functional profiling It is possible to investigate functional differences between metagenome (and metatranscriptome) samples by directly interrogating the read data. We will look at how this can be done with a package called HUMAnN (The HMP Unified Metabolic Analysis Network), a bioBakery pipeline designed to accurately profile the presence/absence and abundance of microbial pathways in metagenomic sequencing data. HUMAnN is on its third version and was developed in tandem with the third version of MetaPhlAn, a computational tool for profiling the composition of microbial communities from metagenomic data. Please see below for a diagram showing the pipeline of HUMAnN: HUMAnN 4 HUMAnN 4 is currently in alpha so these materials teach HUMAnN3. For more information on HUMANnN4 please see: (https://forum.biobakery.org/t/announcing-humann-4-0-alpha/7531) "],["14-HUMAnN.html", "Chapter 14 HUMAnN 14.1 Mamba, directories, and files 14.2 HUMAnN pipeline 14.3 Output files 14.4 Intermediary/temp files", " Chapter 14 HUMAnN We will carry out an example run of the software and briefly explore the output files. HUMAnN can take a long time to run so we will use a small amount of example data. For this tutorial analysis we will use a subset of the HUMAnN databases for speed purposes. You should use the full databases in your real life projects. Information on installing HUMAnN and its databases can be found on its online Home Page. 14.1 Mamba, directories, and files We need a new mamba environment. Open a new terminal (right click on the main screen background, choose Terminal) and run the below: . usebiobakery3.9 Make a new directory and move into it. mkdir ~/4-FunctionalProfiling cd ~/4-FunctionalProfiling Copy over some test data we will carry out the analysis on. This is a demonstration FASTQ file that we will use. It will be small enough to run HUMAnN in a reasonable amount of time. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/humann/demo.fastq.gz . 14.2 HUMAnN pipeline Run the HUMAnN pipeline with our demo data: humann \\ --input demo.fastq.gz \\ --output demo.humann \\ --threads 10 Here, we are telling the software to use demo.fastq.gz as input and to create a new output directory called demo.humann where the results will be generated. As the software runs you might notice that HUMAnN runs MetaPhlAn. The purpose of this is to identify what species are present in the sample, so HUMAnN can tailor generate an appropriate database of genes (from those species) to map against. It will carry out this alignment against the gene database, then a protein database, and finally compute which gene families are present. HUMAnN will determine which functional pathways are present and how abundant they are. If you are using paired end reads the HUMAnN developers recommend concatenating your reads into one file and running HUMAnN on the concatenated file source. For example (don't run the below): cat K1_R1.fq.gz K1_R2.fq.gz &gt; K1.fq.gz humann --input K1.fq.gz --output humann_output --threads 14.3 Output files Once the run has completed, change into the newly created output directory and list the files that are present. cd demo.humann ls You will see that there are three files and one directory. The directory (demo_humann_temp) contains intermediate temporary files and can be disregarded for now. The three output files are: demo_genefamilies.tsv: A table file showing the number of reads mapping to each UniRef90 gene family. Values are normalised by the length of each gene family (i.e. RPK, or Reads per Kilobase). Additionally, the values are stratified so that they show the overall community abundance but also a breakdown of abundance per species detected. This allows researchers to delve into species specific functions, rather than only looking at the metagenomic functions as a whole. demo_pathabundance.tsv: A table file showing the normalised abundance of MetaCyc pathways (RPKs). These abundances are calculated based on the UniRef90 gene family mapping data and are also stratified by species. demo_pathcoverage.tsv: A table file that shows the coverage, or completeness, of pathways. For example, a pathway may contain 5 components (or genes/proteins) Pathway1 : A → B → C → D → E 100% complete A species identified in the sample may only have four of the components, meaning that the pathway is only 80% complete (represented as 0.8) Pathway1 : A → B → C → D → E 80% complete The basic format of these three output files is the same, so let's take a look at the pathway abundance table. less demo_pathabundance.tsv You will see that there are two columns: The first column shows the pathways. UNMAPPED indicates reads that could not be aligned. UNINTEGRATED indicates reads that aligned to targets not implicated in any pathways. The second column shows the abundance. This file is not too interesting to look at as it is only demo data. Therefore, press q to exit less and let's look at some real data. Note: Link to more detail on Output files 14.4 Intermediary/temp files The intermediary output directory created by HUMANnN (the directories ending with temp) can be used for debugging if the output of the analysis seems strange or if the tool produces a warning or error. Information on the files in the intermediary directory can be found in the following link: https://github.com/biobakery/humann?tab=readme-ov-file#5-intermediate-temp-output-files We won't look at these files as they contain a lot of information and the tool ran with no issues. Although useful for troubleshooting the intermediary directory for real life projects are normally very large. A project with many samples can have 100s to 1000s of Gigabytes of storage taken up by the intermediary directories. Storage is a valuable resource on HPCs and computers and so it is best to delete these directories after making sure the HUMAnN run was successful. 14.4.1 Storage check To emphasise the largeness of the intermediary files check the size of all your HUMAnN output. The du command displays the storage that files and directories take up. The parameters used are: h: Human readable output (uses K, M, G etc. to represent Kilobytes, Megabytes, Gigabytes etc.) s: Summarise c: Produce a grand total (at bottom of output) du -hsc * You will see that the intermediary directory takes up 73 megabytes of the total 73 megabytes of all the output. In other words the intermediary file takes up much more space than the actual useful output. 14.4.2 Delete intermediary directory To keep our computer and system administrator happy delete the intermediary directory. rm -r demo_humann_temp It is always good practice to delete these intermediary directories when you are happy with your output. "],["15-Multi_sample_processing.html", "Chapter 15 Multi sample processing 15.1 Pre-made data 15.2 Pathway abundance file 15.3 Combining data 15.4 Split stratified table 15.5 Renormalising data", " Chapter 15 Multi sample processing Looking at the functional profile of one sample in isolation is usually not very informative. First, there is nothing to compare it to and second, there are no biological replicates. We will therefore use all the Korean and Western diet samples. 15.1 Pre-made data It would take many hours to analyse all of the data using HUMAnN and is outside the scope of this course. For this reason, samples were analysed prior to the workshop to generate the output files we covered in the previous chapter. For the purposes of this comparison, we will look at the pathway abundances only. Copy over the data directory containing the gene families tables and have a look in it. #Ensure you are in the correct directory cd ~/4-FunctionalProfiling #Copy directory with pre made results cp -r /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/DietPathAbundance . #Move into the copied directory cd DietPathAbundance #List files ls 15.2 Pathway abundance file You will see there are 12 files prefixed with K and 12 prefixed with W, for the Korean diet and Western diet samples, respectively. Take a look at the file for K1. less K1_pathabundance.tsv There are a lot of pathways in the file. Quit out of the less viewer (q) and look at the entries for one specific pathway, COA-PWY-1 (a coenzyme A biosynthesis II pathway). grep COA-PWY-1 K1_pathabundance.tsv | less This shows 30 entries/lines with the top entry/line: COA-PWY-1: superpathway of coenzyme A biosynthesis III (mammals) 6790.1517478104 This shows the abundance of the pathway across the entire sample (6790.1517478104). The other entries show the species stratification information (mentioned above) of the pathway. I.e. the second line: COA-PWY-1: superpathway of coenzyme A biosynthesis III (mammals)|g__Bacteroides.s__Bacteroides_dorei 1292.7711872228 shows the abundance of the pathway that is contributed by the species Bacteroides dorei (1292.7711872228). Note: The species stratified pathway abundances may not equal the total community pathway abundance. Please see this forum post for details. With this information we will carry out some comparisons including biomarker detection to determine which pathways are differentially abundant between the Western diet and Korean diet samples. Note: The following methods/pipeline can be used for the genefamilies and pathcoverage tables in your own future analyses. 15.3 Combining data First, we need to combine these 24 tables into one large results table. HUMAnN provides a tool to do this: #Change directory to main Functional profiling directory cd ~/4-FunctionalProfiling #Join the tables humann_join_tables --input DietPathAbundance/ --output diet.tsv This command will look for all tables in the DietPathAbundance directory and generate a large, 25 column table called diet.tsv. You can inspect the file to ensure that this has worked correctly. less -S diet.tsv 15.4 Split stratified table For this tutorial we do not want the species stratification information. We will therefore split the table to create 2 new files: diet_unstratified.tsv: This table only contains the total abundance values for the pathways. It does not contain any species stratification information. diet_stratified.tsv: This table only contains the species stratification abundance values for the pathways. It does not contain the total abundance information. To create the split files and output them to your current directory, run the following command: humann_split_stratified_table --input diet.tsv --output . We will use the file diet_unstratified.tsv for our downstream analysis. Before you move on feel free to inspect the output files with the less command. Note: You can use any of the three tables (unsplit table, unstratified table, or stratified table) in your own analysis. This depends on your question and data. 15.5 Renormalising data The next step is to renormalise the data. Currently, all of the abundance values are only normalised within each sample (RPKs). However, they are not normalised between samples, and this is very important to do. For example, if we had sequenced two samples, A and B, and we obtained 5 million reads for sample A and 20 million reads for sample B, without normalisation, it might look there was up to 4x as much functional activity in sample B! To correct for this, we normalise the abundance values based on the number of reads in each sample. We will normalise to relative abundance (--units relab) where all abundances for each sample add up to 1. Renormalisation command: humann_renorm_table \\ --units relab \\ --input diet_unstratified.tsv \\ --special n \\ --output diet_unstratified.relab.tsv This command generates the normalised data in the new table diet_unstratified.relab.tsv. The --special n option tells the script to remove all unmapped and unassigned values (UNMAPPED &amp; UNINTEGRATED) from the table. Note: With the gene families information ensure you normalise by CPM (counts per million) with the option --units cpm. More info can be found on the Normalizing RPKs to relative abundance section of the HUMAnN 3.0 tutorial. "],["16-Heatmap.html", "Chapter 16 Heatmap 16.1 File edit 16.2 hclust2", " Chapter 16 Heatmap Now that we have our combined, unstratified, and normalised table, we can visualise the dataset to see how the two groups compare. Do samples in the same diet group appear to correlate well with each other? Are samples from one diet group distinguishable from those from the other diet group? To visualise this we will create a heatmap with hclust2. 16.1 File edit Before carrying out the command we will need to edit the file. Carry out the following alterations: Remove the _Abundance part of the sample names whilst creating a copy that we will use (It is always a good idea to keep the original file in case a mistake happens). cat diet_unstratified.relab.tsv | sed &quot;s/_Abundance//g&quot; &gt; diet_unstratified.relab.comp.tsv Next using your text editor of choice (we recommend nano) carry out the following changes on the file diet_unstratified.relab.comp.tsv. Remove the # (including the one space after the #) from the start of the header so it starts as Pathway Add in the same metadata line as we did for 12.1 but this time below the header line, i.e. as the 2nd line (ensure you are using tabs instead of spaces) Your top 2 lines of diet_unstratified.relab.comp.tsv should then look like the below (fields/columns are separated by tabs): Pathway K10 K11 K12 K1 K2 K3 K4 K5 K6 K7 K8 K9 W10 W11 W12 W1 W2 W3 W4 W5 W6 W7 W8 W9 diet K K K K K K K K K K K K W W W W W W W W W W W W Intro to unix links: Text editing with sed Redirection with &gt; Text editor nano Issues with creating file? If you are having issues with creating and editing the file all_num.lefse.bracken you can copy a pre-made version. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/lefse/diet_unstratified.relab.comp.tsv . 16.2 hclust2 Now we can use the hclust2 tool to create a heatmap of our pathway abundances. hclust2.py \\ -i diet_unstratified.relab.comp.tsv \\ -o diet_unstratified.relab.heatmap.png \\ --ftop 40 \\ --metadata_rows 1 \\ --dpi 300 Note: You will get 2 MatplotlibDeprecationWarnings, these are normal and can be ignored. However, ensure these are the only warnings/errors before continuing. Parameters -i: The input table file. -o: The output image file. The tool does not specify what types of image files you can use but .png is always a good image file format. --ftop: Specifies how many of the top features (pathways in this case) to be included in the heatmap. --metadata_rows: Specifies which row/s contain the metadata information to be used for the group colouring at the top of the heatmap. Row numbers start at 0 for this tool. Therefore our sample names are in row 0 and the diet info is in row 1. Multiple rows can be specified if you have multiple rows of metadata. e.g. --metadata_rows 1,2,3. --dpi: The image resolution in dpi (dots per inch). 300 dpi is used for publication quality images. There are many more options that can be seen on the hclust2 github. Visualise Now we can view the plot. firefox diet_unstratified.relab.heatmap.png From this, we can see that there is a small amount of clustering caused by the differences between the Korean and Western diet. Other factors that we do not know about the samples must also be casuing an effect. This is normal as we cannot account for everything but it is good to try to account for as much as possible. Note: Your plot should look the same as the below. If it does not you may not have carried out the file edit step correctly. MCQs Which pathway stands out the most? ARO-PWY DTDPRHAMSYN PWY-6385 How many clusters are formed based on diet (Colours on tree at top of heatmap)? 1 2 3 How many clusters are formed based on pathways (Colours on tree at the side of heatmap)? 1 2 3 You can look up the pathway names in the table file to see a fuller description. "],["17-LEfSe_humann.html", "Chapter 17 LEfSe", " Chapter 17 LEfSe For the final part of this section, we will see if there are any statistically significant differences between the two sample groups. There are several ways in which this can be achieved but we will use LEfSe again. Task: Go back to your shotgun_meta terminal (or create a new terminal and use . useshotgun). Then change directory to 4-FunctionalProfiling). Thankfully we already formatted the file to work with LEfSe when we formatted it for hclust2. #LEfSe format lefse_format_input.py \\ diet_unstratified.relab.comp.tsv \\ diet_unstratified.relab.comp.lefse \\ -c 2 -u 1 -o 1000000 #Run LEfSe lefse_run.py \\ diet_unstratified.relab.comp.lefse \\ diet_unstratified.relab.comp.lefse.out #Produce LEfSe plot lefse_plot_res.py \\ --dpi 200 \\ --format png \\ #Below options added to increase max length of feature string --max_feature_len 200 \\ diet_unstratified.relab.comp.lefse.out \\ diet_unstratified.relab.comp.lefse.png #View plot firefox diet_unstratified.relab.comp.lefse.png Look at the output and see what pathways count as biomarkers for the 2 groups. "],["18-HUMAnN_task.html", "Chapter 18 Optional task", " Chapter 18 Optional task Carry out all the steps starting from Multi sample processing with the gene families information. Copy the gene families data from /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/DietGeneFamilies Ensure you set the --units option to cpm in the renormalising data step. On top of analysing the unstratified data you can also analyse the stratified data. That completes the non assembly approach to shotgun metagenomic analysis. The next chapters will cover an assembly approach. "],["19-Metagenome_assembly.html", "Chapter 19 Metagenome assembly 19.1 Mamba 19.2 A primer on short read assembly", " Chapter 19 Metagenome assembly So far we have directly analysed the read data itself which is perfectly fine for taxonomic profiling and for certain methods of functional profiling. However, Illumina reads are generally short and therefore can not provide us with much data on larger constructs that are in the metagenomic samples, e.g. genes. While it is possible to predict from which gene a sequence read might originate, the short nature of the query can sometimes lead to ambiguous results. Additionally, depending on the application it can become computationally intensive to analyse large numbers of reads. Here, we are only using samples with 1 million reads. Some metagenome samples consist of 50-100 million+ read pairs. If such a sample belonged to a set of 100 samples, that would be up to 10 billion read pairs, or 2 trillion bases of sequence data, with many of these being redundant. For this reason, it is sometimes advantageous to assemble the reads into contigs, using a meta-genome assembler. This has the dual effect of: Reducing the overall size of the data for analysis. If a metagenome was sequenced at 50x depth, then by assembling it you could theoretically reduce the amount of sequence to analyse by 50-fold. Increase the size of the fragments you will analyse. This is the main advantage of an assembly, as the ~100 bp reads can be pieced together to form 100,000 kb+ contigs. These contigs will contain complete genes, operons and regulatory elements: Reconstructed genome sections. Here, we will carry out a couple of assemblies on our dataset. 19.1 Mamba We will use the shotgun_meta conda environment so use a terminal where this is activated or open a new one and run . useshotgun. 19.2 A primer on short read assembly Illumina reads are too short and numerous to use traditional overlap-layout-consensus assemblers as such an approach would be far too computationally intensive. Instead, we use De Bruijn graph based assemblers. Briefly, these operate as follows: All reads are broken down into k-length overlapping fragments (k-mers). e.g. if we choose a k-mer size of 5 bp, the following two sequences (blue) would be broken down into the k-mers below them (red): All k-mers are linked to other k-mers which match with a k-1 length overlap (i.e. that overlap by all but one base): Paths are routed through the graph and longer contigs are generated: The example here is a vast oversimplification of the complexity of a De Bruijn graph (i.e. there are no branches!). Routing through the graph is never as simple as this as some k-mers will lead to multiple k-mers, which can result in the break point of a contig. This is especially true for complex metagenomic data. Generally speaking, the shorter the k-mer, the more branches there will be, the trickier the graph is to resolve, so the resulting contigs are smaller. Assemblers usually perform better with longer k-mer lengths but even then there might not be enough depth of sequencing to generate all k-mers that form overlaps, therefore leading to break points. Finding the right k-mer size usually involves testing several. Fortunately, the assembler we will use, MEGAHIT, allows us to build an assembly using multiple k-mer lengths iteratively. The other great advantage about MEGAHIT is that it is quick and efficient. We will use MEGAHIT on our data soon, but first there is an additional processing step for our sequences... "],["20-FLASH.html", "Chapter 20 FLASH 20.1 Stitching 20.2 Sticthed fastqs and histogram 20.3 MCQs", " Chapter 20 FLASH Longer k-mers generally perform better for assemblies. However, our maximum read length is 100 bp so we are limited to a maximum k-mer length of 99 bp. Thankfully we can get even longer k-mers if we stitch our read pairs together. Note: This method will not work if your reads have no overlap. If you are not sure if your reads have overlap ask the team who sequenced them. A read pair consists of two sequences read from each end of a fragment of DNA (or RNA). If the two sequences meet and overlap in the middle of the fragment, there will be a region of homology. We can use this to merge the two reads together (See next image). First, we obtain our forward and reverse reads, derived from different ends of the same fragment. Second, we look for sufficient overlap between the 3' ends of our sequences. Third, if there is sufficient overlap, we combine, or stitch, the two reads together to form one long sequence. Once we have longer stitched reads, we can increase the k-mer length for our assembly. There are a number of pieces of software that can be used to stitch reads (e.g. Pear, Pandaseq) but today we will use one called FLASH. 20.1 Stitching Make a new output directory for the stitched reads and run FLASH: #Change directory to home cd ~ #Make and move into new directory mkdir 5-Stitched cd 5-Stitched #Run flash flash -o K1 -z -t 12 -d . \\ ../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz Parameters -o: Sets the prefix of the output files. -z: The input is zipped. -t: Number of threads to use. -d: The directory the output files will be placed. The last 2 flag-less parameters are the forward and reverse read files for stitching. 20.2 Sticthed fastqs and histogram Once FLASH has finished running, it will display on screen how well the stitching process went, in this case a low amount of reads were combined. Have a look what files have been generated. ls We have three new fastq.gz files: The stitched reads: K1.extendedFrags.fastq.gz The forward reads unable to be stitched: K1.notCombined_1.fastq.gz The reverse reads unable to be sticthed: K1.notCombined_2.fastq.gz Note: The unstitched forward and revers reads (notCombined) are paired. We can also see what the new read lengths are: less K1.histogram Scroll down with the down key and you will see that we are looking at a histogram showing the proportion of stitched reads at different lengths. 20.3 MCQs What length has the highest proportion of stitched reads? 101 177 188 What length has the lowest proportion of stitched reads? 101 177 188 "],["21-MEGAHIT.html", "Chapter 21 MEGAHIT 21.1 MEGAHIT 21.2 QUAST 21.3 MCQs 21.4 Metagenome assembly summary", " Chapter 21 MEGAHIT We will use our stitched and unstitched reads to produce an assembly with MEGAHIT. 21.1 MEGAHIT Create a new directory to store our assembly in. cd .. mkdir 6-Assembly cd 6-Assembly Run the metagenome assembler MEGAHIT using our stitched read data. We are usign the stithced and unstitched reads. megahit \\ -r ../5-Stitched/K1.extendedFrags.fastq.gz \\ -1 ../5-Stitched/K1.notCombined_1.fastq.gz \\ -2 ../5-Stitched/K1.notCombined_2.fastq.gz \\ -o K1 \\ -t 12 \\ --k-list 29,49,69,89,109,129,149,169,189 Parameters -r: Single-end reads to be used for assembly. We are using our successfully stitched reads. -1: Forward reads of paired end reads to be used for assembly. We are using the reads that did not stitch as they still have useful information. -2: Reverse reads of paired end reads to be used for assembly. We are using the reads that did not stitch as they still have useful information. -o: Output directory. -t: Number of threads to be used for process. --k-list: K-mer list. The k-mer list instructs MEGAHIT to first generate an assembly using a k-mer size of 29 bp and when that is complete, integrate the results into an assembly using a k-mer size of 49 bp, and so on up to a final iteration using a k-mer size of 189 bp. This large range of k-mer lengths should give us a good assembly, given the data. However, it may take a while to run. This might be a good time to read on or take a break whilst the command runs. If you need a command prompt (your current one is busy because MEGAHIT is running), right click on the main screen, choose Terminal. Once the assembly is complete, look at the output FASTA file containing the contigs: less K1/final.contigs.fa There is not much to see. When happy, quit less (q) and carry on to QUAST. 21.2 QUAST We will generate assembly metrics with QUAST. QUAST is a very popular genomeevaluation tool that produces a html report with various metrics such as the number of contigs and length of the assembly. 21.2.1 Assembly assessment Create a directory for the QUAST output. #Create QUAST output directory #The option -p will create a directory and any required # parent directories mkdir -p quast/K1 Run QUAST. quast -o quast/K1 K1/final.contigs.fa 21.2.2 Report QUAST will run relatively quickly. Once complete view the QUAST report with firefox. firefox quast/K1/report.html The report tells us quite a bit about the assembly quality. Two definitions that you may not be aware of are N50 and L50. To calculate these values: Order the contigs from largest to smallest. Total up the sizes from biggest downwards. The contig we reach where our total is at least 50% of the size of the whole assembly is the N50 contig. N50 equals the length of the N50 contig. L50 is the number of contigs with a length equal to or greater than N50. 21.3 MCQs Brilliant! Using the QUAST report answer the below MCQs. Note: Due to the assembly process your values may be slightly different (&lt; 1%). Please choose the closest value. What is the total length of the assembly? 39.88 18,650 16,187,259 How many contigs does the assembly consist of? 39.88 18,650 16,187,259 What is the GC% of the assembly? 39.88 18,650 16,187,259 What is the N50 of the assembly? 1,363 2,178 87,586 What is the L50 of the assembly? 1,363 2,178 87,586 What is the length of the largest contig? 1,363 2,178 87,586 Questions How do the contig metrics compare to the original reads? Are there more reads than contigs? Are the contigs longer than the original reads? 21.4 Metagenome assembly summary We now have an assembly. It is not a brilliant one due to us only having used 1 million reads. In real analysis we would prefer fewer but longer contigs. We will explore some tools we can use with our metagenome assembly in the next chapters. There is also a metaQUAST specifically for metagenome assemblies but it requires reference assemblies. "],["22-Genome_binning.html", "Chapter 22 Metagenome binning", " Chapter 22 Metagenome binning A metagenome assembly consists of contigs from many different genomes. At this stage we don't know which contigs are from which species. We could try to taxonomically classify each contig but there are 2 problems with this approach: Some contigs may be misclassified which can lead to multiple contigs from the same genome/organism being classified as various taxa. Databases are incomplete and so some contigs will not be classified at all (microbial dark matter). To alleviate these issues genomic binning can be carried out. This will cluster contigs into bins based on: Coverage: Contigs with similar coverage are more likely to be from the same genome. Composition: Contigs with similar GC content are more likely to belong to the same genome. Genomic binning has been used to discover many new genomes. Additionally, it makes downstream analyses quicker as the downstream steps will be carried out on the sets of bins rather than on one large metagenome assembly. Binning produces \"bins\" of contigs of various quality (e.g. draft, complete). These bins are also known as MAGs . In other words a MAG is a single genome assembly that was assembled with other genomes in a metagenome assembly but later separated from the other assemblies. The term MAGs has been adopted by the GSC (Genomics Standards Consortium). It is recommended to ensure you do not have a poor quality metagenome assembly. Binning requires contigs of good length and good coverage. Extremely low coverage and very short contigs will be excluded from binning. "],["23-Depth_calculation.html", "Chapter 23 Depth calculation 23.1 Index assembly 23.2 Alignment 23.3 SAM to sorted BAM 23.4 Summarise depths 23.5 View depths 23.6 Summarise with R 23.7 Remove SAM and BAM files", " Chapter 23 Depth calculation We will use MetaBAT2 for genome binning. It requires depth information of the contigs. Depth will be calculated for each base of the assembly by aligning the trimmed reads to the metagenome assembly. The depth will be equal to the number of reads/sequences that align to the base, this represents how many times the base was sequenced. If a base from an assembly aligns to 10 of the reads/sequences used for assembly it will have a depth of 10x. The average of all the base depths in a contig is its average depth. 23.1 Index assembly For the alignment we will use bwa. We need to index our assembly file prior to alignment. bwa index ~/6-Assembly/K1/final.contigs.fa 23.2 Alignment Next we will align our trimmed paired reads we used to create the stitched reads. We will carry this out with the bwa mem command. bwa mem is a good aligner for short reads. If you are using long reads (PacBio or Nanopore) minimap2 will be more appropriate. bwa mem ~/6-Assembly/K1/final.contigs.fa \\ ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz &gt; \\ K1.sam 23.3 SAM to sorted BAM After alignment we need to get the file ready for the contig depth summarisation step. This requires converting the SAM file to a BAM (binary form of a SAM file) file and then sorting the BAM file. # Convert sam to bam file samtools view -bu K1.sam &gt; K1.bam # Created sorted bam file samtools sort K1.bam &gt; K1.sort.bam 23.4 Summarise depths Now we can summarise the contig depths from the sorted bam files with MetaBAT2's jgi_summarize_bam_contig_depths command. jgi_summarize_bam_contig_depths --outputDepth K1.depth.txt K1.sort.bam 23.5 View depths Have a look at the depth file. less K1.depth.txt Many contigs have low depths of less than 10 (totalAvgDepth) and short lengths of less than &lt;1500 (contigLen). 23.6 Summarise with R For tutorial purposes we will have a better at the depth by summarising the contig depths with R. Activate R: R 23.6.1 Column summaries Read in the file and get a summary() of it. #Read in the table as an object called df (short for data frame) #We want the first row to be the column names (header=TRUE) #We do not want R to check the column names and &quot;fix&quot; them (check.names=FALSE) df &lt;- read.table(&quot;K1.depth.txt&quot;, header=TRUE, check.names=FALSE) #Create a summary of the data summary(df) The last command gave us summary information of all the columns. This includes the minimum, maximum, mean, median, and Inter-Quartile Range (IQR) values. We can see the values of the contigLen and totalAvgDepth are very low. However, this is most likely due to a bunch of short and low coverage contigs which will be ignored by MetaBAT2, our binning tool. 23.6.2 Filter then summarise We'll remove rows with information on contigs shorter than 1500 and rerun the summary. MetaBAT2's documentation dictates the minimum contig length should be &gt;=1500 with its default being 2500. #Set the new object &quot;df_min1500len&quot; as all rows #where the value in the column &quot;contigLen&quot; of &quot;df&quot; #Is greater than or equal to 1500 df_min1500len &lt;- df[df$contigLen &gt;= 1500,] #Summary of our new data frame summary(df_min1500len) That is looking better. The minimum average depth for MetaBAT2 is 1 and our minimum value is 2.700 with a maximum of ~93 (your values may differ slightly). 23.6.3 Quit R Now you can quit R and continue. #quit R q() #On the prompt to save your workspace press &quot;n&quot; and then enter. Note: One of the reasons for our short contigs is that we only used a subset of our sequencing dataset for this tutorial due to time concerns. 23.7 Remove SAM and BAM files SAM and BAM files are large so it is good practice to delete them when you don't need them anymore. Delete you SAM and BAM files. rm K1.sam K1.bam K1.sort.bam Great! With our K1.depth.txt file we will next carry out MetaBAT2 genome binning. "],["24-MetaBAT2.html", "Chapter 24 MetaBAT2 24.1 MetaBAT2 24.2 Bins", " Chapter 24 MetaBAT2 We will use MetaBAT2 for genome binning. It has three major upsides that makes it very popular: It has very reliable default parameters meaning virtually no parameter optimisation is required. It performs very well amongst genome binners. It is computationally efficient compared to other binners (requires less RAM, cores etc.). Make a new directory and move into it. #Make directory mkdir -p ~/7-Binning/K1 #Move into it cd ~/7-Binning/K1 24.1 MetaBAT2 With our assembly and its depth information we can run MetaBAT2 for binning. #make a diretcory for the bins mkdir bins #Run MetaBAT2 metabat2 \\ --inFile ~/6-Assembly/K1/final.contigs.fa \\ --outFile bins/K1 \\ --abdFile ~/6-Assembly/K1.depth.txt \\ --minContig 1500 Parameters --inFile: Input metagenome assembly fasta file. --outFile: Prefix of output files. --abdFile: Contig depth file. Caluclated in the previous Depth calculation chapter --minContig: Minimum size of contigs to be used for binning. The default is 2500. We used the minimum value of 1500 as we are using tutorial data. We recommend using the default in your own analysis. 24.2 Bins List the contents of the output directory and you'll see there is 1 fasta file with the prefix of K1. This is a bin that will hopefully contain 1 MAG. In your future analysis you may get many bins, each hopefully containing only one MAG. ls bins Super! In the next chapter we will assess the quality of these bins. "],["25-CoCoPyE.html", "Chapter 25 CoCoPyE 25.1 Workflow concept 25.2 Mamba 25.3 Predict bin quality 25.4 Stats", " Chapter 25 CoCoPyE CoCoPyE is a useful tool to assess the quality of assembled microbial genomes. This can be used on assemblies produced from single cell, single isolate, or metagenome data. 25.1 Workflow concept The tool utilises protein domain families from the Pfam database. A protein domain is a region of a protein that folds independently from the rest of the protein. They vary in length from ~50-250 amino acids. Many proteins consist of several domains and a domain can appear in many different proteins. CoCoPyE estimates quality of a genome by estimating completeness and contamination through an input-preprocessing step followed by a two stage prediction step. Below is the workflow as a diagram followed by a breif description on how it works. For full details please see the CoCoPyE publication: https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae079/7841111 25.1.1 Input Pre-processing A protein domain search is carried out on all translated open reading frames of our our query genomes, MAGs/bins in our case. The Pfam protein database is used and the tool produces protein domain counts for our query genomes. The completeness of the query genome is estimated based on one of 2 universal marker sets. The universal marker sets contain the protein domains found in either all reference bacterial genomes or all reference archaeal genomes. Query genomes (MAGs/bins) with a completeness score &lt;10% are rejected and removed from further analysis. 25.1.2 Prediction: Stage I In Stage I the nearest neighbours of our query's protein domains are searched for in the Pfam database. This creates a list of reference protein domains for our query. Next a specific marker prediction is carried out to create the stage I (i.e. marker based) completeness &amp; contamination values. The Pfam database has information on which taxa these domains are present in. Therefore, sets of domains can be used as markers for specific lineages. Prediction example Let's go through a fictional example where we have 26 protein domains PDA-Z which cover 2 different lineages: Gigabacteria and Birthobacteria. Both Gigabacteria and Birthobacteria contain domains PDA-F Domains PDG-P are only present in Gigabacteria Domains PDQ-Z are only present in Birthobacteria Analysis finds that our query genome contains the domain families PDD-R. We therefore acquire the below results: The query genome is classified as belonging to the Gigabacteria lineage as it contains many (in this case all) the domains unique to Gigabacteria (PDG-P) and many of the domains shared by both bacteria. Completeness: 81.25% It is missing 3 of the domains found in Gigabacteria (PDA-C) 13/16 = 0.8125 = 81.25% Contamination: 13.33% Of the 15 domains our query has 2 of them do not belong to it in the reference (PDQ &amp; PDR) 2/15 = 0.1333 = 13.33%. Thresholds Any query genomes with a completeness value of &lt;60% or contamination value &gt;30% have their values recorded as their output but they do not move onto stage II. Other query genomes move onto the neural network stage (stage II). 25.1.3 Predition: Stage II The next step utilises the domain database neighbours found in stage I. With this it creates a count ratio histogram (CRH) to use for a machine learning (ML)/neural network approach. This is carried out to reduce the dimensionality of the data to improve iterative training and reduce the risk of overfitting. But what is a CRH? Count Ratio Histogram Each Pfam domain family is given a count ratio of Cq:Cr. Cq: The count of the domain family in the query genome. Cr: The count of the domain family in the reference genome. The reference genome is determined in stage I. Examples: PDA domain Appears 2 times in the query Appears 2 times in the reference Count ratio = 1/1 This indicates high completeness and low contamination. PDH domain Appears 4 times in the query Appears 2 times in the reference Count ratio = 2/1 This indicates contamination because there are too many copies of the domain in our query PDS domain Appears 4 times in the query Appears 12 times in the reference Count ratio = 1/3 This indicates low completeness because there are too few copies of the domain in our query The relative frequency of all the different domain count ratios are used to create a histogram, the CRH. The process itself will use a text histogram but below are two visualisations of CRHs. The main features of a CRH are: The central point (1/1): This indicates high completeness and low contamination. If the central peak contained 100% of the count ratios this would indicate 100% completeness and 0% contamination. Right of the central point: Histogram bins on the right hand side indicate contamination. Left of the central point: Histogram bins on the left hand side indicate decreasing completeness. The Above plot was created from a query genome with 61% completeness and 30% contamination. The Above plot was created from a query genome with 90% completeness and 10% contamination. Machine learning The machine learning step uses the CRHs of the query genomes to improve/refine the marker-based estimations from stage I. Two neural networks are used in this prediction refinement step. One is used to improve the completeness estimation and one is used to improve the contamination estimation. The neural network implementations are from sklearn. If you are interested in how the machine learning step works please see the CoCoPyE publication: https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae079/7841111 25.2 Mamba Due to program version conflicts we will use the CoCoPyE conda environment for this section. Open a new terminal and activate the CoCoPyE environment. . usecocopye Ensure you are in the correct directory. cd ~/7-Binning/K1/ 25.3 Predict bin quality cocopye has one main command to predict bin quality: cocopye run. Run the cocopye command (this will take a while): cocopye run \\ -t 12 \\ -i bins/ \\ -o cocopye_output.csv Parameters -t : Number of threads/cores/CPUs to utilise for the process. -i : Input directory containing the bins in FASTA format. -o : Output file (more info below). Other options include: --file-extensions : The allowed file extensions for input files within the input directory. The default is fasta,fna,fa. Our bin files produced from MetaBAT2 have the file extension/suffix of .fa so the default works. -v : The output file verbosity. Values are: standard (default), extended, and full. More information on the output below. 25.4 Stats As we have only used a subset of data the results are not very good. We'll therefore look at premade results. These premade results were produced using the entire K1 dataset. First you will need to copy them. cd ~/7-Binning mkdir K1_fullset cd K1_fullset ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/binning/K1_fullset/* . Now we can look at the results table that is in your current directory. #Use tr to tranform commas (,) to tabs (\\t) #This makes it easier to visually parse the columns cat cocopye_output.csv | tr &quot;,&quot; &quot;\\t&quot; | less CoCoPye statistics definitions Output columns when -v set to standard (also the default) bin: Name of the input bin (filename excluding file extension) completeness: Completeness value between 0 and 1 contamination: Contamination value between 0 and 1 method: One of three results: rejected: Failed the threshold in the input-preprocessing step. markers: Values based on stage I as failed to pass onto stage II due to the stage I thresholds markers + neural network: Values based on stage II as passed all thresholds taxonomy: Taxonomy estimate based on a consensus between the nearest neighbors taxonomy_level: Rank of the taxonomy estimate notes: Additional notes (currently this is always empty, but we might add some notes in the future) Please see the full list of statistics definitions from CoCoPyE wiki. This includes the defintions when -v is set to extended or full. There is a lot of information. In the next chapter we'll calculate a bin quality score value with the CoCoPyE statistics. "],["26-Bin_quality_scores.html", "Chapter 26 Bin quality scores 26.1 Quality file 26.2 Calculate quality with awk 26.3 Add quality to the checkm results file 26.4 MCQs 26.5 Bin quality summary", " Chapter 26 Bin quality scores One quick way to calculate the overall quality of a MAG/bin is with the following equation: \\[ q = comp - (5 * cont) \\] Where: q = Overall quality comp = Completeness cont = Contamination A score of at least 70-80% (i.e. 0.7 to 0.8) would be the aim, with a maximum/perfect value being 100% (100% completeness, 0% contamination). We'll therefore calculate this for the bins with some bash and awk scripting. Note: Values will range from: 100% (i.e. 1): 1 Completeness - (5 * 0 Contamination) -500% (i.e. -5): 0 Completeness - (5 * 1 Contamination) 26.1 Quality file We will create a new file with only the quality information called \"MAGS_quality.csv\". Make the file with it only containing the header \"quality. We will add the quality scores to this later. echo &quot;quality&quot; &gt; MAGS_quality.csv 26.2 Calculate quality with awk Next is the most complicated command. We will be calculating the Overall quality (see calculation above) for each row except the header row. We will be using a complicated linux based language called awk. This is very useful as it can carry out calculations on columns or as awk calls them, fields. As this is new and complicated we will build up our command step by step. 26.2.1 Extract fields/columns The first step is to extract the completeness and contamination fields/columns. awk -F, &#39;{print $2,$3}&#39; cocopye_output.csv -F,: Indicates the input fields are separated by commas (,). '': All the awk options are contained within the quotes. {}: We can supply a function to awk within the braces. print $2,$3: This function instructs awk to print the 2nd (completeness) and 3rd (contamination) fields. It is common to put commas (,) between fields if printing multiple fields. cocopye_output.csv: Our last parameter is the input file. We are not changing the contents of the file, only printing information to screen/stdout. 26.2.2 Ignore header We do not want the header in our calculation so we will add an extra awk option. awk -F, &#39;NR&gt;1 {print $2,$3}&#39; cocopye_output.csv NR&gt;1: NR stands for number of records. Rows are called records in awk. Therefore NR&gt;1 means awk will only carry out the functions on the records numbered greater than 1. I.e. skip row 1, the header row. 26.2.3 Calculate quality The next step is to carry out the overall quality calculation. awk -F, &#39;NR&gt;1 {print $2 - (5 * $3)}&#39; cocopye_output.csv Our new function, {print $2 - (5 * 13)}, carries out the overall quality calculation and prints it for each record/row except the first (NR&gt;1). You will notice that we have values that equal 4. Let us fix that. 26.2.4 Fix values Some quality values come out as 4. This is not correct and comes about as some completeness and contamination values have been set to -1 (-1 - (5 * -1) = 4). If you look at the file cocopye_output.csv you will notice the bins with -1 values have the rejected for their method value. These are bins which failed the Input Pre-procesing step. We will therefore change these quality values to the lowest possible value of -5 (0 - (5 * 1) = -5). awk -F, &#39;NR&gt;1 {print $2 - (5 * $3)}&#39; cocopye_output.csv | \\ sed &quot;s/^4$/-5/&quot; In this case we pipe (|) our output to sed to substitute lines that start with (^) and end with ($) the same 4 with -5. In other words we replace lines that only contain a 4 with a -5. 26.2.5 Append to quality file Finally we can append the quality values to our MAGS_quality.csv file. awk -F, &#39;NR&gt;1 {print $2 - (5 * $3)}&#39; cocopye_output.csv | \\ sed &quot;s/^4$/-5/&quot; &gt;&gt; MAGS_quality.csv In the above case we use &gt;&gt; to append the information to the file MAGS_quality.csv. We append because we want to retain the header we added to the file earlier. You can view the file to ensure it worked. The first and second values should be 0.9838 and 0.493 less MAGS_quality.csv 26.3 Add quality to the checkm results file Now we can combine the files cocopye_output.csv and MAGS_quality.csv with the paste command into a new file called cocopye_quality.csv. The -d \",\" option indicates the merged files will be separated by commas (,), matching the column separation in cocopye_output.csv. paste -d &quot;,&quot; cocopye_output.csv MAGS_quality.csv &gt; cocopye_quality.csv 26.4 MCQs Viewing the file cocopye_output.csv attempt the below questions. Tip: You can use the cut command to look at specific columns. For example: #look at the &quot;bin&quot; and &quot;quality&quot; columns #Convert the printed output&#39;s commas to tabs for readability cut -d &quot;,&quot; -f 1,8 cocopye_quality.csv | tr &quot;,&quot; &quot;\\t&quot; What lineage was assigned to bin K1.1? Bacteria Bacteroides Lachnospiraceae What lineage was assigned to bin K1.22? Bacteria Bacteroides Lachnospiraceae What lineage was assigned to bin K1.8? Bacteria Bacteroides Lachnospiraceae What is the quality value of K1.1? 0.0202 0.6724 0.9769 What is the completeness value of K1.30? 0.0202 0.6724 0.9769 What is the contamination value of K1.12 bin? 0.0202 0.6724 0.9769 Which bin has the highest quality value (98.38%)? K1.20 K1.26 K1.22 Which bin has the quality value of -2.9215? K1.20 K1.26 K1.22 Which bin has the highest completeness value (98.59%)? K1.20 K1.26 K1.22 26.5 Bin quality summary It is always useful to know the quality of your bins so you know which are more reliable than others. With that information you can be more or less certain when concluding your findings. We have some good quality bins but many poorer quality bins too. Ultimately binning is trying to separate all the genomes from each other. A better metagenome assembly would most likely have led to better binning. "],["27-Assembly_functional_annotation.html", "Chapter 27 Assembly functional annotation Taxonomic annotation", " Chapter 27 Assembly functional annotation In this section we will carry out carry out functional annotation of our MAGs/bin/bins. There are many different ways to carry this. In this tutorial we will use 2 tools: Bakta: Annotation of Bacterial genomes and plasmids. MinPath: Prediction of metabolic pathways. Include MetaCyc and KEGG pathways. Conda Environment: Go back to your shotgun_meta terminal (or create a new one and use . useshotgun). Taxonomic annotation Taxonomic annotation of bins can be carried out with Kraken2. As we have already done this for the reads and taxonomic results from read and assembly approaches have similar performances we will not cover it here. "],["28-Bakta.html", "Chapter 28 Bakta 28.1 Annotation 28.2 Annotation files", " Chapter 28 Bakta We will carry out Bakta functional annotation. Bakta can annotate bacterial genomes and plasmids from both isolates and MAGs. Make a new directory and move into it. mkdir ~/8-Annotation cd ~/8-Annotation 28.1 Annotation Now we can annotate one of the bins. The below will take a long time to run (&gt;1 hour). Instead of running it skip onto the next section to copy pre-made output to continue with. This command is here so you know what to run in your own future analyses. bakta \\ --db /pub14/tea/nsc206/databases/bakta/db/ \\ -o K1.1 \\ ~/7-Binning/K1_fullset/bins/K1.1.fa Parameters --db: Location of Bakta database. You will need to install this in your own installation. Instructions are in the appendix. -o: The output directory. This must not exist before running the command. The last parameter is the fasta file containing the genome/plasmid you would like annotated. 28.1.1 Bakta: premade results Link the pre-made results for all the K1 bins. #Link all data ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/bakta/K1_fullset/* . 28.2 Annotation files List the files in the newly created K1.1 directory. Each of the files has the prefix \"K1.1\" and contains the following information: .tsv: annotations as simple human readble TSV .gff3: annotations &amp; sequences in GFF3 format .gbff: annotations &amp; sequences in (multi) GenBank format .embl: annotations &amp; sequences in (multi) EMBL format .fna: replicon/contig DNA sequences as FASTA .ffn: feature nucleotide sequences as FASTA .faa: CDS/sORF amino acid sequences as FASTA .hypotheticals.tsv: further information on hypothetical protein CDS as simple human readable tab separated values .hypotheticals.faa: hypothetical protein CDS amino acid sequences as FASTA .json: all (internal) annotation &amp; sequence information as JSON .txt: summary as TXT .png: circular genome annotation plot as PNG These are only useful for complete/near complete circular genomes I would suggest looking at GenoVi for circular genome plots .svg: circular genome annotation plot as SVG .log: Log file of command View the summary file for bin K1.1. less -S K1.1/K1.1.txt Summary fields Sequence information Length: Number of bases Count: Number of contigs/scaffolds GC: GC% N50: N50 N ratio: Ratio of N bases to non-N bases coding density: Percentage of bases within coding regions Annotation information. tRNAs: Transfer RNAs tmRNAs: Transfer-messenger RNA rRNAs: Ribosomal RNAs ncRNAs: Non-coding RNAs ncRNA regions: Non-coding RNA regions CRISPR arrays: CRISPR arrays CDSs: Coding sequences pseudogenes: Segments of DNA that structurally resembles a gene but is not capable of coding for a protein hypotheticals: Hypothetical genes, which are predicted solely by computer algorithms, are experimentally uncharacterized genes signal peptides: Short peptides (usually 16-30 amino acids long) normally present at the N-terminus of most newly synthesized proteins that are destined toward the secretory pathway sORFs: Short open reading frames (&lt;100 amino acids) gaps: Gaps in the genome assembly oriCs: Chromosome replication origin for bacteria oriVs: Plasmid replication origin oriTs: An origin of transfer (oriT) is a short sequence ranging from 40-500 base pairs in length. It is necessary for the transfer of DNA from a gram-negative bacterial donor to recipient during bacterial conjugation View the gff file for bin K1.1. less K1.1/K1.1.gff3 The GFF file is a tab delimited file containing annotation information for the features in the assembly/bin. In this case it is a GFF3 file (most curent version of GFF). There is quite a lot of information contained in each row so instead of listing all the columns here please have a look at the official documentation: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md Loop used to analyse all bins The code below is for your future analysis, do not run it now as it will take too long. ls -1 ~/7-Binning/K1_fullset/bins/*fa | while read f ; \\ do s=$(basename $f | sed &quot;s/.fa//&quot;) ; echo $s ; \\ bakta --db /pub14/tea/nsc206/databases/bakta/db/ \\ -o ${s} \\ ~/7-Binning/K1_fullset/bins/K1.1.fa We can quickly see if any of the bins contain a specific annotation. For example, if we wanted to know if there were any ATP-binding proteins in any of the bins we could carry out the below command: grep &quot;ATP-binding protein&quot; */*gff3 | less -S We can now view the lines containing \"ATP-binding protein\" with the start of the line containing the file name the line belongs to. In your future analyses you can expect these files further with excel, R, or visualisation software like IGV (https://software.broadinstitute.org/software/igv/GFF). What if you want to know about pathways? "],["29-Minpath.html", "Chapter 29 MinPath 29.1 EC extraction 29.2 MetaCyc predictions 29.3 MetaCyc report 29.4 KEGGs", " Chapter 29 MinPath MinPath can predict MetaCyc metabolic pathways. These pathway are made up of sets of enzymes that catalyse various reactions. Ensure you start in the ~/8-Annotation directory. 29.1 EC extraction Before we can estimate the pathways we need to extract the EC numbers predictions from the GFF file. EC (Enzyme Commission) numbers are a numerical classification for enzymes based on the reaction they catalyse. Unless you know the EC scheme well they are generally not helpful by themselves. An example EC number is EC 3.1.1. The numbers represents the group the enzyme belongs to with the first number being the biggest group. From highest to the lowest grouping 3.1.1 represents: 3: Hydrolase enzymes. 3.1: Hydrolase enzymes that act on ester bonds. 3.1.1: Carboxylic Ester Hydrolases. With all that information we will extract the EC annotations from the GFF files. First we'll create a directory for the output. #Create EC directory mkdir EC We will now use a loop with various parts to create EC annotation files. The input .ec files required for MinPath are tab delimited with two columns: Protein/sequence id. E.g. GDGAPA_12670. EC annotation. E.g. 6.1.1.4. Note: The lack of \\ at the end of most of the lines is intentional. The below is all one command over multiple lines but loops work slightly different and don't need \\ in certain parts. Ensure you press enter at the end of each line. ls -1 */*gff3 | sed &quot;s|.*/||&quot; | sed &quot;s/.gff3//&quot; | while read bin do cat ${bin}/${bin}.gff3 | grep &quot;EC:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | \\ sed &quot;s/;.*EC:/\\t/&quot; | sed &quot;s/,.*//&quot; &gt; EC/${bin}.ec done 29.1.1 Code explanation That is quite a bit of code. You don't need to understand it as it should always work for Bakta output. If you are not currently interested you can skip to the MetaCyc prediction step If you are interested check out the contents of the 2 expandable boxes below. They contain explanations and example code to break down the code. Loop exterior We'll explain the code that initialises the loop. First we list all our .gff files on one line each (ls -1 *gff). Next, we remove the directory name (sed \"s|.*||\"). Then the suffix .gff is substituted with nothing sed \"s/.gff//\". This gives us the name of each bin (e.g. K1.1, K1.2). #List all gff files on one (-1) each ls -1 */*gff3 #Remove the directory name #You can use any character as the divider in sed #Useful when you want to move slashes from a file name ls -1 */*gff3 | sed &quot;s|.*/||&quot; #List all the file prefixes (on one line each) ls -1 */*gff3 | sed &quot;s|.*/||&quot; | sed &quot;s/.gff3//&quot; With the above code we can loop through each file prefix and use the variable bin (arbitrarily chosen) which contains the file prefix. This is carried out with while read bin. All lines between the do (start of the loop) and done (end of loop) are lines run in the loop. Run the loop with an echo command to show we are using the ${bin} variable to specify the input and output files. ls -1 */*gff3 | sed &quot;s|.*/||&quot; | sed &quot;s/.gff3//&quot; | while read bin do echo &quot;${bin}/${bin}.gff3 ../EC/${bin}.ec&quot; done Loop interior Now to look at the command within the loop: cat ${bin}.gff3 | grep &quot;EC:&quot; | cut -f 9 | \\ sed &quot;s/ID=//&quot; | sed &quot;s/;.*;Dbxref=/,/&quot; | \\ sed &quot;s/,.*EC:/\\t/&quot; | sed &quot;s/,.*//&quot; &gt;../EC/${bin}.ec A good way to figure out what a workflow is doing, is by building it up step by step. Run the first part of workflow and then add the next section, run, repeat. This shows you how each new section is affecting the output. Additionally, it is always good to run it on one file and head the output so we have a manageable amount of data to look at. We'll do that with the K1.1.gff3 file. Note: Remember we are adding head to the end for ease of viewing. Tips: Use the up arrow to go back to previously run commands that you can then edit. Remember the clear command. #Grab every line containing &quot;EC:&quot; cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | head #Cut out the 9th column/field (-f) (i.e. only keep the 9th column) #This is the attributes field in GFF3 #This contains a plethora of information including the EC annotation if present #cut uses tabs as the default column/field delimiter cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | cut -f 9 | head #The gff3 attributes field starts with the ID #We want to keep this but remove the &quot;ID=&quot; part cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | head #We don&#39;t want any of the info between the ID and the EC number #Therefore we want to remove everything (.*) between # the first &quot;;&quot; (at the end of the ID info) # and &quot;EC=&quot; #We&#39;ll replace this with a \\t to seprarate the ID and EC # columns with a tab (required by MinPath) cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | \\ sed &quot;s/;.*EC:/\\t/&quot; | head #Finally remove all the info after the EC number #This info will be after the last , cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | \\ sed &quot;s/;.*EC:/\\t/&quot; | sed &quot;s/,.*//&quot; | head In the looped command, output is written into files: &gt; EC/${bin}.ec. 29.2 MetaCyc predictions With our .ec files we can create our MetaCyc predictions. First we'll change directory into the EC directory. Then create an output directory for the MetaCyc predictions. cd ./EC mkdir ../MetaCyc Now we can loop through the file suffixes to run MinPath. ls -1 *ec | sed &quot;s/.ec//&quot; | while read bin do python /pub14/tea/nsc206/git_installs/Minpath/MinPath.py \\ -ec ${bin}.ec \\ -report ../MetaCyc/${bin}.minpath done A lot of output will be printed to screen but this can be ignored unless you see warnings. 29.3 MetaCyc report First, change directory into the MetaCyc directory. #Change directory cd ../MetaCyc #List contents ls From the CoCoPyE results we found that bin K1.22 was very good with a quality score &gt;98%. We will therefore have a look at its output. Have a look at the .minpath -report file for K1.22. less K1.22.minpath The file contains the following columns Pathway ID Pathway reconstruction: Only available for genomes annotated in MetaCyc database. Naive: Indicates if pathway was reconstructed by the naive mapping approach (1) or not (0). Minpath: Indicates if the pathway was kept (1) or removed (0) by MinPath. Fam0: The number of families involved in the pathway. Fam-found: Number of families in pathway that were annotated/found. Name: Description of pathway. Quit (q) less when you are happy. There are some quick things we can do in bash with these files. #Count number of pathways found in each bin with word count wc -l *minpath #Grab every line with &quot;PWY-6972&quot; from every file grep &quot;PWY-6972&quot; *minpath | less 29.4 KEGGs You can also get KEGG information with MinPath. The code: #Change to correct directory cd ~/8-Annotation #Make directory for KEGGs mkdir KEGG #KEGG extractions ls -1 */*gff3 | sed &quot;s|.*/||&quot; | sed &quot;s/.gff3//&quot; | while read bin do cat ${bin}/${bin}.gff3 | grep &quot;KEGG:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | \\ sed &quot;s/;.*KEGG:/\\t/&quot; | sed &quot;s/,.*//&quot; &gt; KEGG/${bin}.kegg done #Make directory for KEGG minpath output mkdir KEGG_minpath #Change directory to KEGG cd KEGG #Run MinPath ls -1 *kegg | sed &quot;s/.kegg//&quot; | while read bin do python /pub14/tea/nsc206/git_installs/Minpath/MinPath.py \\ -ko ${bin}.kegg \\ -report ../KEGG_minpath/${bin}.minpath done With these files you can then investigate what bins have which pathways. Additionally, with more samples analysed you can determine which samples have which pathways present. Splendid! That is the end of the workshop materials aside from the appendix in the next chapter. I hope you enjoyed it and foudn it useful. "],["30-Appendix.html", "A Mamba installs A.1 Mamba installation and environment B Next steps C Manuals D Obtaining Read Data", " A Mamba installs A.1 Mamba installation and environment Mamba is a reimplementation of conda. It is a great tool for installing bioinformatic packages including R packages. Mamba github: https://github.com/mamba-org/mamba The best way to use Mamba is to install Miniforge. It has both Conda and Mamba commands. Miniforge installation: https://github.com/conda-forge/miniforge Mamba guide: https://mamba.readthedocs.io/en/latest/user_guide/mamba.html A.1.1 shotgun_meta To create the mamba environment shotgun_meta run the below commands in your bash. You will need to have installed mamba first. #shotgun_meta mamba create -n shotgun_meta mamba activate shotgun_meta #Install packages mamba install -c bioconda fastqc trim-galore multiqc bowtie2 kraken2 \\ krona bracken lefse flash megahit quast metabat2 prokka bbmap bakta circos #Update krona taxonomy database ktUpdateTaxonomy.sh #Install GenoVi via pip pip install genovi You will need to install the Bakta database as well. This requires you have a directory to store the database. In the below example you will need a directory with the path ~/databases/bakta_db, of course feel free to use a different location you have. bakta_db download --output ~/databases/bakta --type full Install MinPath via git. I suggest creating a directory called \"git_installs\" in your home directory and running this code there. #git_installs directory mkdir ~/git_installs cd ~/git_installs #MinPath git clone https://github.com/mgtools/MinPath To run MinPath in your own machines you will need to use the full path of the python file. If you installed these in your \"~/git_installs\" examples are below. #MinPath help page python ~/git_installs/Minpath/MinPath.py -h A.1.2 cocopye To create the mamba environment cocopye run the below commands in your bash. You will need to have installed mamba first. #checkm mamba create -n cocopye mamba activate checkm2 mamba install bioconda::checkm2=1.1.0 #Download required databases checkm2 database --download A.1.3 bibakery3.9 To create the mamba environment biobakery3.9 run the below commands in your bash. You will need to have installed mamba first. #biobakery3 mamba create -n biobakery3.9 mamba activate biobakery3.9 #Add required channels conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda config --add channels biobakery #Install huamnn3.9 mamba install -c biobakery humann=3.9 #Install hclust2 and matplotlib mamba install bioncda::hclust2=1.0.0 conda-forge::matplotlib=3.7.3 Next you will need to install the HUMAnN and MetaPhlAn databases. #Ensure you have biobakery3 mamba environment activated #Update HUMAnN databases humann_databases --download chocophlan full /path/to/databases --update-config yes humann_databases --download uniref uniref90_diamond /path/to/databases --update-config yes humann_databases --download utility_mapping full /path/to/databases --update-config yes #Install MetaPhlAn databases metaphlan --install --index mpa_vOct22_CHOCOPhlAnSGB_202403 Installing the MetaPhlAn databases may not work with the command above. If not follow the below instructions. Go to the following link: http://cmprod1.cibio.unitn.it/biobakery4/metaphlan_databases/ Download the following files (or the most recent vOct22 files): mpa_vOct22_CHOCOPhlAnSGB_202403.md5 mpa_vOct22_CHOCOPhlAnSGB_202403.tar bowtie2_indexes/mpa_vOct22_CHOCOPhlAnSGB_202403_bt2.md5 bowtie2_indexes/mpa_vOct22_CHOCOPhlAnSGB_202403_bt2.tar Put these in the MetaPhlAn databases directory. This will be in your mamba environment directory. E.g.: ~/mambaforge/envs/biobakery3.9/lib/python3.7/site-packages/metaphlan/metaphlan_databases Note: this may not be your exact path. Run metaphlan --install --index mpa_vOct22_CHOCOPhlAnSGB_202403 again. Untar the CHOCOPhlan tar file Change directory to your MetaPhlAn database directory. tar -xvf mpa_vOct22_CHOCOPhlAnSGB_202403.tar Once the databses are all setup you can test HUMAnN. humann_test B Next steps Below are some good links to start with before carrying out your own projects. KrakenTools: A suite of scripts to be used alongside the Kraken, KrakenUniq, Kraken 2, or Bracken programs. https://github.com/jenniferlu717/KrakenTools A review of computational tools for generating metagenome-assembled genomes from metagenomic sequencing data https://www.sciencedirect.com/science/article/pii/S2001037021004931 A review paper on metagenome assembly approaches Genome-resolved metagenomics using environmental and clinical samples: https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbab030/6184411 bioBakery tools for meta'omic profiling https://github.com/biobakery/biobakery Assessing the performance of different approaches for functional and taxonomic annotation of metagenomes https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6289-6 MicrobeAnnotator: Easy-to-use pipeline for the comprehensive metabolic annotation of microbial genomes. https://github.com/cruizperez/MicrobeAnnotator MetaEuk: Functional annotation of eukaryotic metagenome https://github.com/soedinglab/metaeuk _ Methods for Metagenomic data visualisation and analysis https://www.researchgate.net/publication/318252633_Methods_for_The_Metagenomic_Data_Visualization_and_Analysis Visualizing metagenomic and metatranscriptomic data: A comprehensive review https://www.sciencedirect.com/science/article/pii/S2001037024001430 C Manuals Conda: https://conda.io/projects/conda/en/latest/user-guide/getting-started.html FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ MultiQC: https://multiqc.info/ Trim Galore: https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/ Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml samtools: http://www.htslib.org/ BBTools: https://jgi.doe.gov/data-and-tools/bbtools/ Kraken2: https://github.com/DerrickWood/kraken2/wiki/Manual Krona: https://github.com/marbl/Krona/wiki/KronaTools Bracken: https://ccb.jhu.edu/software/bracken/index.shtml?t=manual LEfSe: https://huttenhower.sph.harvard.edu/lefse/ HUMAnN 3.0: https://huttenhower.sph.harvard.edu/humann/ MetaPhlAn 4.0: https://github.com/biobakery/MetaPhlAn/wiki/MetaPhlAn-4 Biobakery: https://github.com/biobakery/biobakery hclust2: https://github.com/SegataLab/hclust2 MegaHit: https://github.com/voutcn/megahit BWA: https://github.com/lh3/bwa minimap2: https://github.com/lh3/minimap2 MetaBAT2: https://bitbucket.org/berkeleylab/metabat/src/master/ CoCoPyE: https://github.com/gobics/cocopye PhyloPhlAn: https://github.com/biobakery/phylophlan/wiki Prokka: https://github.com/tseemann/prokka MinPath: https://github.com/mgtools/MinPath/blob/master/readme MetaCyc: https://metacyc.org/ D Obtaining Read Data The following commands can be used to obtain the sequence data used in this practical, directly from the EBI metagenomics site. It is worth noting that these are the full set of data, not like the miniaturised version you have used in the tutorial. wget -O K1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_1.fastq.gz wget -O K1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_2.fastq.gz wget -O K2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_1.fastq.gz wget -O K2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_2.fastq.gz wget -O K3_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505095/ERR505095_1.fastq.gz wget -O K3_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505095/ERR505095_2.fastq.gz wget -O K4_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505096/ERR505096_1.fastq.gz wget -O K4_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505096/ERR505096_2.fastq.gz wget -O K5_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505097/ERR505097_1.fastq.gz wget -O K5_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505097/ERR505097_2.fastq.gz wget -O K6_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505098/ERR505098_1.fastq.gz wget -O K6_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505098/ERR505098_2.fastq.gz wget -O K7_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505099/ERR505099_1.fastq.gz wget -O K7_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505099/ERR505099_2.fastq.gz wget -O K8_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505100/ERR505100_1.fastq.gz wget -O K8_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505100/ERR505100_2.fastq.gz wget -O K9_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505101/ERR505101_1.fastq.gz wget -O K9_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505101/ERR505101_2.fastq.gz wget -O K10_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505103/ERR505103_1.fastq.gz wget -O K10_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505103/ERR505103_2.fastq.gz wget -O K11_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505105/ERR505105_1.fastq.gz wget -O K11_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505105/ERR505105_2.fastq.gz wget -O K12_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505106/ERR505106_1.fastq.gz wget -O K12_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505106/ERR505106_2.fastq.gz wget -O W1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_1.fastq.gz wget -O W1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_2.fastq.gz wget -O W2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505088/ERR505088_1.fastq.gz wget -O W2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505088/ERR505088_2.fastq.gz wget -O W3_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505084/ERR505084_1.fastq.gz wget -O W3_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505084/ERR505084_2.fastq.gz wget -O W4_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505085/ERR505085_1.fastq.gz wget -O W4_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505085/ERR505085_2.fastq.gz wget -O W5_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505086/ERR505086_1.fastq.gz wget -O W5_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505086/ERR505086_2.fastq.gz wget -O W6_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505087/ERR505087_1.fastq.gz wget -O W6_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505087/ERR505087_2.fastq.gz wget -O W7_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505089/ERR505089_1.fastq.gz wget -O W7_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505089/ERR505089_2.fastq.gz wget -O W8_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505091/ERR505091_1.fastq.gz wget -O W8_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505091/ERR505091_2.fastq.gz wget -O W9_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505092/ERR505092_1.fastq.gz wget -O W9_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505092/ERR505092_2.fastq.gz wget -O W10_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505093/ERR505093_1.fastq.gz wget -O W10_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505093/ERR505093_2.fastq.gz wget -O W11_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505094/ERR505094_1.fastq.gz wget -O W11_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505094/ERR505094_2.fastq.gz wget -O W12_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505083/ERR505083_1.fastq.gz wget -O W12_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505083/ERR505083_2.fastq.gz "]]
