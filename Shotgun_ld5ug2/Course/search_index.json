[["01-Shotgun_metagenomics.html", "Shotgun Metagenomics Chapter 1 Introduction Table of contents", " Shotgun Metagenomics Sam Haldenby and Matthew R. Gemmell 2025-09-24 Chapter 1 Introduction There are many unknown and unculturable microbes found in a vast array of different environments. Shotgun metagenomics is an approach to capture all the genetic information in a sample, providing the taxonomic and metabolic information of all present organisms. In this course we will analyse shotgun metagenomic sequencing data from stool samples to compare Western and Korean diets. Sessions will start with a brief presentation followed by self-paced computer practicals guided by this online interactive book. This book will contain theory and practice code. Multiple choice questions will guide the interpretation of results. At the end of the course learners will be able to: Carry out quality control of sequencing data, including host removal. Explain how Kraken2 utilises k-mers to taxonomically classify sequence data. Quantify taxonomic composition of shotgun metagenomic data with Kraken2 &amp; Bracken. View taxonomic abundances with Krona. Utilise LEfSe for biomarker detection. Profile the microbial pathways in metagenomic sequencing data with the bioBakery suite of tools, including HUMAnN. Perform a metagenome assembly with MEGAHIT, and assess it with QUAST. Carry out genomic binning of the metagenome assembly with MetaBAT2 to try to separate the different species present. Functionally annotate the bins with Bakta. Table of contents Overview Raw data Trimming data Host removal Taxonomic profiling Functional profiling Metagenome assembly Binning Functional annotation Appendix This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Overview.html", "Chapter 2 Overview 2.1 What is metagenomics? 2.2 Why metagenomics? 2.3 Metagenomics vs Metagenetics 2.4 Tutorial overview", " Chapter 2 Overview 2.1 What is metagenomics? Meta /ˈmɛtə/ : prefix meaning “higher” or “beyond” Metagenomics is the study of genes and genetic material recovered from environmental samples (whether from the sea, soil, human gut, or anywhere else you can imagine). Unlike genomics, metagenomics deals with a multitude of usually diverse species rather than focussing on a single species/genome. 2.2 Why metagenomics? Microbes exist virtually everywhere on Earth, even in some of the most seemingly hostile environments. Every process on our planet is influenced in some way by the actions of microbes, and all higher organisms are intrinsically associated with microbial communities. While much can be learned from studying the genome of a single microbial species in isolation, it does not provide us with any information regarding that species' neighbours, i.e. what else is in its natural environment? Metagenomics offers a top-down approach which allows researchers to investigate and understand interactions between species in different environments, thus providing a much broader and complete picture. 2.3 Metagenomics vs Metagenetics Broadly speaking, there are two families of metagenomic analysis: Amplicon-based: This utilises sequencing data generated from amplified marker sequences, for example, regions of the 16S rRNA. Sequences are clustered together and taxonomically assigned to estimate the species abundance in a sample. This is sometimes referred to as metagenetics, as it does not consist of any genomic analysis beyond the marker gene regions. Shotgun: This utilises sequencing data generated from random fragments from total genomic DNA from environmental samples, rather than targeting specific genes. This approach allows for not only species abundance determination but direct functional analysis, too, due to having information on a wide range of genetic data sampled from the population. This is sometimes referred to as metagenomics, as it involves genome-wide analyses. Shotgun metagenomics is the focus of this practical session. 2.4 Tutorial overview 2.4.1 Basics This tutorial and practical session focuses on performing a range of metagenomic analyses using shotgun sequence data from the Illumina platforms. The analyses discussed here are by no means exhaustive and are instead intended to provide a sample of what can be done with a metagenomic dataset. 2.4.2 Structure We prefer to allow people to work at a pace that they are comfortable with rather than ensuring that everyone is at the same point of the tutorial at the same time. There will be no instructor telling you what to type and click. Instead, everything you require to carry out the practical is written in this document. Take your time; it's important to spend some time understanding why you are running the commands, rather than simply typing them out. If at any point you are having trouble or have a question, let one of us know and we'll provide 1-to-1 assistance. 2.4.3 Content This practical is broken up into the following broad sections. Raw data: We will first link to a dataset that we have downloaded for this tutorial. We will take a quick look at what the sequence files look like and briefly discuss the origin of the samples. Trimming data: This entails preprocessing our data to ensure that it is of good quality. Host removal: When sequencing the genomic content of host's microbiota (bacteriome, archaeome, mycobiome, and more) it is likely you will also sequence the host's genome. This step shows a method of removing possible host contamination. Taxonomic profiling: We will analyse the dataset to determine the species abundance in each sample. Following this, we will visualise the data and compare the samples. Functional profiling: We will analyse the dataset to determine the pathway abundance and completeness in each sample. Following this, we will visualise the data and compare the samples. Metagenome assembly: Here, we will move away from just analysing the reads directly and will assemble the metagenome into contigs. Prior to this, we will 'stitch' the reads together to ensure we get the best assembly possible. Binning: This step attempts to seperate each assembled genomes into bins. These genome assemblies are called Metagenome-assembled Genomes (MAGs). Functional annotation: We will take our MAGs, predict genes and then functionally annotate them with MetaCyc. "],["03-Cluster_Introduction.html", "Chapter 3 Cluster Introduction 3.1 Logon instructions 3.2 The Terminal Window", " Chapter 3 Cluster Introduction 3.1 Logon instructions For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent. You will now be in a logged-in Linux VNC desktop with two terminals. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance. If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal. How to zoom with your browser You may need to zoom out with your browser so you can see the full webVNC window. Chrome: Click on the three dots in vertical line ( ) on the top left for a dropdown menu which includes zoom options. Edge: Click on the three horizontal lines ( ) on the top left for a dropdown menu which includes zoom options. Firefox: Click on the three dots in horizontal line ( ) on the top left for a dropdown menu which includes zoom options. These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Mint Linux The following link is a guide to install Mint Linux: https://linuxmint-installation-guide.readthedocs.io/en/latest/ If you have a windows machine and would like to install linux on it you can install Windows Subsystem for Linux (WSL). The following link is a guide to install linux via (WSL): https://learn.microsoft.com/en-us/windows/wsl/install 3.2 The Terminal Window In our case the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences it may look different. Already there is useful information for us on the terminal window. nsc206: This is the login name, also known as the username. In this case nsc206 is a demonstrator's account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into. ada02: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux (OS) and others '~' is a shortcut to the user's home directory. Everything after the '$' is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen, choose Terminal. "],["04-Start_conda.html", "Chapter 4 Startup &amp; mamba", " Chapter 4 Startup &amp; mamba During this practical you will use a number of installed programs and scripts. To ensure that the system knows where to look for the scripts, run the following command (ensure this starts with a full stop and a space .): . useshotgun The use scripts in this workshop are custom scripts that set up mamba environments. You can look at the above script with less /usr/local/bin/useshotgun if you are interested in its contents. Also, there’s a chance you’re currently not in your home directory, so let’s make sure you are with the following command: cd ~ "],["05-Raw_data.html", "Chapter 5 Raw data 5.1 Obtaining the data 5.2 Checking quality control", " Chapter 5 Raw data The very first thing we need to do is to obtain a dataset to work with. The European Bioinformatics Institute (EBI) provides an excellent metagenomics resource (https://www.ebi.ac.uk/metagenomics/) which allows users to download publicly available metagenomic and metagenetic datasets. Have a browse of some of the projects by selecting one of the biomes on the website. We have selected a dataset from this site that consists of DNA shotgun data generated from 24 human faecal samples. 12 of these samples are from subjects who were fed a Western diet and 12 are from subjects who were fed a Korean diet. This dataset comes from the EBI metagenomics resource (https://www.ebi.ac.uk/metagenomics/projects/ERP005558). 5.1 Obtaining the data First, we need to create a directory to put the data in and then change directory to it. mkdir 1-Raw cd 1-Raw Now we can generate a symbolic links (i.e. shortcut) to the raw sequence data files, which will appear in the current directory: ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/raw_fastq/* . If you would like to know more about the ln command please check out: https://linuxize.com/post/how-to-create-symbolic-links-in-linux-using-the-ln-command/. Check the symbolic links are in your current directory: ls There should be six files in the directory, two for each sample in the dataset. e.g. K1_R1.fastq.gz The file ID has three components: K1 is the sample ID. R1 is for the forward reads in the Illumina reads pair (R2 is for the set corresponding to the other end of the reads). fastq.gz tells us that this is a gzipped FASTQ file. The three samples are: K1: Fecal sample of individual of Korean diets K2: Fecal sample of individual of Korean diets W1: Fecal sample of individual of Western diets So, what do the R1 and R2 actually mean? With Illumina sequencing the vast majority of sequencing is paired end. i.e. DNA is first fragmented and both ends of each fragment are sequenced as shown here: This results in two sequences generated for each sequenced fragment: One reading in from the 3' end (R1) and the other reading in from the 5' end (R2). FASTQ is a sequence format much like FASTA, with the addition of quality scores. To see what a FASTQ file looks like, we can inspect the first few lines on one of our sequence files: zcat K1_R1.fastq.gz | head -n 4 | less -S The pipe symbol ( | ) is used to pass the output of one command as input to the next command. So, this command: Shows the unzipped contents of the FASTQ file Displays only the first 4 lines Displays them without wrapping lines (with –S, for easy viewing) The lines displayed represent one FASTQ sequence entry, or one read of a read pair: The corresponding second read can be viewed by running the same command on K1_R2.fastq.gz. The first line is the read identifier, the second line is the sequence itself, the third line is a secondary header (which is usually left blank except for '+') and the fourth line is the sequence quality score: For each base in the sequence, there is a corresponding quality encoded in this string of characters. To return to the command prompt, press q. Due to computational constraints, the files you have linked to are a subset of the original data (i.e. 1 million read pairs from each sample). 5.2 Checking quality control We can generate and visualise various sequence data metrics for quality control purposes using FastQC. We will run FastQC on the R1 and R2 reads separately as it is good to visualise them in two different reports. This is because R1 and R2 reads have different quality patterns, generally due to the poorer quality of R2. Run FastQC on the files: #R1 fastqc #Make an output directory mkdir R1_fastqc #Run fastqc on all the R1.fastq.gz files #* matches any pattern #*R1.fastq.gz matches any file that ends R1.fastq.gz in the current directory #-t 3 indicates to use 3 threads, chosen as there are three R1 files fastqc -t 3 -o R1_fastqc *R1.fastq.gz #R2 fastqc #Make output directory mkdir R2_fastqc #Run fastqc fastqc -t 3 -o R2_fastqc *R2.fastq.gz Once the FastQC commands are run we can run MultiQC to create interactive html reports for the outputs. #R1 multiqc fastqc report #Create output directory mkdir R1_fastqc/multiqc #Create multiqc output multiqc -o R1_fastqc/multiqc R1_fastqc #R2 multiqc fastqc report #Create output directory mkdir R2_fastqc/multiqc #Create multiqc report multiqc -o R2_fastqc/multiqc R2_fastqc Once completed, view the MultiQC reports (NB: The &amp; runs the command in the background, therefore allowing you to continue to run commands while Firefox is still open). This is a longer command so we've split it across multiple lines with bash escape. A \\ at the end of a line allows you to press return/enter without running the command, meaning you can continue to add to that command. When this happens, the $ changes to a &gt;. For more information please see our Intro to Unix materials Note if you do use the \\ character, the next key you press must be return/enter. If you use \\ in the middle of a line without pressing return afterwards, it will break the command! firefox R1_fastqc/multiqc/multiqc_report.html \\ R2_fastqc/multiqc/multiqc_report.html &amp; The FastQC report (via MultiQC) contains a number of metrics. The \"Sequence Quality Histograms\" shows the sequence quality across the length of the reads, you can hover over each line to show which sample it belongs to. Note how quality decreases as the length of the read increases. While this is normal with Illumina sequencing, we will improve the situation a bit in the next chapter. For more information on the plots of FactQC please see this online resource. Once you have finished inspecting, minimise the Firefox window. "],["06-Trimming_data.html", "Chapter 6 Quality control 6.1 Removing adapters and low quality bases 6.2 Rename the files 6.3 Inspect the trimmed data", " Chapter 6 Quality control Now that we've obtained the raw data and had a look at it, we should clean it up. With any sequencing data, it is very important to ensure that you use the highest quality data possible: Rubbish goes in, rubbish comes out. There are two main methods employed to clean sequence data, and a third method specific to some metagenomic datasets. Remove low quality bases from the end of the reads: These are more likely to be incorrect, so are best trimmed off. Remove adapters: Sometimes sequencing adapters can be sequenced if the sequencing runs off the end of a fragment. Host removal: If a metagenomic sample derives from a host species then it may be advisable to remove any reads associated with the host genome. 6.1 Removing adapters and low quality bases Go back to your home directory and create a new directory where we will clean the sequences up: cd .. mkdir 2-Trimmed cd 2-Trimmed You are now in your newly created directory. Here we will run Trim Galore! which removes low quality bases and adapters. trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/K1_R1.fastq.gz ../1-Raw/K1_R2.fastq.gz This command will remove any low quality regions from the end of both reads in each read pair (quality score &lt; 20). Additionally, if it detects four or more bases of a sequencing adapter, it will trim that off too. Task: Rerun this command for the other two samples (K2 and W1). Try to run these without looking at the help box below. trim_galore commands #K2 trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/K2_R1.fastq.gz ../1-Raw/K2_R2.fastq.gz #W1 trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/W1_R1.fastq.gz ../1-Raw/W1_R2.fastq.gz 6.2 Rename the files Once that is complete list the contents of your directory: ls You will notice that we have a new bunch of files created: 2 new read files for each sample along with a trimming report for each file trimmed. However, the new names are needlessly long. For example K1_R1_val_1.fq.gz could be shortened to K1_R1.fq.gz. So, we'll rename all of the files with the mv command: mv K1_R1_val_1.fq.gz K1_R1.fq.gz mv K1_R2_val_2.fq.gz K1_R2.fq.gz mv K2_R1_val_1.fq.gz K2_R1.fq.gz mv K2_R2_val_2.fq.gz K2_R2.fq.gz mv W1_R1_val_1.fq.gz W1_R1.fq.gz mv W1_R2_val_2.fq.gz W1_R2.fq.gz Tip: If you want to edit and reuse previous commands, press the up arrow key. Task: Briefly inspect the log files to see how the trimming went (e.g. K1_R1.fastq.gz_trimming_report.txt). 6.3 Inspect the trimmed data To see what difference the trimming made, run FastQC and MultiQC again on the trimmed output files and view it. #R1 fastqc and multiqc mkdir R1_fastqc fastqc -t 3 -o R1_fastqc *R1.fq.gz mkdir R1_fastqc/multiqc multiqc -o R1_fastqc/multiqc R1_fastqc Task Run FastQC and MultiQC for the R2 files and then view the R1 and R2 MultiQC reports with firefox. Try to run the commands without looking at the help box below. How does the quality compare to the untrimmed data? R2 commands #R2 fastqc and multiqc mkdir R2_fastqc fastqc -t 3 -o R2_fastqc *R2.fq.gz mkdir R2_fastqc/multiqc multiqc -o R2_fastqc/multiqc R2_fastqc "],["07-Host_removal.html", "Chapter 7 Host removal 7.1 Index reference 7.2 Alignment 7.3 Unmapped read extraction 7.4 Re-pair 7.5 Host removal summary", " Chapter 7 Host removal It is good practice to remove any host sequences from your data before further analysis. A good method for this is to align/map your reads to a reference of your host genome and remove the mapped sequences (i.e sequences we believe belong to the host). If there is no host genome available before you start your sample collections and sequencing it may be a good idea to attempt to sequence and assemble the host genome. We would recommend long read technologies for single genome assembly projects. This chapter contains a small example on how to carry out host removal. It uses only a section of a human (host of our samples) reference genome assembly. In real life you should use the entire reference. The first step is to copy over the reference fasta file we will use. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/GRCh38_slice.fasta . 7.1 Index reference We will will use the Bowtie2 aligner for mapping/aligning. Prior to alignment/mapping we need to index our reference. bowtie2-build GRCh38_slice.fasta GRCh38_slice.fasta If you use ls you will now see a bunch of files starting with GRCh38_slice.fasta and ending with various suffixes that contain bt2. These are the index files which allow us to use the reference with Bowtie2. 7.2 Alignment With the indexed reference we will align the K1 reads to the reference. This creates a BAM file that contains alignment and read information (K1_mapped.bam). bowtie2 -x GRCh38_slice.fasta -1 K1_R1.fq.gz -2 K1_R2.fq.gz \\ -p12 2&gt; K1_bowtie2_out.log | samtools view -b -S -h &gt; K1_mapped.bam Parameters This command is split into two commands. The first is bowtie2 that creates the alignment. The parameters for the command are: -x: Indexed reference the reads will be aligned to. -1: The forward reads. -2: The reverse reads. -p: Number of threads to be used. 12 in this case. 2&gt;: This will cause the standard error to be redirected to the chosen file. In this case K1_bowtie2_out.log. This is useful for commands that may produce a lot of output to screen. If an error occurs you can view this file to see the error messages. The alignment is then piped (|) to the command samtools view. For more information on pipes please see our Intro to Unix course book. The parameters for samtools view are: -b: Output the alignment as a BAM file. BAM files are a binary form of SAM files so they are smaller in memory size. If you are interested in the SAM format please see its specification file. -S: Auto detect input format. -h: Include header. The binary alignment is redirected to a new file called K1_mapped.bam. For more information on redirection please see our Intro to Unix course book. 7.3 Unmapped read extraction Next step is to extract the reads that did not map to the host reference from the K1_mapped.bam file with the samtools fastq command (unmapped reads). samtools fastq -f 4 -1 K1_R1.u.fastq -2 K1_R2.u.fastq K1_mapped.bam Parameters -f: Output reads that only include the SAM flag. In this case 4 stands for unmapped reads. Therefore, our resulting fastq files will only contain unmapped reads. The following link is very useful to create a SAM flag you may need: https://broadinstitute.github.io/picard/explain-flags.html. -1: The output R1 fastq file of unmapped reads. -2: The output R2 fastq file of unmapped reads. Last file: The last file name is the input file for the command, K1_mapped.bam in this case. This step may make unmatched paired files (why we have .u. in the output file names). This occurs when a read from R2 is removed but the matching read in R1 is not removed, or vice versa. This will cause issues for further analysis. 7.4 Re-pair The below BBTools command will re-pair the reads by removing reads with a missing pair. The command ensures the order of the reads are identical in the 2 output paired files. repair.sh in1=K1_R1.u.fastq in2=K1_R2.u.fastq \\ out1=K1_R1.final.fastq out2=K1_R2.final.fastq \\ outs=K1_singletons.fastq Parameters in1=: The input R1 fastq file of unmapped unpaired reads. in2=: The input R2 fastq file of unmapped unpaired reads. out1=: The output R1 fastq file of unmapped paired reads. out2=: The output R2 fastq file of unmapped paired reads. outs=: The output fastq file containing the left over singletons (a sequence missing a pair). This file can normally be ignored. 7.5 Host removal summary We have run through quality control including host removal for our K1 sample. As our data has pretty much no human data we will skip this step for the other samples and use the trimmed data for the downstream analysis. In a real analysis project you would use a whole genome reference for your host. However, that would have taken too long for this practical. The most current Human reference (when this was written) is GRCh38. We used a random 10kb section to align our reads to. For more resources on the Human reference please see: https://www.ncbi.nlm.nih.gov/genome/guide/human/ The assembly we used was: https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz "],["08-Taxonomic_profiling.html", "Chapter 8 Taxonomic profiling", " Chapter 8 Taxonomic profiling There are a number of methods for determining the species composition of a metagenomic dataset, but for the purposes of this practical we will use Kraken2 &amp; Bracken (Bayesian Reestimation of Abundance with KrakEN). Kraken2 classifies short DNA with taxonomic labels and is frequently used for metagenomic studies. Bracken uses the taxonomic labels assigned by Kraken2 to compute the abundance of species in a set of DNA sequences. First, we'll make a new directory for it and move into it, after returning home: cd .. mkdir 3-Taxonomy cd 3-Taxonomy "],["09-Kraken2.html", "Chapter 9 Kraken2 9.1 Kraken2: run 9.2 Kraken2: output 9.3 Kraken 2: MCQs", " Chapter 9 Kraken2 Prior to running Kraken2 we need to set a variable so Kraken2 knows where to look for the databases it will use. export KRAKEN2_DB_PATH=/pub14/tea/nsc206/NEOF/Shotgun_metagenomics/kraken2_db Note: You can look at the contents of the above directory to see it currently contains the MiniKraken database. This database contains only a subset of the bacteria, archaea, and viral Kraken2 libraries. This is used in this practical due to restrictions on time and computational resources. For your own analyses we would recommend the full Kraken2 database which uses all the bacteria, achaeal and viral complete genomes that are in Refseq at the time of building. See the following links for info on installing the databases. Standard Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database Custom Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases 9.1 Kraken2: run Now, run Kraken2 on sample K1 by running the following command. Note: We are not using the host removed data. This is to save time. In your own analysis ensure you are using host removed data. kraken2 --paired --db minikraken2_v1_8GB \\ --output K1.kraken --report K1.kreport2 \\ ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz Parameters --paired: Indicates that we are providing paired reads to Kraken2. Internally, Kraken2 will concatenate the R1 and R2 reads into one sequence with an N between them. --db: Specify the Kraken2 database to be used for taxonomic classification. Previous to the command we set the KRAKEN2_DB_PATH so in this case the command will look for the directory called minikraken2_v1_8GB within KRAKEN2_DB_PATH. Alternatively the full path of the required database could be provided (/pub14/tea/nsc206/NEOF/Shotgun_metagenomics/kraken2_db/minikraken2_v1_8GB). --output: The output file. More info below. --report: The output report file. More info below. ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz: The trimmed read pairs for K1, which we will use as input. 9.2 Kraken2: output There are two major output formats from Kraken2. Output file The --output parameter creates a .kraken file. Each sequence (or sequence pair, in the case of paired reads) classified by Kraken2 results in a single line of output. Kraken2's output lines contain five tab-delimited fields; from left to right, they are \"C\"/\"U\": a one letter code indicating that the sequence was either classified or unclassified. Sequence ID: Obtained from the FASTA/FASTQ header. Taxonomy ID: Assigned by Kraken2. This is 0 if the sequence is unclassified. Length of the sequence in bp: In the case of paired read data, this will be a string containing the lengths of the two sequences in bp, separated by a pipe character, e.g. \"98|94\". LCA mapping: A space-delimited list indicating the LCA (also known as MRCA) mapping of each k-mer in the sequence(s). For example, \"562:13 561:4 A:31 0:1 562:3\" would indicate that: 562:13 - The first 13 k-mers mapped to taxonomy ID #562 561:4 - The next 4 k-mers mapped to taxonomy ID #561 A:31 - The next 31 k-mers contained an ambiguous nucleotide 0:1 - The next k-mer was not in the database 562:3 - The last 3 k-mers mapped to taxonomy ID #562 Note: that paired read data will contain a \"|:|\" token in this list to indicate the end of one read and the beginning of another. Report file The --report parameter creates a .kreport2 file. This is the report output format. This is required for bracken. It is tab-delimited with one line per taxon. The fields of the output, from left-to-right, are as follows: Percentage of paired reads covered by the clade rooted at this taxon. Number of paired reads covered by the clade rooted at this taxon. Number of paired reads assigned directly to this taxon. A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., \"G2\" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank. NCBI taxonomic ID number Indented scientific name Screen output The output to screen will show how many sequences are classified. This will be lower than normal as we are using a mini Kraken2 database. Confidence threshold In a real analysis you may use the option --confidence which represents the \"Confidence score threshold\". The default is 0.0, which is the lowest, with the maximum value being 1. A good place to start may be 0.1. Too many classifications are removed if you attempt it with this dataset, due to the mini Kraken2 database used. More info on the confidence scoring can be found at: https://github.com/DerrickWood/kraken2/wiki/Manual#confidence-scoring Task Once the Kraken2 command has finished running, run it on the other two samples. Attempt the commands without looking at the help box. Hint: You will need to change all instances of K1 to K2 or W1 in the above command K2 &amp; W1 Kraken 2 commands #K2 kraken2 --paired --db minikraken2_v1_8GB \\ --output K2.kraken --report K2.kreport2 \\ ~/2-Trimmed/K2_R1.fq.gz ~/2-Trimmed/K2_R2.fq.gz #W1 kraken2 --paired --db minikraken2_v1_8GB \\ --output W1.kraken --report W1.kreport2 \\ ~/2-Trimmed/W1_R1.fq.gz ~/2-Trimmed/W1_R2.fq.gz 9.3 Kraken 2: MCQs Viewing the Kraken2 output files with your favourite text viewer (less, nano, vim, etc.), attempt the below MCQs. How many paired reads were unclassified for K1? 2 486,723 555,162 How many paired reads were classified for K2 (i.e. number of reads classified at root level and below)? 2 486,723 555,162 How many paired reads were assigned directly to root level for W1? 2 486,723 555,162 What percentage of W1's paired reads were assigned to the clade of Bacteroidetes (Phylum)? 0.12 0.59 14.89 What percentage of K2's paired reads were assigned to the clade of Rikenellaceae (Family)? 0.12 0.59 14.89 What percentage of K1's paired reads were assigned to the clade of Bacteroides helcogenes (Species)? 0.12 0.59 14.89 "],["10-Krona.html", "Chapter 10 Krona plot 10.1 Krona: run 10.2 Krona: visualise 10.3 Krona: MCQs", " Chapter 10 Krona plot Krona is an interactive metagenome species abundance visualisation tool. 10.1 Krona: run We can use the Kraken2 report files to create our Krona plots. With the below command we can import our Kraken2 taxonomy (within the report file) into a Krona html. ktImportTaxonomy -o kraken2.krona.html *.kreport2 -o is our output html file The final argument *.kreport2 represents all of our .kreport2 files in the current directory. The * is a wild-card, meaning any characters any number of times. Therefore *.kreport2 identifies the files K1.kreport2 K2.kreport2 and W1.kreport2. You will get a warning that not all taxonomy IDs were found. We will ignore this but in your own future installations this should be addressed with Krona's updateTaxonomy.sh command. 10.2 Krona: visualise We can view our interactive chart in a web browser. This shows the percentage of reads that were classified to various taxonomies at different levels. firefox kraken2.krona.html &amp; This is an interactive multi-tiered pie chart with many options. Some of the interactivity is described below: To choose a sample to view, click on the sample name in the top left. To zoom into a lower taxonomy, double click on the taxonomy's name on the pie chart. E.g. double click the word \"Bacteria\" on the pie chart to zoom into Bacteria and therefore ignore Eukaryota, and \"[other Root]\". To zoom out to a higher taxonomy, click on the taxonomy's name in the middle of the pie chart. E.g. click the word \"root\" in the middle of the pie chart to zoom back out from any lower level of taxonomy. To view percentage stats of a taxonomy, click on the taxonomy name on the pie chart. This will display the percentage this taxonomy covers of all the different taxonomies above it that it belongs to. This is displayed on the right side of the screen. E.g. Click on Pseudomonadota to see the percentage of the Root it accounts for and to see the percentage of Bacteria it accounts for in the sample. 10.3 Krona: MCQs Viewing the Krona, attempt the below MCQs. What percentage of the root was classified to Bacteria for K1? 2% 34% 35% What percentage of bacteria was classified to Pseudomonadota for K2? 2% 34% 35% What percentage of the root was classified to Myxococcota for W1? 2% 34% 35% What percentage of the Terrabacteria groups was classified to Cellulomonas gilvus for K2? 39% 8% 6% What percentage of Alphaproteobacteria was classified to Phenylobacterium immobile for K1? 39% 8% 6% What percentage of Viridiplantae (in Eukaryota) was classified to Parasponia for W1? 39% 8% 6% "],["11-Bracken.html", "Chapter 11 Bracken 11.1 Bracken: run 11.2 Bracken: output 11.3 Bracken: MCQs 11.4 Bracken: merging output 11.5 Bracken: extracting output", " Chapter 11 Bracken Bracken (Bayesian Reestimation of Abundance with KrakEN) uses taxonomy labels assigned by Kraken2 to compute estimated abundances of species in a metagenomic sample. 11.1 Bracken: run Just like with Krona we can use the Kraken2 report files to run bracken. bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \\ -i K1.kreport2 -o K1.bracken -w K1.breport2 -r 100 -l S -t 5 Parameters -d : Specifies the Kraken2 database that was used for taxonomic classification. In this case bracken requires the variable $KRAKEN2_DB_PATH so the option is provided the full path to the kraken database. For clarity try the command ls $KRAKEN2_DB_PATH/minikraken2_v1_8GB. -i : The Kraken2 report file, this will be used as the input. -o : The output Bracken file. Information about its contents is below. -w: Output report file. This contains the Bracken read counts in a kraken-style report. This is an essential file if you want to use the Bracken output in R using the phyloseq object. This is covered in our R community analysis workshop. We won't cover it more here. -r 100: This is the ideal length of the reads that were used in the Kraken2 classification. It is recommended that the initial read length of the sequencing data is used. We are using 100 here as we used a paired library of 100bp*2 reads. -l S: This specifies the taxonomic level/rank of the Bracken output. In this case S is equal to species with the other options being D, P, C, O, F and G. -t 5: This specifies the minimum number of reads required for a classification at the specified rank. Any classifications with fewer reads than the specified threshold will not receive additional reads from higher taxonomy levels when distributing reads for abundance estimation. Five has been chosen here for this example data but in real datasets you may want to increase this number (default is 10). 11.2 Bracken: output The output file of Bracken contains the following columns: Name: Name of taxonomy at the specified taxonomic level. Taxonomy ID: NCBI taxonomy id Level ID: Letter signifying the taxonomic level of the classification Kraken assigned read: Number of reads assigned to the taxonomy by Kraken2 Added reads with abundance reestimation: Number of reads added to the taxonomy by Bracken abundance reestimation. Total reads after abundance reestimation: Number from field 4 and 5 summed. This is the field that will be used for downstream analysis Fraction of total reads: Relative abundance of the taxonomy Task Repeat the above commands for K2 and W1 K2 &amp; W1 Bracken commands #K2 bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \\ -i K2.kreport2 -o K2.bracken -r 100 -l S -t 5 #W1 bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \\ -i W1.kreport2 -o W1.bracken -r 100 -l S -t 5 11.3 Bracken: MCQs Viewing the Bracken output files (.bracken) with your favourite text viewer (less, nano, vim, etc.), attempt the below MCQs. In K1, how many total reads after abundance reestimation are there for Prevotella fusca? 0.00011 16 702 In K2, how many reads after abundance reestimation were added for Bacteroides caccae? 0.00011 16 702 In W1, what is the fraction of total reads (after abundance reestimation) for Tannerella forsythia? 0.00011 16 702 11.4 Bracken: merging output To make full use of Bracken output, it is best to merge the output into one table. Before we do this we’ll copy the Bracken output of other samples that have been generated prior to the workshop. These are all either Korean or Western Diet samples. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/bracken/* . Now to merge all the K and W Bracken files. combine_bracken_outputs.py --files [KW]*.bracken -o all.bracken This output file contains the first three columns: name = Organism group name. This will be based on the TAX_LVL chosen in the Bracken command and will only show the one level. taxonomy_id = Taxonomy id number. taxonomy_lvl = A single string indicating the taxonomy level of the group. ('D','P','C','O','F','G','S'). After these columns are the following two columns for each sample. ${SampleName}.bracken_num: The number of reads after abundance reestimation ${SampleName}.bracken_frac: Relative abundance of the group in the sample 11.5 Bracken: extracting output We want a file with only the first column (organism name) and the bracken_num columns for each sample. To carry this out we first create a sequence of numbers that will match the bracken_num column numbers. These start at column 4 and are every even numbered column after this. We will use seq to create a sequence of numbers starting at 4 and including every second (2) number up to and including 50 with commas (,) as separators (-s). Note: The number 50 is chosen as 3 (first three info columns) + 24*2 (24 samples with 2 columns each) = 50. #Try out the seq command to see its output seq -s , 4 2 50 #Create variable bracken_num_columns=$(seq -s , 4 2 50) echo $bracken_num_columns Now to use the variable to extract the bracken_num columns plus the first column (species names). cat all.bracken | cut -f 1,$bracken_num_columns &gt; all_num.bracken "],["12-Lefse.html", "Chapter 12 LEfSe biomarker detection 12.1 LEfSe: add metadata 12.2 LEfSe: format 12.3 LEfSe: run 12.4 LEfSe: visualisation 12.5 LEfSe: MCQs 12.6 Kraken2 and Bracken databases", " Chapter 12 LEfSe biomarker detection We will use LEfSe (Linear discriminant analysis Effect Size) to determine which taxa can most likely explain the differences between the Western and Korean diet. LEfSe couples standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. It can be used with other features such as organisms, clades, operational taxonomic units, genes, or functions. In essence it allows for the detection of biomarkers when comparing sample groups. In the LEfSe terminology the sample groups are called the class. 12.1 LEfSe: add metadata We need to add metadata to our Bracken file to be ready for LEfSe. First we will copy the file so we have a backup in case we do anything wrong. cp all_num.bracken all_num.lefse.bracken Using your favourite text editor (e.g. nano, vim, etc.) add the following line to the top of your all_num.lefse.bracken file. The words are separated by tabs. If you are not sure how to carry out this task please ask a demonstrator. diet K K K K K K K K K K K K W W W W W W W W W W W W Note: The above is diet followed by 12 K and 12 W. The singular line should match the order of your samples within the file. This is the metadata line that LEfSe will use to determine which samples belong to each sample group, and therefore which to compare. In this case it is Korean diet samples versus Western diet samples. Issues with creating file? If you are having issues with creating and editing the file all_num.lefse.bracken you can copy a pre-made version. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/lefse/all_num.lefse.bracken . 12.2 LEfSe: format We need to further format and preprocess our file with a LEfSe script. lefse_format_input.py all_num.lefse.bracken all_num.lefse -c 1 -u 2 -o 1000000 Parameters all_num.lefse.bracken : Input Bracken file. all_num.lefse : Output file formatted for the run_lefse command, which we will soon run. -c 1 : Specifies the row with the class info. This is used to determine which samples will be compared against which samples. In this case it is the first row with the Ks and Ws. -u 2 : Specifies the row with the sample names. This is the second row in this case. -o 1000000 : An integer can be indicated to determine to what size (count sum value) each sample should be normalised to. LEfSe developers recommend 1000000 (1 million) when very low values are present. We generally always use 1 million for consistency. 12.3 LEfSe: run Now to run LEfSe. All we need to do is run the command with the formatted input and provide an output file name. lefse_run.py all_num.lefse all_num.lefse.out Output The output file is a tab-delimited file which contains a row for each species. Biomarkers will have the five columns below whilst non-biomarkers will have the first two followed by a \"-\" . Biomarker name Log of highest class average: I.e. get the class with the greater amounts of the biomarker, average the counts and then get the log of this value. Class with the greater amounts of biomarker LDA effect size: A statistical figure for LEfSe. p-value: Biomarkers must have a p-value of &lt;0.05 to be considered significant. The LDA effect size indicates how much of an effect each biomarker has. The default is to only count a species with an LDA effect size of greater than 2 or less than -2 as a biomarker. The further the LDA effect size is from 0 the greater the effect the species causes. Generally, it can be thought of as the order of magnitude difference in the abundance of the biomarker between the sample groups. 12.4 LEfSe: visualisation Next we can visualise the output. lefse_plot_res.py --dpi 200 --format png all_num.lefse.out biomarkers.png --dpi 200 : Dots per inch. This refers to the resolution of the output image. Normally publications want 300 dpi. We’ve chosen 200 as it is good quality and we will not be publishing these results. --format png : Format of output file. png is a commonly used file format for images. all_num.lefse.out : LEfSe output to visualise. biomarkers.png : Plot showing the LDA scores of the species detected as biomarkers. Colouring shows which class (K or W) the species is found in higher abundance. Look at the figure with firefox: firefox biomarkers.png 12.5 LEfSe: MCQs Interrogating the biomarkers.png plot and the all_num.lefse.out file, attempt the below MCQs. Note: In this instance green bars represent biomarkers in higher abundance in the W samples whilst the red bars represent biomarkers in higher abundance in the K samples. Which species biomarker causes the biggest effect in the W class? Adlercreutziaequolifaciens Alistipesshahii Methanosphaerastadtmanae Which species biomarker causes the biggest effect in the K class? Adlercreutziaequolifaciens Alistipesshahii Methanosphaerastadtmanae Which species biomarker (i.e. present in the plot) causes the lowest effect in the W class? Adlercreutziaequolifaciens Alistipesshahii Methanosphaerastadtmanae Which class has more biomarkers associated with it? Korean Western What is the LEfSe p-value for Campylobactercoli? 0.0022136919545913936 3.0266165524513937 3.314396201978439 What is the Log of highest class average for Streptococcussuis? 0.0022136919545913936 3.0266165524513937 3.314396201978439 What is the LDA effect size for Bifidobacteriumlongum? 0.0022136919545913936 3.0266165524513937 3.314396201978439 12.6 Kraken2 and Bracken databases In your own future analysis you will need to create your own Kraken2 and Bracken databases. Please see the following links on information for this: Kraken2 Standard Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database Custom Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases Bracken https://ccb.jhu.edu/software/bracken/index.shtml?t=manual#step1 This requires a Kraken2 database to be built first. "],["13-Functional_profiling.html", "Chapter 13 Functional profiling", " Chapter 13 Functional profiling It is possible to investigate functional differences between metagenome (and metatranscriptome) samples by directly interrogating the read data. We will look at how this can be done with a package called HUMAnN (The HMP Unified Metabolic Analysis Network), a bioBakery pipeline designed to accurately profile the presence/absence and abundance of microbial pathways in metagenomic sequencing data. HUMAnN is on its third version and was developed in tandem with the third version of MetaPhlAn, a computational tool for profiling the composition of microbial communities from metagenomic data. Please see below for a diagram showing the pipeline of HUMAnN: HUMAnN 4 HUMAnN 4 is currently in alpha: (https://forum.biobakery.org/t/announcing-humann-4-0-alpha/7531) I have tested it and could not install it correctly so we will not be teaching its use in this course till it is out of alpha and stable. "],["14-HUMAnN.html", "Chapter 14 HUMAnN 14.1 HUMAnN: mamba, directories, and files 14.2 HUMAnN: run 14.3 HUMAnN: output", " Chapter 14 HUMAnN First, we will carry out an example run of the software and briefly explore the output files. HUMAnN can take a long time to run so we will use a small amount of example data. Additionally, we will use a subset of the HUMAnN databases for the analysis but when running analysis on your own data you should use the full databases. Information on installing HUMAnN and its databases can be found on its online Home Page. 14.1 HUMAnN: mamba, directories, and files We need a new mamba environment. Open a new terminal (right click on the main screen background, choose Terminal) and run the below: . usebiobakery3.9 Make a new directory and move into it. mkdir ~/4-FunctionalProfiling cd ~/4-FunctionalProfiling Copy over some test data we will carry out the analysis on. This is a demonstration FASTQ file that we will use. It will be small enough to run HUMAnN in a reasonable amount of time. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/humann/demo.fastq.gz . 14.2 HUMAnN: run Run the HUMAnN pipeline with our demo data: humann \\ --input demo.fastq.gz \\ --output demo.humann \\ --threads 10 Here, we are telling the software to use demo.fastq.gz as input and to create a new output directory called demo.humann where the results will be generated. As the software runs you might notice that HUMAnN runs MetaPhlAn. The purpose of this is to identify what species are present in the sample, so HUMAnN can tailor generate an appropriate database of genes (from those species) to map against. It will carry out this alignment against the gene database, then a protein database, and finally compute which gene families are present. HUMAnN will determine which functional pathways are present and how abundant they are. If you are using paired end reads the HUMAnN developers recommend concatenating your reads into one file and running HUMAnN on the concatenated file source. For example (don't run the below): cat K1_R1.fq.gz K1_R2.fq.gz &gt; K1.fq.gz humann --input K1.fq.gz --output humann_output --threads 14.3 HUMAnN: output Once the run has completed, change into the newly created output directory and list the files that are present. cd demo.humann ls You will see that there are three files and one directory. The directory (demo_humann_temp) contains intermediate temporary files and can be disregarded here. The three output files are: demo_genefamilies.tsv: A table file showing the number of reads mapping to each UniRef90 gene family. Values are normalised by the length of each gene family (i.e. RPK, or Reads per Kilobase). Additionally, the values are stratified so that they show the overall community abundance but also a breakdown of abundance per species detected. This allows researchers to delve into species specific functions, rather than only looking at the metagenomic functions as a whole. demo_pathabundance.tsv: A table file showing the normalised abundance of MetaCyc pathways (RPKs). These abundances are calculated based on the UniRef90 gene family mapping data and are also stratified by species. demo_pathcoverage.tsv: A table file that shows the coverage, or completeness, of pathways. For example, a pathway may contain 5 components (or genes/proteins) Pathway1 : A → B → C → D → E 100% complete A species identified in the sample may only have four of the components, meaning that the pathway is only 80% complete (represented as 0.8) Pathway1 : A → B → C → D → E 80% complete The basic format of these three output files is the same, so let's take a look at the pathway abundance table. less demo_pathabundance.tsv You will see that there are two columns: The first column shows the pathways. UNMAPPED indicates reads that could not be aligned. UNINTEGRATED indicates reads that aligned to targets not implicated in any pathways. The second column shows the abundance. This file is not too interesting to look at as it is only demo data. Therefore, press q to exit less and let's look at some real data. Note: The directory demo_humann_temp can be very large and so should be deleted in real projects once you are certain they are not needed. However, these files can be useful for debugging. Note: Link to more detail on Output files "],["15-Multi_sample_processing.html", "Chapter 15 Multi sample processing 15.1 Combining data 15.2 Split stratified table 15.3 Renormalising data", " Chapter 15 Multi sample processing Looking at the functional profile of one sample in isolation is usually not very informative. First, there is nothing to compare it to and second, there are no biological replicates. We will therefore use all the Korean and Western diet samples. It would take many hours to analyse all of the data using HUMAnN and is outside the scope of this course. For this reason, samples were analysed prior to the workshop to generate the output files we covered above. For the purposes of this comparison, we will look at the pathway abundances only. First copy over the data directory containg the gene families tables and have a look in it. #Ensure you are in the correct directory cd ~/4-FunctionalProfiling #Copy directory with pre made results cp -r /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/DietPathAbundance . #Move into the copied directory cd DietPathAbundance #List files ls You will see there are 12 files prefixed with K and 12 prefixed with W, for the Korean diet and Western diet samples, respectively. Take a look at the file for K1. less K1_pathabundance.tsv There are a lot of pathways in the file. Quit out of the less viewer (q) and look at the entries for one specific pathway, COA-PWY-1 (a coenzyme A biosynthesis II pathway). grep COA-PWY-1 K1_pathabundance.tsv | less This shows 30 entries/lines with the top entry/line: COA-PWY-1: superpathway of coenzyme A biosynthesis III (mammals) 6790.1517478104 This shows the abundance of the pathway across the entire sample (6790.1517478104). The other entries show the species stratification information (mentioned above) of the pathway. I.e. the second line: COA-PWY-1: superpathway of coenzyme A biosynthesis III (mammals)|g__Bacteroides.s__Bacteroides_dorei 1292.7711872228 shows the abundance of the pathway that is contributed by the species Bacteroides dorei (1292.7711872228). Note: The species stratified pathway abundances may not equal the total community pathway abundance. Please see this forum post for details. With this information we will carry out some comparisons including biomarker detection to determine which pathways are differentially abundant between the Western diet and Korean diet samples. Note: The following methods/pipeline can be used for the genefamilies and pathcoverage tables in your own future analyses. 15.1 Combining data First, we need to combine these 24 tables into one large results table. HUMAnN provides a tool to do this: #Change directory to main Functional profiling directory cd ~/4-FunctionalProfiling #Join the tables humann_join_tables --input DietPathAbundance/ --output diet.tsv This command will look for all tables in the DietPathAbundance directory and generate a large, 25 column table called diet.tsv. You can inspect the file to ensure that this has worked correctly. less -S diet.tsv 15.2 Split stratified table For this tutorial we do not want the species stratification information. We will therefore split the table to create 2 new files: diet_unstratified.tsv: This table only contains the total abundance values for the pathways. It does not contain any species stratification information. diet_stratified.tsv: This table only contains the species stratification abundance values for the pathways. It does not contain the total abundance information. To create the split files and output them to your current directory, run the following command: humann_split_stratified_table --input diet.tsv --output . We will use the file diet_unstratified.tsv for our downstream analysis. Before you move on feel free to inspect the output files with the less command. Note: You can use any of the three tables (unsplit table, unstratified table, or stratified table) in your own analysis. This depends on your question and data. 15.3 Renormalising data The next step is to renormalise the data. Currently, all of the abundance values are only normalised within each sample (RPKs). However, they are not normalised between samples, and this is very important to do. For example, if we had sequenced two samples, A and B, and we obtained 5 million reads for sample A and 20 million reads for sample B, without normalisation, it might look there was up to 4x as much functional activity in sample B! To correct for this, we normalise the abundance values based on the number of reads in each sample. We will normalise to relative abundance (--units relab) where all abundances for each sample add up to 1. Renormalisation command: humann_renorm_table \\ --units relab \\ --input diet_unstratified.tsv \\ --special n \\ --output diet_unstratified.relab.tsv This command generates the normalised data in the new table diet_unstratified.relab.tsv. The --special n option tells the script to remove all unmapped and unassigned values (UNMAPPED &amp; UNINTEGRATED) from the table. Note: With the gene families information ensure you normalise by CPM (counts per million) with the option --units cpm. More info can be found on the Normalizing RPKs to relative abundance section of the HUMAnN 3.0 tutorial. "],["16-Heatmap.html", "Chapter 16 Heatmap", " Chapter 16 Heatmap Now that we have our combined, unstratified, and normalised table, we can visualise the dataset to see how the two groups compare. Do samples in the same diet group appear to correlate well with each other? Are samples from one diet group distinguishable from those from the other diet group? To visualise this we will create a heatmap with hclust2. Before carrying out the command we will need to edit the file. Carry out the following alterations: Remove the _Abundance part of the sample names whilst creating a copy that we will use (It is always a good idea to keep the original file in case a mistake happens). cat diet_unstratified.relab.tsv | sed &quot;s/_Abundance//g&quot; &gt; diet_unstratified.relab.comp.tsv Intro to unix links: Text editing with sed Redirection with &gt; Next using your text editor of choice carry out the following changes on the file diet_unstratified.relab.comp.tsv. Remove the # (including the one space after the #) from the start of the header so it starts as Pathway. Add in the same metadata line as we did for 12.1 but this time below the header line, i.e. as the 2nd line (ensure you are using tabs instead of spaces). Issues with creating file? If you are having issues with creating and editing the file all_num.lefse.bracken you can copy a pre-made version. cp /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/lefse/diet_unstratified.relab.comp.tsv . Now we can use the hclust2 tool to create a heatmap of our pathway abundances. hclust2.py \\ -i diet_unstratified.relab.comp.tsv \\ -o diet_unstratified.relab.heatmap.png \\ --ftop 40 \\ --metadata_rows 1 \\ --dpi 300 Note: You will get 2 MatplotlibDeprecationWarnings, these are normal and can be ignored. However, ensure these are the only warnings/errors before continuing. Parameters -i: The input table file. -o: The output image file. The tool does not specify what types of image files you can use but .png is always a good image file format. --ftop: Specifies how many of the top features (pathways in this case) to be included in the heatmap. --metadata_rows: Specifies which row/s contain the metadata information to be used for the group colouring at the top of the heatmap. Row numbers start at 0 for this tool. Therefore our sample names are in row 0 and the diet info is in row 1. Multiple rows can be specified if you have multiple rows of metadata. e.g. --metadata_rows 1,2,3. --dpi: The image resolution in dpi (dots per inch). 300 dpi is used for publication quality images. There are many more options that can be seen on the hclust2 github. Visualise Now we can view the plot. firefox diet_unstratified.relab.heatmap.png From this, we can see that there is a small amount of clustering caused by the differences between the Korean and Western diet. Other factors that we do not know about the samples must also come into play. This is normal as we cannot account for everything but it is good to try to account for as much as possible. MCQs Which pathway stands out the most? ARO-PWY DTDPRHAMSYN PWY-6385 How many clusters are formed based on diet (Colours on tree at top of heatmap)? 1 2 3 How many clusters are formed based on pathways (Colours on tree at the side of heatmap)? 1 2 3 You can look up the pathway names in the table file to see a fuller description. "],["17-LEfSe_humann.html", "Chapter 17 LEfSe", " Chapter 17 LEfSe For the final part of this section, we will see if there are any statistically significant differences between the two sample groups. There are several ways in which this can be achieved but we will carry out LEfSe again. Task: Go back to your shotgun_meta terminal (or create a new one and use . useshotgun). Then change directory to 4-FunctionalProfiling) Thankfully we already formatted the file to work with LEfSe when we formatted it for hclust2 #LEfSe format lefse_format_input.py \\ diet_unstratified.relab.comp.tsv \\ diet_unstratified.relab.comp.lefse \\ -c 2 -u 1 -o 1000000 #Run LEfSe lefse_run.py \\ diet_unstratified.relab.comp.lefse \\ diet_unstratified.relab.comp.lefse.out #Produce LEfSe plot lefse_plot_res.py \\ --dpi 200 \\ --format png \\ #Below options added to increase max length of feature string --max_feature_len 200 \\ diet_unstratified.relab.comp.lefse.out \\ diet_unstratified.relab.comp.lefse.png #View plot firefox diet_unstratified.relab.comp.lefse.png Look at the output and see what pathways count as biomarkers for the 2 groups. "],["18-HUMAnN_task.html", "Chapter 18 Optional task", " Chapter 18 Optional task Carry out all the steps starting from Multi sample processing with the gene families information. Copy the gene families data from /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/DietGeneFamilies Ensure you set the --units option to cpm in the renormalising data step. On top of analysing the unstratified data you can also analyse the stratified data. That completes the non assembly approach to shotgun metagenomic analysis. The next chapters will cover an assembly approach. "],["19-Metagenome_assembly.html", "Chapter 19 Metagenome assembly 19.1 Metagenome assembly: Mamba 19.2 A primer on short read assembly", " Chapter 19 Metagenome assembly So far we have directly analysed the read data itself which is perfectly fine for taxonomic profiling and for certain methods of functional profiling. However, Illumina reads are generally short and therefore can not provide us with much data on larger constructs that are in the metagenomic samples, e.g. genes. While it is possible to predict from which gene a sequence read might originate, the short nature of the query can sometimes lead to ambiguous results. Additionally, depending on the application it can become computationally intensive to analyse large numbers of reads. Here, we are only using samples with 1 million reads. Some metagenome samples consist of 50-100 million+ read pairs. If such a sample belonged to a set of 100 samples, that would be up to 10 billion read pairs, or 2 trillion bases of sequence data, with many of these being redundant. For this reason, it is sometimes advantageous to assemble the reads into contigs, using a meta-genome assembler. This has the dual effect of: Reducing the overall size of the data for analysis. If a metagenome was sequenced at 50x depth, then by assembling it you could theoretically reduce the amount of sequence to analyse by 50-fold. Increase the size of the fragments you will analyse. This is the main advantage of an assembly, as the ~100 bp reads can be pieced together to form 100,000 kb+ contigs. These contigs will contain complete genes, operons and regulatory elements: Reconstructed genome sections. Here, we will carry out a couple of assemblies on our dataset. 19.1 Metagenome assembly: Mamba We will use the shotgun_meta conda environment so use a terminal where this is activated or open a new one and run . useshotgun. 19.2 A primer on short read assembly Illumina reads are too short and numerous to use traditional overlap-layout-consensus assemblers as such an approach would be far too computationally intensive. Instead, we use De Bruijn graph based assemblers. Briefly, these operate as follows: All reads are broken down into k-length overlapping fragments (k-mers). e.g. if we choose a k-mer size of 5 bp, the following two sequences (blue) would be broken down into the k-mers below them (red): All k-mers are linked to other k-mers which match with a k-1 length overlap (i.e. that overlap by all but one base): Paths are routed through the graph and longer contigs are generated: The example here is a vast oversimplification of the complexity of a De Bruijn graph (i.e. there are no branches!). Routing through the graph is never as simple as this as some k-mers will lead to multiple k-mers, which can result in the break point of a contig. This is especially true for complex metagenomic data. Generally speaking, the shorter the k-mer, the more branches there will be, the trickier the graph is to resolve, so the resulting contigs are smaller. Assemblers usually perform better with longer k-mer lengths but even then there might not be enough depth of sequencing to generate all k-mers that form overlaps, therefore leading to break points. Finding the right k-mer size usually involves testing several. Fortunately, the assembler we will use, MEGAHIT, allows us to build an assembly using multiple k-mer lengths iteratively. The other great advantage about MEGAHIT is that it is quick and efficient. We will use MEGAHIT on our data soon, but first there is an additional processing step for our sequences... "],["20-Stitching_reads.html", "Chapter 20 Stitching read pairs 20.1 FLASH: run 20.2 FLASH: Output 20.3 FLASH: MCQs", " Chapter 20 Stitching read pairs Longer k-mers generally perform better for assemblies. However, our maximum read length is 100 bp so we are limited to a maximum k-mer length of 99 bp. Thankfully we can get even longer k-mers if we stitch our read pairs together. Note: This method will not work if your reads have no overlap. If you are not sure if your reads have overlap ask the team who sequenced them. A read pair consists of two sequences read from each end of a fragment of DNA (or RNA). If the two sequences meet and overlap in the middle of the fragment, there will be a region of homology. We can use this to merge the two reads together (See next image). First, we obtain our forward and reverse reads, derived from different ends of the same fragment. Second, we look for sufficient overlap between the 3' ends of our sequences. Third, if there is sufficient overlap, we combine, or stitch, the two reads together to form one long sequence. Once we have longer stitched reads, we can increase the k-mer length for our assembly. There are a number of pieces of software that can be used to stitch reads (e.g. Pear, Pandaseq) but today we will use one called FLASH: 20.1 FLASH: run Make a new output directory for the stitched reads and run FLASH: #Change directory to home cd ~ #Make and move into new directory mkdir 5-Stitched cd 5-Stitched #Run flash flash -o K1 -z -t 12 -d . \\ ../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz Parameters -o: Sets the prefix of the output files. -z: The input is zipped. -t: Number of threads to use. -d: The directory the output files will be placed. The last 2 flag-less parameters are the forward and reverse read files for stitching. 20.2 FLASH: Output Once FLASH has finished running, it will display on screen how well the stitching process went, in this case a low amount of reads were combined. Have a look what files have been generated. ls We have three new fastq.gz files. One containing the stitched reads (K1.extendedFrags.fastq.gz) and two containing the reads from pairs that could not be combined (K1.notCombined_1.fastq.gz and K1.notCombined_2.fastq.gz). We can also see what the new read lengths are: less K1.histogram Scroll down with the down key and you will see that we are looking at a histogram showing the proportion of stitched reads at different lengths. 20.3 FLASH: MCQs What length has the highest proportion of stitched reads? 101 177 188 What length has the lowest proportion of stitched reads? 101 177 188 "],["21_Megahit.html", "Chapter 21 Megahit Assembly 21.1 MEGAHIT 21.2 QUAST 21.3 Metagenome assembly summary", " Chapter 21 Megahit Assembly We will use our stitched and unstitched reads to produce an assembly withMEGAHIT. 21.1 MEGAHIT Create a new directory to store our assembly in. cd .. mkdir 6-Assembly cd 6-Assembly Now run the metagenome assembler MEGAHIT using our newly stitched read data. megahit \\ -r ../5-Stitched/K1.extendedFrags.fastq.gz \\ -1 ../5-Stitched/K1.notCombined_1.fastq.gz \\ -2 ../5-Stitched/K1.notCombined_2.fastq.gz \\ -o K1 \\ -t 12 \\ --k-list 29,49,69,89,109,129,149,169,189 Parameters -r: Single-end reads to be used for assembly. We are using our successfully stitched reads. -1: Forward reads of paired end reads to be used for assembly. We are using the reads that did not stitch as they still have useful information. -2: Reverse reads of paired end reads to be used for assembly. We are using the reads that did not stitch as they still have useful information. -o: Output directory. -t: Number of threads to be used for process. --k-list: K-mer list. The k-mer list instructs MEGAHIT to first generate an assembly using a k-mer size of 29 bp and when that is complete, integrate the results into an assembly using a k-mer size of 49 bp, and so on up to a final iteration using a k-mer size of 189 bp. This large range of k-mer lengths should give us a good assembly, given the data. However, it may take a while to run. This might be a good time to read on or take a break whilst the command runs. If you need a command prompt (your current one is busy because MEGAHIT is running), right click on the main screen, choose Terminal. Once the assembly is completed, we can look at the output FASTA file containing the contigs: less K1/final.contigs.fa There is not much to see. When happy, quit the less (q) and carry on to QUAST. 21.2 QUAST We can generate some metrics based on the assembly with QUAST, but first we will create a directory for the QUAST output. #Create QUAST output directory #The option -p will create a directory and any required #parent directories mkdir -p quast/K1 21.2.1 QUAST: run Now to run QUAST. quast -o quast/K1 K1/final.contigs.fa 21.2.2 QUAST: visualise QUAST will run relatively quickly. Once complete view the output with firefox. firefox quast/K1/report.html The report tells us quite a bit about the assembly quality. Two definitions that you may not be aware of are N50 and L50. To calculate these values: Order the contigs from largest to smallest. Total up the sizes from biggest downwards. The contig we reach where our total is at least 50% of the size of the whole assembly is the N50 contig. N50 equals the length of the N50 contig. L50 is the number of contigs with a length equal to or greater than N50. 21.2.3 QUAST: MCQs Note: Due to the assembly process your values may be slightly different (&lt; 1%). Please choose the closest value What is the total length of the assembly? 39.88 18,650 16,187,259 How many contigs does the assembly consist of? 39.88 18,650 16,187,259 What is the GC% of the assembly? 39.88 18,650 16,187,259 What is the N50 of the assembly? 1,363 2,178 87,586 What is the L50 of the assembly? 1,363 2,178 87,586 what is the length of the largest contig? 1,363 2,178 87,586 Questions - How do the contig metrics compare to the original reads? 21.3 Metagenome assembly summary We now have an assembly. It is not a brilliant one due to us only having used 1 million reads. In real analysis we would prefer fewer but longer contigs. We will explore some tools we can use with our metagenome assembly in the next chapters. There is also a metaQUAST specifically for metagenome assemblies but it requires reference assemblies be provided. "],["22-Genome_binning.html", "Chapter 22 Genome binning 22.1 MetaBAT2 22.2 CheckM 22.3 Binning summary", " Chapter 22 Genome binning A metagenome assembly consists of contigs from many different genomes. At this stage we don't know which contigs are from which species. We could try to taxonomically classify each contig but there are 2 problems with this approach: Some contigs may be misclassified which can lead to multiple contigs from the same genome/organism being classified as various taxa. Databases are incomplete and so some contigs will not be classified at all (microbial dark matter). To alleviate these issues genomic binning can be carried out. This will cluster contigs into bins based on: Coverage: Contigs with similar coverage are more likely to be from the same genome. Composition: Contigs with similar GC content are more likely to belong to the same genome. Genomic binning has been used to discover many new genomes. Additionally, it makes downstream analyses quicker as the downstream steps will be carried out on the sets of bins rather than on one large metagenome assembly. Binning produces \"bins\" of contigs of various quality (e.g. draft, complete). These bins are also known as MAGs (Metagenome-assembled genomes). In other words a MAG is a single assembled genome that was assembled with other genomes in a metagenome assembly but later separated from the other assemblies. The term MAG has been adopted by the GSC (Genomics Standards Consortium). It is recommended to ensure you do not have a poor quality metagenome assembly. Binning requires contigs of good length and good coverage. Extremely low coverage and very short contigs will be excluded from binning. 22.1 MetaBAT2 We will use MetaBAT2 for our genome binning. It has three major upsides that makes it very popular: It has very reliable default parameters meaning virtually no parameter optimisation is required. It performs very well amongst genome binners. It is computationally efficient compared to other binners (requires less RAM, cores etc.) Make a new directory and move into it. #Make directory mkdir -p ~/7-Binning/K1 #Move into it cd ~/7-Binning/K1 22.1.1 MetaBAT2: depth calculation To carry out effective genome binning MetaBAT2 uses coverage information of the contigs. To calculate depth we need to align the reads to the metagenome assembly. Index assembly For the alignment we will use bwa. We need to index our assembly file prior to alignment. bwa index ~/6-Assembly/K1/final.contigs.fa Alignment Next we will align our trimmed paired reads we used to create the stitched reads. We will carry this out with the bwa mem command. bwa mem is a good aligner for short reads. If you are using long reads (PacBio or Nanopore) minimap2 will be more appropriate. bwa mem ~/6-Assembly/K1/final.contigs.fa \\ ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz &gt; \\ K1.sam Sam to sorted bam After alignment we need to get the file ready for the contig depth summarisation step. This requires converting the sam file to a bam (binary form of a sam file) file and then sorting the bam file. # Convert sam to bam file samtools view -bu K1.sam &gt; K1.bam # Created sorted bam file samtools sort K1.bam &gt; K1.sort.bam Summarise depths Now we can summarise the contig depths from the sorted bam files with MetaBAT2's jgi_summarize_bam_contig_depths command. jgi_summarize_bam_contig_depths --outputDepth K1.depth.txt K1.sort.bam View summary depth You can have a look at the depth file and you will notice there are many contigs with low coverage (&lt;10) and of short length (&lt;1500). less K1.depth.txt To get a better look we will open the file in R and look at a summary of the file's table. Activate R: R Now in R we will read in the file and get a summary() of it. #Read in the table as an object called df (short for data frame) #We want the first row to be the column names (header=TRUE) #We do not want R to check the column names and &quot;fix&quot; them (check.names=FALSE) df &lt;- read.table(&quot;K1.depth.txt&quot;, header=TRUE, check.names=FALSE) #Create a summary of the data summary(df) The last command gave us summary information of all the columns. This includes the minimum, maximum, mean, median, and Inter-Quartile Range (IQR) values. We can see the values of the contigLen and totalAvgDepth are very low. However, this is most likely due to a bunch of short and low coverage contigs which will be ignored by MetaBAT2. Therefore we will remove rows with information on contigs shorter than 1500 and rerun the summary. MetaBAT2's documentation dictates the minimum contig length should be &gt;=1500 with its default being 2500. #Set the new object &quot;df_min1500len&quot; as all rows #where the value in the column &quot;contigLen&quot; of &quot;df&quot; #Is greater than or equal to 1500 df_min1500len &lt;- df[df$contigLen &gt;= 1500,] #Summary of our new data frame summary(df_min1500len) That is looking better. The minimum average coverage for MetaBAT2 is 1 and our minimum value is 2.700 with a maximum of 93.285 (your values may differ slightly). Now you can quit R and continue. #quit R q() #On the prompt to save your workspace press &quot;n&quot; and then enter. Note: One of the reasons for our short contigs is that we only used a subset of our sequencing dataset for this tutorial due to time concerns. 22.1.2 MetaBAT2: run With our assembly and its depth information we can run MetaBAT2 for binning. #make a diretcory for the bins mkdir bins #Run MetaBAT2 metabat2 \\ --inFile ~/6-Assembly/K1/final.contigs.fa \\ --outFile bins/K1 \\ --abdFile K1.depth.txt \\ --minContig 1500 Parameters --inFile: Input metagenome assembly fasta file. --outFile: Prefix of output files. --abdFile: Base depth file. --minContig: Minimum size of contigs to be used for binning. The default is 2500. We used the minimum value of 1500 as we are using tutorial data. We recommend using the default in your own analysis. 22.1.3 MetaBAT2: output List the contents of the output directory and you'll see there is 1 fasta file with the prefix of K1. This is a bin that will hopefully contain 1 MAG (Metagenome-Assembled Genome). In your future analysis you may get many bins, each hopefully only having one MAG. ls bins 22.2 CheckM CheckM is a useful tool to assess the quality of assembled bacterial and archaeal genomes. This can be used on assemblies produced from single cell, single isolate, or metagenome data. Additionally, it can be used to identify bins that are likely candidates for merging. This occurs when one genome has been separated into different bins. An important part of CheckM is the ubiquitous and single-copy genes it utilises. It has sets of these genes for different phylogenetic lineages. With these it can determine: What lineage a bin/MAG belongs to. Does it contain genes only found in Escherichia? How complete the bin/MAG is. A set of lineage specific genes should all be found in a genome belonging to the lineage (ubiquitous). What percentage of these lineage specific genes are present in the MAG? &gt;95% is very good &gt;80% is good &gt;70% is ok &lt;70% is poor to poorer How contaminated the bin/MAG is. Only one copy of each gene should be present (single-copy). Are there any markers for other lineages present? 22.2.1 CheckM: Mamba Due to program version conflicts we will use the checkm conda environment for this section. Open a new terminal and activate the checkm environment. . usecheckm Ensure you are in the correct directory. cd ~/7-Binning/K1/ 22.2.2 CheckM: run CheckM has many different commands. We will use one of the common workflows it provides called lineage_wf. This carries out five of its commands in a workflow (i.e. the next step uses output from the previous step). tree - Places bins in the reference genome tree. This reference tree comes with CheckM. tree_qa - Assess the phylogenetic markers found in each bin. lineage_set - Infers lineage-specific marker sets for each bin. analyze - Identifies marker genes in bins. qa - Assesses the bins for contamination and completeness. Run the CheckM command (this will take a while): checkm lineage_wf \\ --tab_table -f MAGS_checkm.tsv \\ -x fa \\ bins/ checkm_output Parameters --tab_table : Prints results to a tab separated table. -f : File name to print result to (if not specified results will go to stdout). -x : Suffix/extension of bin files. Other files are ignored in the specified bin directory. fa is used as our MetaBAT2 analysis produced fasta files that end in .fa. bins : The second last argument is the bin_dir, the directory containing all the bins to be analysed in fasta format. checkm_output : The last argument is the directory to store the output to. This directory should not exist prior to running. 22.2.3 CheckM: output As we have only used a subset of data the results are not very good. We'll therefore look at premade results. These premade results were produced using the entire K1 dataset. First you will need to copy them. cd ~/7-Binning mkdir K1_fullset cd K1_fullset ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/binning/K1_fullset/* . Now we can look at the results table that is in your current directory. less MAGS_checkm.tsv CheckM statistics definitions List of statistics definitions from CheckM wiki Note: These may not all appear in your MAGS_checkm.tsv file. bin id: unique identifier of genome bin (derived from input fasta file) marker lineage: indicates the taxonomic rank of the lineage-specific marker set used to estimated genome completeness, contamination, and strain heterogeneity. More detailed information about the placement of a genome within the reference genome tree can be obtained with the tree_qa command. The UID indicates the branch within the reference tree used to infer the marker set applied to estimate the bins quality. # genomes: number of reference genomes used to infer the lineage-specific marker set markers: number of marker genes within the inferred lineage-specific marker set marker sets: number of co-located marker sets within the inferred lineage-specific marker set 0-5+: number of times each marker gene is identified completeness: estimated completeness of genome as determined from the presence/absence of marker genes and the expected collocalization of these genes (see Methods in the PeerJ preprint for details) contamination: estimated contamination of genome as determined by the presence of multi-copy marker genes and the expected collocalization of these genes (see Methods in the PeerJ preprint for details) strain heterogeneity: estimated strain heterogeneity as determined from the number of multi-copy marker pairs which exceed a specified amino acid identity threshold (default = 90%). High strain heterogeneity suggests the majority of reported contamination is from one or more closely related organisms (i.e. potentially the same species), while low strain heterogeneity suggests the majority of contamination is from more phylogenetically diverse sources (see Methods in the CheckM manuscript for more details). genome size: number of nucleotides (including unknowns specified by N's) in the genome # ambiguous bases: number of ambiguous (N's) bases in the genome # scaffolds: number of scaffolds within the genome # contigs: number of contigs within the genome as determined by splitting scaffolds at any position consisting of more than 10 consecutive ambiguous bases N50 (scaffolds): N50 statistics as calculated over all scaffolds N50 (contigs): N50 statistics as calculated over all contigs longest scaffold: the longest scaffold within the genome longest contig: the longest contig within the genome GC: number of G/C nucleotides relative to all A,C,G, and T nucleotides in the genome coding density: the number of nucleotides within a coding sequence (CDS) relative to all nucleotides in the genome translation table: indicates which genetic code was used to translate nucleotides into amino acids # predicted genes: number of predicted coding sequences (CDS) within the genome as determined using Prodigal 22.2.4 CheckM: Quality score One quick way to calculate the overall quality of a bin is with the following equation: \\[ q = comp - (5 * cont) \\] Where: q = Overall quality comp = Completeness cont = Contamination A score of at least 70-80% would be the aim, with a maximum/perfect value being 100% (100% completeness, 0% contamination). Therefore let us calculate this for the bins with some bash and awk scripting. Tab to comma seperated First it is good to make a copy of the file in case we make a mistake and want to start over. Additionally, we will make the copy a comma separated file. I find these easier to edit as typing comma characters (,) in commands is more reliable than tab characters (\\t). This is carried out with tr which can translate characters. In the below case we translate/convert the tabs (\\t) into commas (,). The converted output is then redirected (&gt;) to the file \"MAGS_checkm.csv\". cat MAGS_checkm.tsv | tr &quot;\\t&quot; &quot;,&quot; &gt; MAGS_checkm.csv Quality file We will create a new file with only the quality information. We'll start by making a file with only a header. echo &quot;Quality&quot; &gt; MAGS_quality.csv Calculate quality with awk Next is the most complicated command. We will be calculating the Overall quality (see calculation above) for each row except the header row. We will be using a complicated linux based language called awk. This is very useful as it can carry out calculations on columns or as awk calls them, fields. As this is new and complicated we will build up our command step by step. The first step is to extract the completeness and contamination fields/columns. awk -F, &#39;{print $12,$13}&#39; MAGS_checkm.csv -F,: Indicates the input fields are separated by commas (,). '': All the awk options are contained within the quotes. {}: We can supply a function to awk within the braces. print $12,$13: This function instructs awk to print the 12th (completeness) and 13th (contamination) fields. It is common to put commas (,) between fields if printing multiple fields. MAGS_checkm.csv: Our last parameter is the input file. We are not changing the contents of the file, only printing information to screen/stdout. We do not want the header in our calculation so we will add an extra awk option. awk -F, &#39;NR&gt;1 {print $12,$13}&#39; MAGS_checkm.csv NR&gt;1: NR stands for number of records. Rows are called records in awk. Therefore NR&gt;1 means awk will only carry out the functions on the records numbered greater than 1. I.e. skip row 1, the header row. The final step is to carry out the overall quality calculation and append the information to the \"MAGS_quality.csv\" file. awk -F, &#39;NR&gt;1 {print $12 - (5 * $13)}&#39; MAGS_checkm.csv &gt;&gt; MAGS_quality.csv {print $12 - (5 * $13)}: Our new function carries out the overall quality calculation and prints it for each record/row except the first (NR&gt;1). &gt;&gt; MAGS_quality.csv: The printed information is appended (&gt;&gt;) to the file \"MAGS_quality.csv\". We append because we want to retain the header we added to the file earlier. You can view the file to ensure it worked. The first and second values should be 33.62 and -0.89 less MAGS_quality.csv Add quality to the checkm results file Now we can combine the files \"MAGS_checkm.csv\" and \"MAGS_quality.csv\" with the paste command. The -d \",\" option indicates the merged files will be separated by commas (,), matching the column separation in \"MAGS_checkm.csv\". paste -d &quot;,&quot; MAGS_checkm.csv MAGS_quality.csv &gt; MAGS_checkm_quality.csv 22.2.5 CheckM: MCQs Viewing the file \"MAGS_checkm_quality.csv\" attempt the below questions. Tip: You can use the cut command to look at specific columns. For example: #look at the &quot;Bin Id&quot; and &quot;Quality&quot; #Convert the printed output&#39;s commas to tabs for readability cut -d &quot;,&quot; -f 1,15 MAGS_checkm_quality.csv | tr &quot;,&quot; &quot;\\t&quot; What lineage was assigned to bin K1.1? root (UID1) k_Bacteria (UID203) o_Lachnospiraceae (UID1286) What lineage was assigned to bin K1.22? root (UID1) k_Bacteria (UID203) o_Lachnospiraceae (UID1286) What lineage was assigned to bin K1.8? root (UID1) k_Bacteria (UID203) o_Lachnospiraceae (UID1286) What is the quality value of K1.1? 4.17 33.62 172 What is the completeness value of K1.27? 4.17 33.62 172 How many genomes are within the K1.12 bin? 4.17 33.62 172 Which bin has the highest quality value (96.38%)? K1.20 K1.21 K1.22 Which bin has the lowest quality value (-300.67%)? K1.20 K1.21 K1.22 Which bin has the highest completeness value (98.27%)? K1.20 K1.21 K1.22 22.3 Binning summary It is always useful to know the quality of your bins so you know which are more reliable than others. With that information you can be more or less certain when concluding your findings. We have some good quality bins. However, the best bins would only have one genome. Ultimately binning is trying to separate all the genomes from each other. A better metagenome assembly would most likely have led to better binning. "],["23-Assembly_functional_annotation.html", "Chapter 23 Assembly functional annotation 23.1 Taxonomic annotation 23.2 Bakta 23.3 MinPath", " Chapter 23 Assembly functional annotation 23.1 Taxonomic annotation Taxonomic annotation of bins can be carried out with Kraken2. As we have already done this for the reads and taxonomic results from read and assembly approaches have similar performances we will not cover it here. Instead we will move straight onto functional annotation with Bakta. Conda Environment: Go back to your shotgun_meta terminal (or create a new one and use . useshotgun). 23.2 Bakta We will carry out Bakta functional annotation. Bakta can annotate bacterial genomes and plasmids from both isolates and MAGs. Make a new directory and move into it. mkdir ~/8-Annotation cd ~/8-Annotation 23.2.1 Bakta: run Now we can annotate one of the bins. The below will take a long time to run (&gt;1 hour). Instead of running it skip onto the next section to copy pre-made output to continue with. This command is here so you know what to run in your own future analyses. bakta \\ --db /pub14/tea/nsc206/databases/bakta/db/ \\ -o K1.1 \\ ~/7-Binning/K1_fullset/bins/K1.1.fa Parameters --db: Location of Bakta database. You will need to install this in your own installation. Instructions are in the appendix. -o: The output directory. This must not exist before running the command. The last parameter is the fasta file containing the genome/plasmid you would like annotated. 23.2.2 Bakta: premade results Link the pre-made results for all the K1 bins. #Link all data ln -s /pub14/tea/nsc206/NEOF/Shotgun_metagenomics/bakta/K1_fullset/* . 23.2.3 Bakta: output List the files in the newly created K1.1 directory. Each of the files has the prefix \"K1.1\" and contains the following information: .tsv: annotations as simple human readble TSV .gff3: annotations &amp; sequences in GFF3 format .gbff: annotations &amp; sequences in (multi) GenBank format .embl: annotations &amp; sequences in (multi) EMBL format .fna: replicon/contig DNA sequences as FASTA .ffn: feature nucleotide sequences as FASTA .faa: CDS/sORF amino acid sequences as FASTA .hypotheticals.tsv: further information on hypothetical protein CDS as simple human readble tab separated values .hypotheticals.faa: hypothetical protein CDS amino acid sequences as FASTA .json: all (internal) annotation &amp; sequence information as JSON .txt: summary as TXT .png: circular genome annotation plot as PNG These are only useful for complete/near complete circular genomes. I would suggest looking at GenoVi for circular genome plots. .svg: circular genome annotation plot as SVG .log: Log file of command. View the summary file for bin K1.1. less -S K1.1/K1.1.txt Summary fields Sequence information Length: Number of bases. Count: Number of contigs/scaffolds. GC: GC%. N50: N50. N ratio: Ratio of N bases to non-N bases. coding density: Percentage of bases within coding regions. Annotation information. tRNAs: Transfer RNAs. tmRNAs: Transfer-messenger RNA. rRNAs: Ribosomal RNAs. ncRNAs: Non-coding RNAs. ncRNA regions: Non-coding RNA regions. CRISPR arrays: CRISPR arrays. CDSs: Coding sequences. pseudogenes: Segments of DNA that structurally resembles a gene but is not capable of coding for a protein hypotheticals: Hypothetical genes, which are predicted solely by computer algorithms, are experimentally uncharacterized genes. signal peptides: Short peptides (usually 16-30 amino acids long) normally present at the N-terminus of most newly synthesized proteins that are destined toward the secretory pathway. sORFs: Short open reading frames (&lt;100 amino acids). gaps: Gaps in the genome assembly. oriCs: Chromosome replication origin for bacteria. oriVs: Plasmid replication origin. oriTs: An origin of transfer (oriT) is a short sequence ranging from 40-500 base pairs in length. It is necessary for the transfer of DNA from a gram-negative bacterial donor to recipient during bacterial conjugation. View the gff file for bin K1.1. less K1.1/K1.1.gff3 The GFF file is a tab delimited file containing annotation information for the features in the assembly/bin. In this case it is a GFF3 file (most curent version of GFF). There is quite a lot of information contained in each row so instead of listing all the columns here please have a look at the official documentation: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md Loop used to analyse all bins The code below is for your future analysis, do not run it now as it will take too long. ls -1 ~/7-Biinning/K1_fullset/bins/*fa | while read f ; \\ do s=$(basename $f | sed &quot;s/.fa//&quot;) ; echo $s ; \\ bakta --db /pub14/tea/nsc206/databases/bakta/db/ \\ -o ${s} \\ ~/7-Binning/K1_fullset/bins/K1.1.fa A quick thing we can do with these files is to see if any of the bins contain a specific annotation. For example, if we wanted to know if there were any ATP-binding proteins in any of the bins we could carry out the below command. grep &quot;ATP-binding protein&quot; */*gff3 | less -S We can now view the lines containing \"ATP-binding protein\" with the start of the line containing the file name the line belongs to. In your future analyses you can expect these files further with excel, R, or visualisation software like IGV (https://software.broadinstitute.org/software/igv/GFF). What if you want to know about pathways? 23.3 MinPath MinPath can predict MetaCyc metabolic pathways. These pathway are made up of sets of enzymes that catalyse various reactions. Ensure you start in the ~/8-Annotation directory. 23.3.1 MinPath: EC extraction Before we can estimate the pathways we need to extract the EC numbers predictions from the GFF file. EC (Enzyme Commission) numbers are a numerical classification for enzymes based on the reaction they catalyse. Unless you know the EC scheme well they are generally not helpful by themselves. An example EC number is EC 3.1.1. The numbers represents the group the enzyme belongs to with the first number being the biggest group. From highest to the lowest grouping 3.1.1 represents: 3: Hydrolase enzymes. 3.1: Hydrolase enzymes that act on ester bonds. 3.1.1: Carboxylic Ester Hydrolases. With all that information we will extract the EC annotations from the GFF files. First we'll create a directory for the output. #Create EC directory mkdir EC We will now use a loop with various parts to create EC annotation files. The input .ec files required for MinPath are tab delimited with two columns: Protein/sequence id. E.g. GDGAPA_12670. EC annotation. E.g. 6.1.1.4. Note: The lack of \\ at the end of most of the lines is intentional. The below is all one command over multiple lines but loops work slightly different and don't need \\ in certain parts. Ensure you do press enter at the end of each line. ls -1 */*gff3 | sed &quot;s|.*/||&quot; | sed &quot;s/.gff3//&quot; | while read bin do cat ${bin}/${bin}.gff3 | grep &quot;EC:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | \\ sed &quot;s/;.*EC:/\\t/&quot; | sed &quot;s/,.*//&quot; &gt; EC/${bin}.ec done 23.3.1.1 Code explanation That is quite a bit of code. You don't need to understand it as it should always work for Bakta output. If you are not currently interested you can skip to the MinPath: run section If you are interested in how it works we'll break it down with examples for you to run. The first part lists all our .gff files on one line each (ls -1 *gff). Next, we remove the directory name (sed \"s|.*||\"). Then the suffix .gff is substituted with nothing sed \"s/.gff//\". This gives us the name of each bin (e.g. K1.1, K1.2). #List all gff files on one (-1) each ls -1 */*gff3 #Remove the directory name #You can use any character as the divider in sed #Useful when you want to move slashes from a file name ls -1 */*gff3 | sed &quot;s|.*/||&quot; #List all the file prefixes (on one line each) ls -1 */*gff3 | sed &quot;s|.*/||&quot; | sed &quot;s/.gff3//&quot; With the above code we can loop through each file prefix and use the variable bin (arbitrarily chosen) which contains the file prefix. This is carried out with while read bin. All the lines between the do (start of the loop) and done (end of loop) are line run in the loop. Run the loop with an echo command to show we are using the ${bin} variable to specify the input and output files. ls -1 */*gff3 | sed &quot;s|.*/||&quot; | sed &quot;s/.gff3//&quot; | while read bin do echo &quot;${bin}/${bin}.gff3 ../EC/${bin}.ec&quot; done Now to look at the command within the loop: cat ${bin}.gff3 | grep &quot;EC:&quot; | cut -f 9 | \\ sed &quot;s/ID=//&quot; | sed &quot;s/;.*;Dbxref=/,/&quot; | \\ sed &quot;s/,.*EC:/\\t/&quot; | sed &quot;s/,.*//&quot; &gt;../EC/${bin}.ec A good way to figure out what a workflow is doing, is by building it up step by step. Run the first part of workflow and then add the next section, run, repeat. This shows you how each new section is affecting the output. Additionally, it is always good to run it on one file and head the output so we have a manageable amount of data to look at. We'll do that with the K1.1.gff3 file. Note: Remember we are adding head to the end for ease of viewing. Tips: Use the up arrow to go back to previously run commands that you can then edit. Remember the clear command. #Grab every line containing &quot;EC:&quot; cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | head #Cut out the 9th column/field (-f) (i.e. only keep the 9th column) #This is the attributes field in GFF3 #This contains a plethora of information including the EC annotation if present #cut uses tabs as the default column/field delimiter cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | cut -f 9 | head #The gff3 attributes field starts with the ID #We want to keep this but remove the &quot;ID=&quot; part cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | head #We don&#39;t want any of the info between the ID and the EC number #Therefore we want to remove everything (.*) between # the first &quot;;&quot; (at the end of the ID info) # and &quot;EC=&quot; #We&#39;ll replace this with a \\t to seprarate the ID and EC # columns with a tab (required by MinPath) cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | \\ sed &quot;s/;.*EC:/\\t/&quot; | head #Finally remove all the info after the EC number #This info will be after the last , cat K1.1/K1.1.gff3 | grep &quot;EC:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | \\ sed &quot;s/;.*EC:/\\t/&quot; | sed &quot;s/,.*//&quot; | head In the looped command, output is written into files: &gt; EC/${bin}.ec. 23.3.2 MinPath: run With our .ec files we can create our MetaCyc predictions. First we'll change directory into the EC directory. Then create an output directory for the MetaCyc predictions. cd ./EC mkdir ../MetaCyc Now we can loop through the file suffixes to run MinPath. ls -1 *ec | sed &quot;s/.ec//&quot; | while read bin do python /pub14/tea/nsc206/git_installs/Minpath/MinPath.py \\ -ec ${bin}.ec \\ -report ../MetaCyc/${bin}.minpath done A lot of output will be printed to screen but this can be ignored unless you see warnings. 23.3.3 MinPath: output First, change directory into the MetaCyc directory. #Change directory cd ../MetaCyc #List contents ls From the CheckM results we found that bin K1.22 was very good with a quality score &gt;96%. We will therefore have a look at its output. Have a look at the .minpath -report file for K1.22. less K1.22.minpath The file contains the following columns Pathway ID Pathway reconstruction: Only available for genomes annotated in MetaCyc database. Naive: Indicates if pathway was reconstructed by the naive mapping approach (1) or not (0). Minpath: Indicates if the pathway was kept (1) or removed (0) by MinPath. Fam0: The number of families involved in the pathway. Fam-found: Number of families in pathway that were annotated/found. Name: Description of pathway. Quit (q) less when you are happy. There are some quick things we can do in bash with these files. #Count number of pathways found in each bin with word count wc -l *minpath #Grab every line with &quot;PWY-6972&quot; from every file grep &quot;PWY-6972&quot; *minpath | less 23.3.4 KEGGs You can also get KEGG information with MinPath. The code: #Change to correct directory cd ~/8-Annotation #Make directory for KEGGs mkdir KEGG #KEGG extractions ls -1 */*gff3 | sed &quot;s|.*/||&quot; | sed &quot;s/.gff3//&quot; | while read bin do cat ${bin}/${bin}.gff3 | grep &quot;KEGG:&quot; | cut -f 9 | sed &quot;s/ID=//&quot; | \\ sed &quot;s/;.*KEGG:/\\t/&quot; | sed &quot;s/,.*//&quot; &gt; KEGG/${bin}.kegg done #Make directory for KEGG minpath output mkdir KEGG_minpath #Change directory to KEGG cd KEGG #Run MinPath ls -1 *kegg | sed &quot;s/.kegg//&quot; | while read bin do python /pub14/tea/nsc206/git_installs/Minpath/MinPath.py \\ -ko ${bin}.kegg \\ -report ../KEGG_minpath/${bin}.minpath done With these files you can then investigate what bins have which pathways. Additionally, with more samples analysed you can determine which samples have which pathways present. "],["24-Appendix.html", "A Mamba installs A.1 Mamba installation and environment B Next steps C Manuals D Obtaining Read Data", " A Mamba installs A.1 Mamba installation and environment Mamba is a reimplementation of conda. It is a great tool for installing bioinformatic packages including R packages. Mamba github: https://github.com/mamba-org/mamba The best way to use Mamba is to install Miniforge. It has both Conda and Mamba commands. Miniforge installation: https://github.com/conda-forge/miniforge Mamba guide: https://mamba.readthedocs.io/en/latest/user_guide/mamba.html To create the mamba environment shotgun_meta run the below commands in your bash. You will need to have installed mamba first. #shotgun_meta mamba create -n shotgun_meta mamba activate shotgun_meta #Install packages mamba install -c bioconda fastqc trim-galore multiqc bowtie2 kraken2 \\ krona bracken lefse flash megahit quast metabat2 prokka bbmap bakta circos #Update krona taxonomy database ktUpdateTaxonomy.sh #Install GenoVi via pip pip install genovi You will need to install the Bakta database as well. This requires you have a directory to store the database. In the below example you will need a directory with the path ~/databases/bakta_db, of course feel free to use a different location you have. bakta_db download --output ~/databases/bakta --type full Install MinPath via git. I suggest creating a directory called \"git_installs\" in your home directory and running this code there. #git_installs directory mkdir ~/git_installs cd ~/git_installs #MinPath git clone https://github.com/mgtools/MinPath To run MinPath in your own machines you will need to use the full path of the python file. If you installed these in your \"~/git_installs\" examples are below. #MinPath help page python ~/git_installs/Minpath/MinPath.py -h To create the mamba environment checkm run the below commands in your bash. You will need to have installed mamba first. #checkm mamba create -n checkm mamba activate checkm mamba install -c bioconda checkm-genome To create the mamba environment biobakery3.9 run the below commands in your bash. You will need to have installed mamba first. #biobakery3 mamba create -n biobakery3.9 mamba activate biobakery3.9 #Add required channels conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda config --add channels biobakery #Install huamnn3.9 mamba install -c biobakery humann=3.9 #Install hclust2 and matplotlib mamba install bioncda::hclust2=1.0.0 conda-forge::matplotlib=3.7.3 Next you will need to install the HUMAnN and MetaPhlAn databases. #Ensure you have biobakery3 mamba environment activated #Update HUMAnN databases humann_databases --download chocophlan full /path/to/databases --update-config yes humann_databases --download uniref uniref90_diamond /path/to/databases --update-config yes humann_databases --download utility_mapping full /path/to/databases --update-config yes #Install MetaPhlAn databases metaphlan --install --index mpa_vOct22_CHOCOPhlAnSGB_202403 Installing the MetaPhlAn databases may not work with the command above. If not follow the below instructions. Go to the following link: http://cmprod1.cibio.unitn.it/biobakery4/metaphlan_databases/ Download the following files (or the most recent vOct22 files): mpa_vOct22_CHOCOPhlAnSGB_202403.md5 mpa_vOct22_CHOCOPhlAnSGB_202403.tar bowtie2_indexes/mpa_vOct22_CHOCOPhlAnSGB_202403_bt2.md5 bowtie2_indexes/mpa_vOct22_CHOCOPhlAnSGB_202403_bt2.tar Put these in the MetaPhlAn databases directory. This will be in your mamba environment directory. E.g.: ~/mambaforge/envs/biobakery3.9/lib/python3.7/site-packages/metaphlan/metaphlan_databases Note: this may not be your exact path. Run metaphlan --install --index mpa_vOct22_CHOCOPhlAnSGB_202403 again. Untar the CHOCOPhlan tar file Change directory to your MetaPhlAn database directory. tar -xvf mpa_vOct22_CHOCOPhlAnSGB_202403.tar Once the databses are all setup you can test HUMAnN. humann_test B Next steps Below are some good links to start with before carrying out your own projects. KrakenTools: A suite of scripts to be used alongside the Kraken, KrakenUniq, Kraken 2, or Bracken programs. https://github.com/jenniferlu717/KrakenTools A review of computational tools for generating metagenome-assembled genomes from metagenomic sequencing data https://www.sciencedirect.com/science/article/pii/S2001037021004931 A review paper on metagenome assembly approaches Genome-resolved metagenomics using environmental and clinical samples: https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbab030/6184411 bioBakery tools for meta'omic profiling https://github.com/biobakery/biobakery Assessing the performance of different approaches for functional and taxonomic annotation of metagenomes https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6289-6 MicrobeAnnotator: Easy-to-use pipeline for the comprehensive metabolic annotation of microbial genomes. https://github.com/cruizperez/MicrobeAnnotator MetaEuk: Functional annotation of eukaryotic metagenome https://github.com/soedinglab/metaeuk _ Methods for Metagenomic data visualisation and analysis https://www.researchgate.net/publication/318252633_Methods_for_The_Metagenomic_Data_Visualization_and_Analysis Visualizing metagenomic and metatranscriptomic data: A comprehensive review https://www.sciencedirect.com/science/article/pii/S2001037024001430 C Manuals Conda: https://conda.io/projects/conda/en/latest/user-guide/getting-started.html FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ MultiQC: https://multiqc.info/ Trim Galore: https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/ Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml samtools: http://www.htslib.org/ BBTools: https://jgi.doe.gov/data-and-tools/bbtools/ Kraken2: https://github.com/DerrickWood/kraken2/wiki/Manual Krona: https://github.com/marbl/Krona/wiki/KronaTools Bracken: https://ccb.jhu.edu/software/bracken/index.shtml?t=manual LEfSe: https://huttenhower.sph.harvard.edu/lefse/ HUMAnN 3.0: https://huttenhower.sph.harvard.edu/humann/ MetaPhlAn 4.0: https://github.com/biobakery/MetaPhlAn/wiki/MetaPhlAn-4 Biobakery: https://github.com/biobakery/biobakery hclust2: https://github.com/SegataLab/hclust2 MegaHit: https://github.com/voutcn/megahit BWA: https://github.com/lh3/bwa minimap2: https://github.com/lh3/minimap2 MetaBAT2: https://bitbucket.org/berkeleylab/metabat/src/master/ CheckM: https://github.com/Ecogenomics/CheckM/wiki PhyloPhlAn: https://github.com/biobakery/phylophlan/wiki Prokka: https://github.com/tseemann/prokka MinPath: https://github.com/mgtools/MinPath/blob/master/readme MetaCyc: https://metacyc.org/ D Obtaining Read Data The following commands can be used to obtain the sequence data used in this practical, directly from the EBI metagenomics site. It is worth noting that these are the full set of data, not like the miniaturised version you have used in the tutorial. wget -O K1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_1.fastq.gz wget -O K1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_2.fastq.gz wget -O K2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_1.fastq.gz wget -O K2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_2.fastq.gz wget -O K3_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505095/ERR505095_1.fastq.gz wget -O K3_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505095/ERR505095_2.fastq.gz wget -O K4_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505096/ERR505096_1.fastq.gz wget -O K4_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505096/ERR505096_2.fastq.gz wget -O K5_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505097/ERR505097_1.fastq.gz wget -O K5_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505097/ERR505097_2.fastq.gz wget -O K6_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505098/ERR505098_1.fastq.gz wget -O K6_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505098/ERR505098_2.fastq.gz wget -O K7_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505099/ERR505099_1.fastq.gz wget -O K7_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505099/ERR505099_2.fastq.gz wget -O K8_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505100/ERR505100_1.fastq.gz wget -O K8_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505100/ERR505100_2.fastq.gz wget -O K9_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505101/ERR505101_1.fastq.gz wget -O K9_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505101/ERR505101_2.fastq.gz wget -O K10_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505103/ERR505103_1.fastq.gz wget -O K10_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505103/ERR505103_2.fastq.gz wget -O K11_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505105/ERR505105_1.fastq.gz wget -O K11_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505105/ERR505105_2.fastq.gz wget -O K12_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505106/ERR505106_1.fastq.gz wget -O K12_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505106/ERR505106_2.fastq.gz wget -O W1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_1.fastq.gz wget -O W1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_2.fastq.gz wget -O W2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505088/ERR505088_1.fastq.gz wget -O W2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505088/ERR505088_2.fastq.gz wget -O W3_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505084/ERR505084_1.fastq.gz wget -O W3_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505084/ERR505084_2.fastq.gz wget -O W4_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505085/ERR505085_1.fastq.gz wget -O W4_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505085/ERR505085_2.fastq.gz wget -O W5_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505086/ERR505086_1.fastq.gz wget -O W5_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505086/ERR505086_2.fastq.gz wget -O W6_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505087/ERR505087_1.fastq.gz wget -O W6_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505087/ERR505087_2.fastq.gz wget -O W7_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505089/ERR505089_1.fastq.gz wget -O W7_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505089/ERR505089_2.fastq.gz wget -O W8_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505091/ERR505091_1.fastq.gz wget -O W8_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505091/ERR505091_2.fastq.gz wget -O W9_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505092/ERR505092_1.fastq.gz wget -O W9_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505092/ERR505092_2.fastq.gz wget -O W10_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505093/ERR505093_1.fastq.gz wget -O W10_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505093/ERR505093_2.fastq.gz wget -O W11_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505094/ERR505094_1.fastq.gz wget -O W11_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505094/ERR505094_2.fastq.gz wget -O W12_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505083/ERR505083_1.fastq.gz wget -O W12_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505083/ERR505083_2.fastq.gz "]]
