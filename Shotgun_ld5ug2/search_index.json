[["01-Shotgun_metagenomics.html", "Shotgun Metagenomics Chapter 1 Introduction", " Shotgun Metagenomics Sam Haldenby and Matthew R. Gemmell 2022-11-08 Chapter 1 Introduction This practical session aims to introduce you to the analysis of Shotgun metagenomic data. The topics covered are: Overview Raw data Trimming data Host removal Taxonomic profiling Functional profiling Metagenome assembly Binning Functional annotation This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Overview.html", "Chapter 2 Overview 2.1 What is metagenomics? 2.2 Why metagenomics? 2.3 Metagenomics vs Metagenetics 2.4 Tutorial overview", " Chapter 2 Overview 2.1 What is metagenomics? Meta /ˈmɛtə/ : prefix meaning “higher” or “beyond” Metagenomics is the study of genes and genetic material recovered from environmental samples (whether from the sea, soil, human gut, or anywhere else you can imagine). Unlike genomics, metagenomics deals with a multitude of usually diverse species rather than focussing on a single species/genome. 2.2 Why metagenomics? Microbes exist virtually everywhere on Earth, even in some of the most seemingly hostile environments. Every process on our planet is influenced in some way by the actions of microbes, and all higher organisms are intrinsically associated with microbial communities. While much can be learned from studying the genome of a single microbial species in isolation, it does not provide us with any information regarding that species' neighbours, i.e. what else is in its natural environment? Metagenomics offers a top-down approach which allows researchers to investigate and understand interactions between species in different environments, thus providing a much broader and complete picture. 2.3 Metagenomics vs Metagenetics Broadly speaking, there are two families of metagenomic analysis: Amplicon-based: This utilises sequencing data generated from amplified marker sequences, for example, regions of the 16S rRNA. Sequences are clustered together and taxonomically assigned to estimate the species abundance in a sample. This is sometimes referred to metagenetics, as it does not consist of any genomic analysis beyond the marker gene regions. Shotgun: This utilises sequencing data generated from random fragments from total genomic DNA from environmental samples, rather than targeting specific genes. This approach allows for not only species abundance determination but direct functional analysis, too, due to having information on a wide range of genetic data sampled from the population. This is sometimes referenced as metagenomics, as it involves genome-wide analyses. Shotgun metagenomics is the focus of this practical session. 2.4 Tutorial overview 2.4.1 Basics This tutorial and practical session focuses on performing a range of metagenomic analyses using shotgun sequence data from the Illumina platforms. The analyses discussed here are by no means exhaustive and are instead intended to provide a sample of what can be done with a metagenomic dataset. 2.4.2 Structure We prefer to allow people to work at a pace that they are comfortable with rather than ensuring that everyone is at the same point of the tutorial at the same time. There will be no instructor telling you what to type and click. Instead, everything you require to carry out the practical is written in the document. Take your time; it's important to spend some time understanding why you are running the commands, rather than simply typing them out. If at any point you are having trouble or have a question, let one of us know and we'll provide 1-to-1 assistance. 2.4.3 Content This practical is broken up into the following broad sections. Raw data: We will first link to a dataset that we have downloaded for this tutorial. We will take a quick look at what the sequence files look like and briefly discuss the origin of the samples. Trimming data: This entails preprocessing our data to ensure that it is of good quality. Host removal: When sequencing the genomic content of host's microbiota (bacteriome, archaeome, mycobiome, and more) it is likely you will also sequence the host's genome. This step shows a method of removing possible host contamination. Taxonomic profiling: We will analyse the dataset to determine the species abundance in each sample. Following this, we will visualise the data and compare the samples. Functional profiling: We will analyse the dataset to determine the pathway abundance and completeness in each sample. Following this, we will visualise the data and compare the samples. Metagenome assembly: Here, we will move away from just analysing the reads directly and will assemble the metagenome into contigs. Prior to this, we will 'stitch' the reads together to ensure we get the best assembly possible. Binning: This step attempts to seperate each assembled genomes into bins. These genome assemblies are called Metagenome-assembled Genomes (MAGs). Functional annotation: We will take our MAGs, predict genes and then functionally annotate them with MetaCyc. "],["03-Cluster_Introduction.html", "Chapter 3 Cluster Introduction 3.1 Logon instructions 3.2 The Terminal Window", " Chapter 3 Cluster Introduction 3.1 Logon instructions For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent. You will now be in a logged-in Linux VNC desktop. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance. If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal. These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Mint Linux The following link is a guide to install Mint Linux: https://linuxmint-installation-guide.readthedocs.io/en/latest/ 3.2 The Terminal Window In our case the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences it may look different. Already there is useful information for us on the terminal window. nsc065: This is the login name, also known as the username. In this case nsc065 is a demonstrator's account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into. gauss03: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux OS and others '~' is a shortcut to the user's home directory. Everything after the '$' is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen, choose Applications -&gt; Shell -&gt; bash "],["04-Start.html", "Chapter 4 Before we start", " Chapter 4 Before we start During this practical you will use a number of installed programs and scripts. To ensure that the system knows where to look for the scripts, run the following command (ensure this starts with a full stop and a space .): . useshotgun The use scripts in this workshop are custom scripts that set up conda environments. You can look at the above script with less /usr/local/bin/useshotgun if you are interested in its contents. Also, there’s a chance you’re currently not in your home directory, so let’s make sure you are with the following command: cd ~ "],["05-Raw_data.html", "Chapter 5 Raw data 5.1 Obtaining the data 5.2 Checking quality control", " Chapter 5 Raw data The very first thing we need to do is to obtain a dataset to work with. The European Bioinformatics Institute (EBI) provides an excellent metagenomics resource (https://www.ebi.ac.uk/metagenomics/) which allows users to download publicly available metagenomic and metagenetic datasets. Have a browse of some of the projects by selecting one of the biomes on this page. We have selected a dataset from this site that consists of DNA shotgun data generated from 24 human faecal samples. 12 of these samples are from subjects who were fed a western diet and 12 are from subjects who were fed a Korean diet. This dataset comes from the EBI metagenomics resource (https://www.ebi.ac.uk/metagenomics/projects/ERP005558). 5.1 Obtaining the data First, we need to create a directory to put the data in and then change directory to it. mkdir 1-Raw cd 1-Raw Now we can generate a symbolic links (i.e. shortcut) to the raw sequence data files, which will appear in the current directory: ln -s /pub39/tea/nsc006/NEOF/Shotgun_metagenomics/raw_fastq/* . If you would like to know more about the ln command please check out: https://linuxize.com/post/how-to-create-symbolic-links-in-linux-using-the-ln-command/. Now, check the symbolic links are in your current directory: ls There should be six files in the directory, two for each sample in the dataset. e.g. K1_R1.fastq.gz The file ID has three components: K1 is the sample ID. R1 is for the forward reads in the Illumina reads pair (R2 is for the set corresponding to the other end of the reads). fastq.gz tells us that this is a zipped FASTQ file. The three samples are: K1: Fecal sample of individual of Korean diets K2: Fecal sample of individual of Korean diets W1: Fecal sample of individual of Western diets So, what do the R1 and R2 actually mean? With Illumina sequencing the vast majority of sequencing is paired end. i.e. DNA is first fragmented and both ends of each fragment are sequenced as shown here: This results in two sequences generated for each sequenced fragment: One reading in from the 3' end (R1) and the other reading in from the 5' end (R2). FASTQ is a sequence format much like FASTA, with the addition of quality scores. To see what a FASTQ file looks like, we can inspect the first few lines on one of our sequence files: zcat K1_R1.fastq.gz | head -n 4 | less -S The pipe symbol ( | ) is used to pass the output of one command as input to the next command. So, this command (1) shows the unzipped contents of the FASTQ file, (2) displays only the first 4 lines, and (3) displays them without wrapping lines (with –S, for easy viewing). The lines displayed represent one FASTQ sequence entry, or one read of a read pair: The corresponding second read can be viewed by running the same command on K1_R2.fastq.gz. The first line is the read identifier, the second line is the sequence itself, the third line is a secondary header (which is usually left blank except for '+') and the fourth line is the sequence quality score: For each base in the sequence, there is a corresponding quality encoded in this string of characters. To return to the command prompt, press q. Due to computational constraints, the files you have linked to are a subset of the original data (i.e. 1 million read pairs from each sample). 5.2 Checking quality control We can generate and visualise various sequence data metrics for quality control purposes using FastQC. We will run FastQC on the R1 and R2 reads separately as it is good to visualise them in two different reports. This is beacause R1 and R2 reads have different quality patterns, generally due to the poorer quality of R2. Run FastQC on the files: #R1 fastqc #Make an output directory mkdir R1_fastqc #Run fastqc on all the R1.fastq.gz files #* matches any pattern #*R1.fastq.gz matches any file that ends R1.fastq.gz in the current directory #-t 3 indicates to use 3 threads, chosen as there are three R1 files fastqc -t 3 -o R1_fastqc *R1.fastq.gz #R2 fastqc #Make output directory mkdir R2_fastqc #Run fastqc fastqc -t 3 -o R2_fastqc *R2.fastq.gz Once the FastQC commands are run we can run MultiQC to create interactive html reports for the outputs. #R1 multiqc fastqc report #Create output directory mkdir R1_fastqc/multiqc #Create multiqc output multiqc -o R1_fastqc/multiqc R1_fastqc #R2 multiqc fastqc report #Create output directory mkdir R2_fastqc/multiqc #Create multiqc report multiqc -o R2_fastqc/multiqc R2_fastqc Once completed, view the MultiQC reports (NB: The &amp; runs the command in the background, therefore allowing you to continue to run commands while Firefox is still open): firefox R1_fastqc/multiqc/multiqc_report.html \\ R2_fastqc/multiqc/multiqc_report.html &amp; This is a longer command so we've split it across multiple lines. A \\ at the end of a line allows you to press return without running the command, meaning you can continue to add to that command. When this happens, the $ changes to a &gt;. Note if you do use the \\ character, the next key you press must be return. If you use \\ in the middle of a line without pressing return afterwards, it will break the command! The FastQC report (via MultiQC) contains a number of metrics. The \"Sequence Quality Histograms\" shows the sequence quality across the length of the reads, you can hover over each line to show which sample it belongs to. Note how quality decreases as the length of the read increases. While this is normal with Illumina sequencing, we will improve the situation a bit in the next chapter. Once you have finished inspecting, minimise the Firefox window. "],["06-Trimming_data.html", "Chapter 6 Quality control 6.1 Removing adapters and low quality bases 6.2 Rename the files 6.3 Inspect the trimmed data", " Chapter 6 Quality control Now that we've obtained the raw data and had a look at it, we should clean it up. With any sequencing data, it is very important to ensure that you use the highest quality data possible: Rubbish goes in, rubbish comes out. There are two main methods employed to clean sequence data, and a third method specific to some metagenomic datasets. Remove low quality bases from the end of the reads: These are more likely to be incorrect, so are best trimmed off. Remove adapters: Sometimes sequencing adapters can be sequenced if the sequencing runs off the end of a fragment. Host removal: If a metagenomic sample derives from a host species then it may be advisable to remove any reads associated with the host genome. 6.1 Removing adapters and low quality bases Go back to your home directory and create a new directory where we will clean the sequences up: cd .. mkdir 2-Trimmed cd 2-Trimmed You are now in your newly created directory. Here we will run Trim Galore! which removes low quality bases and adapters. trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/K1_R1.fastq.gz ../1-Raw/K1_R2.fastq.gz This command will remove any low quality regions from the end of both reads in each read pair (quality score &lt; 20). Additionally, if it detects four or more bases of a sequencing adapter, it will trim that off too. Task: Rerun this command for the other two samples (K2 and W1). Try to run these without looking at the help box below. trim_galore commands #K2 trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/K2_R1.fastq.gz ../1-Raw/K2_R2.fastq.gz #W1 trim_galore --paired --quality 20 --stringency 4 \\ ../1-Raw/W1_R1.fastq.gz ../1-Raw/W1_R2.fastq.gz 6.2 Rename the files Once that is complete if you run: ls you will notice that we have a new bunch of files created: 2 new read files for each sample along with a trimming report for each file trimmed. However, the new names are needlessly long. For example K1_R1_val_1.fq.gz could be shortened to K1_R1.fq.gz. So, we'll rename all of the files with the mv command: mv K1_R1_val_1.fq.gz K1_R1.fq.gz mv K1_R2_val_2.fq.gz K1_R2.fq.gz mv K2_R1_val_1.fq.gz K2_R1.fq.gz mv K2_R2_val_2.fq.gz K2_R2.fq.gz mv W1_R1_val_1.fq.gz W1_R1.fq.gz mv W1_R2_val_2.fq.gz W1_R2.fq.gz Tip: If you want to edit and reuse previous commands, press the up arrow key. Task: Briefly inspect the log files to see how the trimming went (e.g. K1_R1.fastq.gz_trimming_report.txt). 6.3 Inspect the trimmed data To see what difference the trimming made, run FastQC and MultiQC again on the trimmed output files and view it. #R1 fastqc and multiqc mkdir R1_fastqc fastqc -t 3 -o R1_fastqc *R1.fq.gz mkdir R1_fastqc/multiqc multiqc -o R1_fastqc/multiqc R1_fastqc Task: Run FastQC and MultiQC for the R2 files and then view the R1 and R2 MultiQC reports with firefox. Try to run the commands without looking at the help box below. How does the quality compare to the untrimmed data? R2 commands #R2 fastqc and multiqc mkdir R2_fastqc fastqc -t 3 -o R2_fastqc *R2.fq.gz mkdir R2_fastqc/multiqc multiqc -o R2_fastqc/multiqc R2_fastqc "],["07-Host_removal.html", "Chapter 7 Host removal 7.1 Index reference 7.2 Alignment 7.3 Unmapped read extraction 7.4 Re-pair 7.5 Host removal summary", " Chapter 7 Host removal It is good practice to remove any host sequences from your data before further analysis. A good method for this is to align/map your reads to a reference of your host genome and remove the mapped sequences (i.e sequences we believe to belong to the host). If there is no host genome available before you start your sample collections and sequencing it may be a good idea to attempt to sequence and assemble the host genome. We would recommend long read technologies for single genome assembly projects. This chapter contains a small example on how to carry out host removal. It uses only a section of a human (host of our samples) reference genome assembly. In real life you should use the entire reference. The first step is to copy over the reference fasta file we will use. cp /pub39/tea/nsc006/NEOF/Shotgun_metagenomics/GRCh38_slice.fasta . 7.1 Index reference We will will use the Bowtie2 aligner for mapping/aligning. Prior to alignment/mapping we need to index our reference. bowtie2-build GRCh38_slice.fasta GRCh38_slice.fasta If you use ls you will now see a bunch of files starting with GRCh38_slice.fasta and ending with various suffixes that contain bt2. These are the index files which allow us to use the reference with Bowtie2. 7.2 Alignment With the indexed reference we will align the K1 reads to the reference. This creates a BAM file that contains alignment and read information (K1_mapped.bam). bowtie2 -x GRCh38_slice.fasta -1 K1_R1.fq.gz -2 K1_R2.fq.gz \\ -p12 2&gt; K1_bowtie2_out.log | samtools view -b -S -h &gt; K1_mapped.bam Parameters This command is split into two commands. The first is bowtie2 that creates the alignment. The parameters for the command are: -x: Indexed reference the reads will be aligned to. -1: The forward reads. -2: The reverse reads. -p: Number of threads to be used. 12 in this case. 2&gt;: This will cause the standard error to be redirected to the chosen file. In this case out.log. This useful for commands that may produce a lot of output to screen. If an error occurs you can view this file to see the error messages. The alignment is then piped (|) to the command samtools view. For more information on pipes please see our Intro to Unix course book. The parameters for samtools view are: -b: Output the alignment as a BAM file. BAM files are a binary form of SAM files so they are smaller in memory size. If you are interested in the SAM format please see its specification file. -S: Auto detect input format. -h: Include header. The binary alignment is redirected to a new file called K1_mapped.bam. For more information on redirection please see our Intro to Unix course book. 7.3 Unmapped read extraction Next step is to extract the reads that did not map to the host reference from the K1_mapped.bam file with the samtools fastq command (unmapped reads). samtools fastq -f 4 -1 K1_R1.u.fastq -2 K1_R2.u.fastq K1_mapped.bam Parameters -f: Output reads that only include the SAM flag. In this case 4 stands for unmapped reads. Therefore, our resulting fastq files will only contain unmapped reads. The following link is very useful to create a SAM flag you may need: https://broadinstitute.github.io/picard/explain-flags.html. -1: The output R1 fastq file of unmapped reads. -2: The output R2 fastq file of unmapped reads. This step may make unmatched paired files (why we have .u. in the output file names). This occurs when a read from R2 is removed but the matching read in R1 is not removed, or vice versa. This will cause issues for further analysis. 7.4 Re-pair The below BBTools command will re-pair the reads by removing reads with a missing pair. The command ensures the order of the reads are identical in the 2 paired files. repair.sh in1=K1_R1.u.fastq in2=K1_R2.u.fastq \\ out1=K1_R1.final.fastq out2=K1_R2.final.fastq \\ outs=singletons.fastq Parameters in1=: The input R1 fastq file of unmapped unpaired reads. in2=: The input R2 fastq file of unmapped unpaired reads. out1=: The output R1 fastq file of unmapped paired reads. out2=: The output R2 fastq file of unmapped unpaired reads. outs=: The output fastq file containing the left over singletons (a sequence missing a pair). This file can normally be ignored. 7.5 Host removal summary We have run through quality control including host removal for our K1 sample. As our data has pretty much no human data we will skip this step for the other samples and use the trimmed data for the downstream analysis. In a real analysis project you would use a whole genome reference for your host. However, that would have taken too long for this practical. The most current Human reference (when this was written) is GRCh38. We used a random 10kb section to align our reads to. For more resources on the Human reference please see: https://www.ncbi.nlm.nih.gov/genome/guide/human/ The assembly we used was: https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz "],["08-Taxonomic_profiling.html", "Chapter 8 Taxonomic profiling", " Chapter 8 Taxonomic profiling There are a number of methods for determining the species composition of a metagenomic dataset, but for the purposes of this practical we will use Kraken2 &amp; Bracken (Bayesian Reestimation of Abundance with KrakEN). Kraken2 classifies short DNA with taxonomic labels and is frequently used for metagenomic studies. Bracken uses the taxonomic labels assigned by Kraken2 to compute the abundance of species in a set of DNA sequences. First, we'll make a new directory for it and move into it, after returning home: cd .. mkdir 3-Taxonomy cd 3-Taxonomy "],["09-Kraken2.html", "Chapter 9 Kraken2 9.1 Kraken2: run 9.2 Kraken2: output 9.3 Kraken 2: MCQs", " Chapter 9 Kraken2 Prior to running Kraken2 we need to set a variable so Kraken2 knows where to look for the databases it will use. export KRAKEN2_DB_PATH=/pub39/tea/nsc006/NEOF/Shotgun_metagenomics/kraken2_db Note: You can look at the contents of the above directory to see it currently contains the MiniKraken database. This database contains only a subset of the bacteria, archaea, and viral Kraken2 libraries. This is used in this practical due to restrictions on time and computational resources. For your own analyses we would recommend the full Kraken2 database which uses all the bacteria, achaeal and viral complete genomes that are in Refseq at the time of building. See the following links for info on installing the databases. Standard Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database Custom Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases 9.1 Kraken2: run Now, run Kraken2 on sample K1 by running the following command. Note: We are not using the host removed data to save time. IN your own analysis ensure you are using host removed data. kraken2 --paired --db minikraken2_v1_8GB \\ --output K1.kraken --report K1.kreport2 \\ ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz Parameters --paired: Indicates that we are providing paired reads to Kraken2. Internally, Kraken2 will concatenate the R1 and R2 reads into one sequence with an N between them. --db: Specify the Kraken2 database to be used for taxonomic classification. Previous to the command we set the KRAKEN_DB_PATH so in this case the command will look for the directory called minikraken2_v1_8GB within KRAKEN_DB_PATH. Alternatively the full path of the required database could be provided. --threads: Number of CPUs the process will use. --output: The output file. More info below. --report: The output report file. More info below. ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz: The trimmed read pairs for K1, which we will use as input. 9.2 Kraken2: output There are two major output formats from Kraken2: --output, .kraken: Each sequence (or sequence pair, in the case of paired reads) classified by Kraken2 results in a single line of output. Kraken2's output lines contain five tab-delimited fields; from left to right, they are: \"C\"/\"U\": a one letter code indicating that the sequence was either classified or unclassified. The sequence ID, obtained from the FASTA/FASTQ header. The taxonomy ID Kraken2 used to label the sequence; this is 0 if the sequence is unclassified. The length of the sequence in bp. In the case of paired read data, this will be a string containing the lengths of the two sequences in bp, separated by a pipe character, e.g. \"98|94\". A space-delimited list indicating the LCA mapping of each k-mer in the sequence(s). For example, \"562:13 561:4 A:31 0:1 562:3\" would indicate that: the first 13 k-mers mapped to taxonomy ID #562 the next 4 k-mers mapped to taxonomy ID #561 the next 31 k-mers contained an ambiguous nucleotide the next k-mer was not in the database the last 3 k-mers mapped to taxonomy ID #562 Note: that paired read data will contain a \"|:|\" token in this list to indicate the end of one read and the beginning of another. --report, .kreport2: The report output format. This is required for bracken. It is tab-delimited with one line per taxon. The fields of the output, from left-to-right, are as follows: Percentage of paired reads covered by the clade rooted at this taxon. Number of paired reads covered by the clade rooted at this taxon. Number of paired reads assigned directly to this taxon. A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., \"G2\" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank. NCBI taxonomic ID number Indented scientific name The output to screen will show how many sequences are classified. This will be lower than normal as we are using a mini Kraken2 database. In a real analysis you may use the option --confidence which represents the \"Confidence score threshold\". The default is 0.0, which is the lowest, with the maximum value being 1. A good place to start may be 0.1. Too many classifications are removed if you attempt it with this dataset, due to the mini Kraken2 database used. More info on the confidence scoring can be found at: https://github.com/DerrickWood/kraken2/wiki/Manual#confidence-scoring Task: Once the Kraken2 command has finished running, run it on the other two samples. Attempt the commands without looking at the help box. Hint: You will need to change all instances of K1 to K2 or W1 in the above command K2 &amp; W1 Kraken 2 commands #K2 kraken2 --paired --db minikraken2_v1_8GB \\ --output K2.kraken --report K2.kreport2 \\ ~/2-Trimmed/K2_R1.fq.gz ~/2-Trimmed/K2_R2.fq.gz #W1 kraken2 --paired --db minikraken2_v1_8GB \\ --output W1.kraken --report W1.kreport2 \\ ~/2-Trimmed/W1_R1.fq.gz ~/2-Trimmed/W1_R2.fq.gz 9.3 Kraken 2: MCQs Viewing the Kraken2 output files with your favourite text viewer (less, nano, vim, etc.), attempt the below MCQs. How many paired reads were unclassified for K1? 2 486,723 555,162 How many paired reads were classified for K2 (i.e. number of reads classified at root level and below)? 2 486,723 555,162 How many paired reads were assigned directly to root level for W1? 2 486,723 555,162 What percentage of W1's paired reads were assigned to the clade of Bacteroidetes (Phylum)? 0.12 0.59 14.88 What percentage of K2's paired reads were assigned to the clade of Rikenellaceae (Family)? 0.12 0.59 14.88 What percentage of K1's paired reads were assigned to the clade of Bacteroides helcogenes (Species)? 0.12 0.59 14.88 "],["10-Krona.html", "Chapter 10 Krona plot", " Chapter 10 Krona plot Krona is an interactive metagenome species abundance visualisation tool. We can use the Kraken2 report files to create our Krona plots. With the below command we can import our Kraken2 taxonomy (within the report file) into a Krona html. ktImportTaxonomy -o kraken2.krona.html *.kreport2 -o is our output html file, and the final argument *.kreport2 represents all of our .kreport2 files in the current directory. The * is a wild-card, meaning any characters any number of times. Therefore *.kreport2 identifies the files K1.kreport2 K2.kreport2 and W1.kreport2. You will get a warning that not all taxonomy IDs were found. We will ignore this but in your own future installations this should be addressed with Krona's updateTaxonomy.sh command. Now we can view our interactive chart in a web browser. firefox kraken2.krona.html &amp; Question: Can you tell which sample looks the most different in terms of bacterial species that are present and absent? "],["11-Bracken.html", "Chapter 11 Bracken 11.1 Bracken: output 11.2 Bracken: merging output 11.3 Bracken: extracting output", " Chapter 11 Bracken Bracken (Bayesian Reestimation of Abundance with KrakEN) uses taxonomy labels assigned by Kraken2 to compute estimated abundances of species in a metagenomic sample. 11.0.1 Bracken: run Just like with Krona we can use the Kraken2 report files to run bracken. bracken -d $KRAKEN2_DB_PATH/minikraken2_v1_8GB \\ -i K1.kreport2 -o K1.bracken -r 100 -l S -t 5 Let's look at what those command line options do: -d : Specifies the Kraken2 database that was used for taxonomic classification. In this case bracken requires the variable $KRAKEN_DB_PATH so the option is provided the full path to the kraken database. For clarity try the command ls $KRAKEN2_DB_PATH/minikraken2_v1_8GB. -i : The Kraken2 report file, this will be used as the input. -o : The output Bracken file. Information about its contents is below. -r 100: This is the ideal length of the reads that were used in the Kraken2 classification. It is recommended that the initial read length of the sequencing data is used. We are using 100 here as we used a paired library of 100 bp reads. -l S: This specifies the taxonomic level/rank of the Bracken output. In this case S is equal to species with the other options being D, P, C, O,F and G. -t 5: This specifies the minimum number of reads required for a classification at the specified rank. Any classifications with fewer reads than the specified threshold will not receive additional reads from higher taxonomy levels when distributing reads for abundance estimation. Five has been chosen here for this example data but in real datasets you may want to increase this number (default is 10). 11.1 Bracken: output The output file of Bracken contains the following columns: Name: Name of taxonomy at the specified taxonomic level. Taxonomy ID: NCBI taxonomy id Level ID: Letter signifying the taxonomic level of the classification Kraken assigned read: Number of reads assigned to the taxonomy by Kraken2. Added reads with abundance reestimation: Number of reads added to the taxonomy by Bracken abundance reestimation. Total reads after abundance reestimation: Number from field 4 and 5 summed. This is the field that will be used for downstream analysis. Fraction of total reads: Relative abundance of the taxonomy. Use less or vim to look at the bracken output. Task: Repeat the above commands for K2 and W1 11.2 Bracken: merging output To make full use of Bracken output, it is best to merge the output into one table. However before we do this we’ll copy the Bracken output of other samples that have been generated prior to the workshop. These are all either Korean or Western Diet samples. cp /pub39/tea/nsc006/NEOF/Shotgun_metagenomics/bracken/* . Now to merge all the K and W Bracken files. combine_bracken_outputs.py --files [KW]*.bracken -o all.bracken This output file contains the first three columns: name = Organism group name. This will be based on the TAX_LVL chosen in the Bracken command and will only show the one level. taxonomy_id = Taxonomy id number. taxonomy_lvl = A single string indicating the taxonomy level of the group. ('D','P','C','O','F','G','S'). Following these columns are the following two columns for each sample. ${SampleName}.bracken_num: The number of reads after abundance reestimation ${SampleName}.bracken_frac: Relative abundance of the group in the sample 11.3 Bracken: extracting output We want a file with only the first column (organism name) and the bracken_num columns for each sample. To carry this out we first create a sequence of numbers that will match the bracken_num column numbers. These start at column 4 and are every even numbered column after this. We will use seq to create every 2 numbers from the numbers 4 to 50 with commas (,) as separators (-s). Note: The number 50 is chosen as 3 (first three info columns) + 24*2 (24 samples with 2 columns each) = 50. #Try out the seq command to see its output seq -s , 4 2 50 #Create variable bracken_num_columns=$(seq -s , 4 2 50) echo $bracken_num_columns Now to use the variable to extract the bracken_num columns plus the first column (species names). cat all.bracken | cut -f 1,$bracken_num_columns &gt; all_num.bracken "],["12-Lefse.html", "Chapter 12 LEfSe biomarker detection 12.1 LEfSe: add metadata 12.2 LEfSe: conda 12.3 LEfSe: format 12.4 LEfSe: run 12.5 LEfSe: visualisation 12.6 Kraken2 and Bracken databases", " Chapter 12 LEfSe biomarker detection We will use LEfSe (Linear discriminant analysis Effect Size) to determine which taxa can most likely explain the differences between the Western and Korean diet. LEfSe couples standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. It can be used with other features such as organisms, clades, operational taxonomic units, genes, or functions. In essence it allows for the detection of biomarkers when comparing sample groups. In the LEfSe terminology the sample groups are called the class. 12.1 LEfSe: add metadata We need to add metadata to our Bracken file to be ready for LEfSe. First we will copy the file so we have a backup in case we do anything wrong. cp all_num.bracken all_num.lefse.bracken Using your favourite text editor (e.g. nano, vim, etc.) add the following line to the top of your all_num.lefse.bracken file. The words are separated by tabs. If you are not sure how to carry out this task please ask a demonstrator. diet K K K K K K K K K K K K W W W W W W W W W W W W Note: The above is diet followed by 12 K and 12 W. The singular line should match the order of your samples within the file. This is the metadata line that LEfSe will use to determine which samples belong to each sample group, and therefore which to compare. In this case it is Korean diet samples versus Western diet samples. 12.2 LEfSe: conda LEfSe requires python2 whilst we have been using packages that required python3. We therefore need to use a different conda environment. We are currently using the conda env called shotgun_meta as represented by (shotgun_meta). This was activated with the command . useshotgun. Open a new terminal (right click on the main screen background, choose Applications -&gt; Shell -&gt; bash) and run the following in the new terminal to activate the lefse conda environment. #Setup environment . uselefse #Change directory cd ~/3-Taxonomy 12.3 LEfSe: format We need to further format and preprocess our file with a LEfSe script. lefse-format_input.py all_num.lefse.bracken all_num.lefse -c 1 -u 2 -o 1000000 all_num.lefse.bracken : Input Bracken file. all_num.lefse : Output file formatted for the run_lefse command, which we will soon run. -c 1 : Specifies the row with the class info. This is used to determine which samples will be compared against which samples. In this case it is the first row with the Ks and Ws. -u 2 : Specifies the row with the sample names. This is the second row in this case. -o 1000000 : An integer can be indicated to determine to what size (count sum value) each sample should be normalised to. LEfSe developers recommend 1000000 (1 million) when very low values a present. We generally always use 1 million for consistency. 12.4 LEfSe: run Now to run LEfSe. All we need to do is run the command with the formatted input and provide an output file name. run_lefse.py all_num.lefse all_num.lefse.out The output file is a tab-delimited file which contains a row for each species. Biomarkers will have the five columns below whilst non-biomarkers will have the first two followed by a \"-\" . Biomarker name Log of highest class average: I.e. get the class with the greater amounts of the biomarker, average the counts and then get the log of this value. Class with the greater amounts of biomarker LDA effect size: A statistical figure for LEfSe.. p-value: Biomarkers must have a p-value of &lt;0.05 to be considered significant. The LDA effect size indicates how much of an effect each biomarker has. The default is to only count a species with an LDA effect size of greater than 2 or less than -2 as a biomarker. The further the LDA effect size is from 0 the greater the effect the species causes. 12.5 LEfSe: visualisation Next we can visualise the output. lefse-plot_res.py --dpi 200 --format png all_num.lefse.out biomarkers.png --dpi 200 : Dots per inch. This refers to the resolution of the output image. Normally publications want 300 dpi. We’ve chosen 200 as it is good quality and we will not be publishing these results. --format png : Format of output file. png is a commonly used file format for images. all_num.lefse.out : LEfSe output to visualise. biomarkers.png : Plot showing the LDA scores of the species detected as biomarkers. Colouring shows which class (K or W) the species is found in higher abundance. Look at the figure with the program okular: okular biomarkers.png Questions: Which species causes the biggest effect in the W class and in the K class? Which class has more biomarkers associated with it? Note: In this instance green bars represent biomarkers in higher abundance in the W samples whilst the red bars represent biomarkers in higher abundance in the K samples. 12.6 Kraken2 and Bracken databases In your own future analysis you will need to create your own Kraken2 and Bracken databases. Please sse the following links on information for this: Kraken2 Standard Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#standard-kraken-2-database Custom Kraken2 databases: https://github.com/DerrickWood/kraken2/wiki/Manual#custom-databases Bracken https://ccb.jhu.edu/software/bracken/index.shtml?t=manual#step1 This requires a Kraken2 database to be built first. "],["13-Functional_profiling.html", "Chapter 13 HUMAnN Functional profiling 13.1 HUMAnN 13.2 Multi sample processing 13.3 Heatmap 13.4 LEfSe 13.5 Optional task", " Chapter 13 HUMAnN Functional profiling It is possible to investigate functional differences between metagenome (and metatranscriptome) samples by directly interrogating the read data. We will look at how this can be done with a package called HUMAnN (The HMP Unified Metabolic Analysis Network), a bioBakery pipeline designed to accurately profile the presence/absence and abundance of microbial pathways in metagenomic sequencing data. HUMAnN is on its third version and was developed in tandem with the third version of MetaPhlAn, a computational tool for profiling the composition of microbial communities from metagenomic data. It is highly recommended you use the new version of HUMAnN as it contains 2 times more species pangenomes and 3 times more gene families than HUMAnN2. Please see below for a diagram showing the pipline of HUMAnN: 13.1 HUMAnN First, we will carry out an example run of the software and briefly explore the output files. HUMAnN can take a long time to run so we will use a small amount of example data. Additionally, we will use a subset of the HUMAnN databases for the analysis but when running analysis on your own data you should use the full databases. information on installing HUMAnN and its databases can be found in its online Home Page. 13.1.1 HUMAnN: conda, directories, and files We need a new conda environment again. Open a new terminal (right click on the main screen background, choose Applications -&gt; Shell -&gt; bash) and run the below: chos 8 . usebiobakery3 Make a new directory and move into it. mkdir ~/4-FunctionalProfiling cd ~/4-FunctionalProfiling Copy over some test data we will carry out the analysis on. This is a demonstration FASTQ file that we will use. It will be small enough to run HUMAnN in a reasonable amount of time. cp /pub39/tea/nsc006/NEOF/Shotgun_metagenomics/humann/demo.fastq.gz . 13.1.2 HUMAnN: run Run the HUMAnN pipline with our demo data: humann \\ --input demo.fastq.gz \\ --output demo.humann \\ --threads 10 Here, we are telling the software to use demo.fastq.gz as input and to create a new output directory called demo.humann where the results will be generated. As the software runs you might notice that HUMAnN runs MetaPhlAn. The purpose of this is to identify what species are present in the sample, so HUMAnN can tailor generate an appropriate database of genes (from those species) to map against. It will carry out this alignment against the gene database, then a protein database, and finally compute which gene families are present. HUMAnN will determine which functional pathways are present and how abundant they are. 13.1.3 HUMAnN: output Once the run has completed, change into the newly created output directory and list the files that are present. cd demo.humann ls You will see that there are three files and one directory. The directory (demo_humann2_temp) contains intermediate temporary files and can be disregarded here. The three output files are: demo_genefamilies.tsv: A table file showing the number of reads mapping to each UniRef90 gene family. Values are normalised by the length of each gene family (i.e. RPK, or Reads per Kilobase). Additionally, the values are stratified so that they show the overall community abundance but also a breakdown of abundance per species detected. This allows researchers to delve into species specific functions, rather than only looking at the metagenomic functions as a whole, demo_pathabundance.tsv: A table file showing the normalised abundance of MetaCyc pathways (RPKs). These abundances are calculated based on the UniRef90 gene family mapping data and are also stratified by species. demo_pathcoverage.tsv: A table file that shows the coverage, or completeness, of pathways. For example, a pathway may contain 5 components (or genes/proteins) Pathway1 : A → B → C → D → E 100% complete A species identified in the sample may only have four of the components, meaning that the pathway is only 80% complete (represented as 0.8) Pathway1 : A → B → C → D → E 80% complete The basic format of these three output files is the same, so let's take a look at the pathway abundance table. less demo_pathabundance.tsv You will see that there are two columns: The first column shows the pathways. UNMAPPED indicates reads that could not be aligned. UNINTEGRATED indicates reads that aligned to targets not implicated in any pathways. The second column shows the abundance. This file is not too interesting to look at as it is only demo data. Therefore, press q to exit less and let's look at some real data. Note: The directory demo_humann2_temp can be very large and so should be deleted in real projects once you are certain they are not needed. However, these files can be useful for debugging. Note: Link to more detail on Output files 13.2 Multi sample processing Looking at the functional profile of one sample in isolation is usually not very informative. First, there is nothing to compare it to and second, there are no biological replicates. We will therefore use all the Korean and Western diet samples. It would take many hours to analyse all of the data using HUMAnN and is outside the scope of this course. For this reason, samples were analysed prior to the workshop to generate the output files we covered above. For the purposes of this comparison, we will look at the pathway abundances only. First copy over the data directory containg the gene families tables and have a look in it. #Ensure you are in the correct directory cd ~/4-FunctionalProfiling #Copy directory with pre made results cp –r /pub39/tea/nsc006/NEOF/Shotgun_metagenomics/DietPathAbundance . #Move into the copied directory cd DietPathAbundance #List files ls You will see there are 12 files prefixed with K and 12 prefixed with W, for the Korean diet and Western diet samples, respectively. Take a look at the file for K1. less K1_pathabundance.tsv There are a lot of pathways in the file. Quit out of the less viewer (q) and look at the entries for one specific pathway, COA-PWY-1 (a coenzyme A biosynthesis II pathway). grep COA-PWY-1 K1_pathabundance.tsv This shows 30 entries/lines with the top entry/line: COA-PWY-1: superpathway of coenzyme A biosynthesis III (mammals) 6790.1517478104 This shows the abundance of the pathway across the entire sample (6790.1517478104). The other entries show the species stratification information (mentioned above) of the pathway. I.e. the second line: COA-PWY-1: superpathway of coenzyme A biosynthesis III (mammals)|g__Bacteroides.s__Bacteroides_dorei 1292.7711872228 shows the abundance of the pathway that is contributed by the species Bacteroides dorei (1292.7711872228). Note: The species stratified pathway abundances may not equal the total community pathway abundance. Please see the this forum post for details. With this information we will carry out some comparisons including biomarker detection to determine which pathways are differentially abundant between the Western diet and Korean diet samples. Note: The following methods/pipeline can be used for the genefamilies and pathcoverage tables in your own future analyses. 13.2.1 HUMAnN: combining data First, we need to combine these 24 tables into one large results table. HUMAnN provides a tool to do this: #Change directory to main Functional profiling directory cd ~/4-FunctionalProfiling #Join the tables humann_join_tables --input DietPathAbundance/ --output diet.tsv This command will look for all tables in the DietPathAbundance directory and generate a large, 25 column table called diet.tsv. You can inspect the file to ensure that this has worked correctly. less -S diet.tsv 13.2.2 HUMAnN: split stratified table For this tutorial we do not want the species stratification information. We will therefore split the table to create 2 new files: diet_unstratified.tsv: This table only contains the total abundance values for the pathways. It does not contain any species stratification information. diet_stratified.tsv: This table only contains the species stratification abundance values for the pathways. It does not contain the total abundance information. To create the split files and output them to your current directory, run the following command: humann_split_stratified_table --input diet.tsv --output . We will use the file diet_unstratified.tsv for our downstream analysis. Before you move on feel free to inspect the output files with the less command. Note: You can use any of the three tables (unsplit table, unstratified table, or stratified table) in your own analysis. This depends on your question and data 13.2.3 HUMAnN: renormalising data The next step is to renormalise the data. Currently, all of the abundance values are only normalised within each sample (RPKs). However, they are not normalised between samples, and this is very important to do. For example, if we had sequenced two samples, A and B, and we obtained 5 million reads for sample A and 20 million reads for sample B, without normalisation, it might look that there was up to 4x as much functional activity in sample B! To correct for this, we normalise the abundance values based on the number of reads in each sample. We will normalise to relative abundance where all abundances for each sample add up to 1. Renormalisation command: humann_renorm_table \\ --units relab \\ --input diet_unstratified.tsv \\ --special n \\ --output diet_unstratified.relab.tsv This command generates the normalised data in the new table diet_unstratified.relab.tsv. The --special n option tells the script to remove all unmapped and unassigned values (UNMAPPED &amp; UNINTEGRATED) from the table. Note: With the gene families information ensure you normalise by CPM (counts per million) with the option --units cpm. More info can be found on the Normalizing RPKs to relative abundance section of the HUMAnN 3.0 tutorial. 13.3 Heatmap Now that we have our combined, unstratified, and normalised table, we can visualise the dataset to see how the two groups compare. Do samples in the same diet group appear to correlate well with each other? Are samples from one diet group distinguishable from those from the other diet group? To visualise this we will create a heatmap with hclust2. Before carrying out the command we will need edit the file. Carry out the following alterations: Remove the _Abundance part of the sample names whilst creating a copy that we will use (It is always a good idea to keep the original file in case a mistake happens). cat diet_unstratified.relab.tsv | sed &quot;s/_Abundance//g&quot; &gt; diet_unstratified.relab.comp.tsv Intro to unix links: Text editing with sed Redirection with &gt; Next using your text editor of choice carry out the following changes on the file diet_unstratified.relab.comp.tsv. Remove the # (including the one space after the #) from the start of the header so it starts as Pathway. Add in the same metadata line as we did for 8.4.1 but this time below the header line, i.e. as the 2nd line (ensure you are using tabs instead of spaces). Now we can use the hclust2 tool to create a heatmap of our pathway abundances. hclust2.py \\ -i diet_unstratified.relab.comp.tsv \\ -o diet_unstratified.relab.heatmap.png \\ --ftop 40 \\ --metadata_rows 1 \\ --dpi 300 Note: You will get 2 MatplotlibDeprecationWarnings, these are normal and can be ignored. However, ensure these ar ethe only warnings/errors before continuing. The arguments for this command are: -i: The input table file. -o: The output image file. The tool does not specify what types of image files you can use but .png is always a good image file format. --ftop: Specifies how many of the top features (pathways in this case) to be included in the heatmap. --metadata_rows: Specifies which row/s contain the metadata information to be used for the group colouring at the top of the heatmap. Row numbers start at 0 for this tool. Therefore our sample names are in row 0 and the diet info is in row 1. Multiple rows can be specified if you have multiple rows of metadata. e.g. --metadata_rows 1,2,3. --dpi: The image resolution in dpi (dots per inch). 300 dpi is used for publication quality images. There are many more options that can be seen on the hclust2 github. Now we can view the plot. firefox diet_unstratified.relab.heatmap.png From this, we can see that there is a small amount of clustering caused by the differences between the Korean and Western diet. Other factors that we do not know about the samples must also come into play. This is normal as we cannot account for everything but it is good to try to account for as much as possible. Questions: Do any of the pathways stand out? If any do, look up the pathway in the table file to see what it does. 13.4 LEfSe For the final part of this section, we will see if there are any statistically significant differences between the two sample groups. There are several ways in which this can be achieved but we will carry out LEfSe again. Task: Go back to your LEfSe terminal (or create a new one and use . uselefse). Then change directory to 4-FunctionalProfiling) Thankfully we already formatted the file to work with LEfSe when we formatted it for hclust2 #LEfSe format lefse-format_input.py \\ diet_unstratified.relab.comp.tsv \\ diet_unstratified.relab.comp.lefse \\ -c 2 -u 1 -o 1000000 #Run LEfSe run_lefse.py \\ diet_unstratified.relab.comp.lefse \\ diet_unstratified.relab.comp.lefse.out #Produce LEfSe plot lefse-plot_res.py \\ --dpi 200 \\ --format png \\ diet_unstratified.relab.comp.lefse.out \\ diet_unstratified.relab.comp.lefse.png #View plot firefox diet_unstratified.relab.comp.lefse.png Look at the output and see what pathways count as biomarkers for the 2 groups. 13.5 Optional task Carry out all the steps starting from Multi sample processing with the gene families information. Copy the gene families data from /pub39/tea/nsc006/NEOF/Shotgun_metagenomics/DietGeneFamilies Ensure you set the --units option to cpm in the renormalising data step. On top of analysing the unstratified data you can also analyse the stratified data. That completes the non assembly approach to shotgun metagenomic analysis. The next chapters will cover an assembly approach. "],["14-Metagenome_assembly.html", "Chapter 14 Metagenome assembly 14.1 Metagenome assembly: Conda 14.2 A primer on short read assembly 14.3 Stitching read pairs 14.4 Assembly 14.5 QUAST", " Chapter 14 Metagenome assembly So far we have directly analysed the read data itself which is perfectly fine for taxonomic profiling and for certain methods of functional profiling. However, Illumina reads are generally short and therefore can not provide us with much data on larger constructs that are in the metagenomic samples, e.g. genes. While it is possible to predict from which gene a sequence read might originate, the short nature of the query can sometimes lead to ambiguous results. Additionally, depending on the application it can become computationally intensive to analyse large numbers of reads. Here, we are only using samples with 1 million reads. Some metagenome samples consist of 50-100 million+ read pairs. If such a sample belonged to a set of 100 samples, that would be up to 10 billion read pairs, or 2 trillion bases of sequence data, with many of these being redundant. For this reason, it is sometimes advantageous to assemble the reads into contigs, using a meta-genome assembler. This has the dual effect of: Reducing the overall size of the data for analysis. If a metagenome was sequenced at 50x depth, then by assembling it you could theoretically reduce the amount of sequence to analyse by 50-fold. Increase the size of the fragments you will analyse. This is the main advantage of an assembly, as the ~100 bp reads can be pieced together to form 100,000 kb+ contigs. These contigs will contain complete genes, operons and regulatory elements: Reconstructed genome sections. Here, we will carry out a couple of assemblies on our dataset. 14.1 Metagenome assembly: Conda We will use the shotgun_meta conda environment so use a terminal where this is activated or open a new one and run . useshotgun. 14.2 A primer on short read assembly Illumina reads are too short and numerous to use traditional overlap-layout-consensus assemblers as such an approach would be far too computationally intensive. Instead, we use De Bruijn graph based assemblers. Briefly, these operate as follows: All reads are broken down into k-length overlapping fragments (k-mers). e.g. if we choose a k-mer size of 5 bp, the following two sequences (blue) would be broken down into the k-mers below them (red): All k-mers are linked to other k-mers which match with a k-1 length overlap (i.e. that overlap by all but one base: Paths are routed through the graph and longer contigs are generated: The example here is a vast oversimplification of the complexity of a De Bruijn graph (i.e. there are no branches!). Routing through the graph is never as simple as this as some k-mers will lead to multiple k-mers, which can result in the break point of a contig. This is especially true for complex metagenomic data. Generally speaking, the shorter the k-mer, the more branches there will be, the trickier the graph is to resolve, so the resulting contigs are smaller. Assemblers usually perform better with longer k-mer lengths but even then there might not be enough depth of sequencing to generate all k-mers that form overlaps, therefore leading to break points. Finding the right k-mer size usually involves testing several. Fortunately, the assembler we will use, MEGAHIT, allows us to build an assembly using multiple k-mer lengths iteratively. The other great advantage about MEGAHIT is that it is quick and efficient. We will use MEGAHIT on our data soon, but first there is an additional processing step for our sequences... 14.3 Stitching read pairs As mentioned, longer k-mers generally perform better, but as our maximum read length is 100 bp, we are limited to a maximum k-mer length of 99 bp. However, we can get even longer k-mers if we stitch our read pairs together. Note: This method will not work if your reads have no overlap. If you are not sure if your reads have overlap ask the team who sequenced them. Remember that a read pair consists of two sequences read from each end of a fragment of DNA (or RNA). If the two sequences meet in the middle of the fragment and then overlap, there will be a region of homology which we can use to merge the two reads in the pair together (See next image). First, we obtain our forward and reverse reads, derived from different ends of the same fragment. Second, we look for sufficient overlap between the 3' ends of our sequences. Third, if there is sufficient overlap, we combine, or stitch, the two reads together to form one long sequence. Once we have longer stitched reads, we can increase the k-mer length for our assembly. There are a number of pieces of software that can be used to stitch reads (e.g. Pear, Pandaseq) but today we will use one called FLASH: Make a new output directory for the stitched reads and run FLASH: #Change directory to home cd ~ #Make and move into new directory mkdir 5-Stitched cd 5-Stitched #Run flash flash -o K1 -z -t 12 -d . \\ ../2-Trimmed/K1_R1.fq.gz ../2-Trimmed/K1_R2.fq.gz Here, we are telling FLASH to use an output file name prefix of K1, that the input is zipped, that the output directory is here (.) and to use the two read files for Sample K1. Once FLASH has finished running, it will display on screen how well the stitching process went, in this case a low amount of reads were combined. Have a look what files have been generated. ls We have three new fastq.gz files. One containing the stitched reads (K1.extendedFrags.fastq.gz) and two containing the reads from pairs that could not be combined (K1.notCombined_1.fastq.gz and K1.notCombined_2.fastq.gz). We can also see what the new read lengths are: less K1.histogram Scroll down with the down key and you will see that we are looking at a histogram showing the proportion of reads at different lengths. We can now start assembling our stitched reads for this sample. 14.4 Assembly Create a new directory to store our assembly in. cd .. mkdir 6-Assembly cd 6-Assembly Now run the metagenome assembler MEGAHIT using our newly stitched read data. megahit \\ -r ../5-Stitched/K1.extendedFrags.fastq.gz \\ -1 ../5-Stitched/K1.notCombined_1.fastq.gz \\ -2 ../5-Stitched/K1.notCombined_2.fastq.gz \\ -o K1 \\ -t 12 \\ --k-list 29,49,69,89,109,129,149,169,189 Here, you have instructed MEGAHIT to use both the stitched and unstitched reads, to output the assembly in a subdirectory called K1 and to use 12 CPUs. The last option --k-list instructs MEGAHIT to first generate an assembly using a k-mer size of 29 bp and when that is complete, integrate the results into an assembly using a k-mer size of 49 bp, and so on up to a final iteration using a k-mer size of 189 bp. This large range of k-mer lengths should give us a good assembly, given the data. However, it may take a while to run so this might be a good time to read on. If you need a command prompt (your current one is gone because MEGAHIT is running), right click on the main screen, choose Applications -&gt; Shell -&gt; bash. Once the assembly is completed, we can look at the output FASTA file containing the contigs: less K1/final.contigs.fa 14.5 QUAST We can also generate some metrics based on the assembly. Due to python version conflict we need to use another conda environment. Open a new terminal (right click on the main screen, choose Applications -&gt; Shell -&gt; bash) and run the below. #use script to activate conda env . usegenoassess We will use QUAST for genome contiguity assessment but first we will change directory to 6-Assembly and create a directory for the QUAST output. #Change directory cd ~/6-Assembly #Create QUAST output directory #The option -p will create a directory and any required #parent directories mkdir -p quast/K1 The -p option of mkdir will cause the command to create any parent directories that are required to create the noted directory. I.e. quast will be created so K1 can be created. Now to run QUAST. #Generate contiguity statistics quast -o quast/K1 K1/final.contigs.fa #View QUAST html report firefox quast/K1/report.html The report tells us quite a bit about the assembly quality. Two definitions that you may not be aware of are N50 and N50 length (or, somewhat confusingly, L50 and N50, respectively!). If we were to order our contigs from largest to smallest, and total up the sizes from biggest downwards, the contig we reach where our total is 50% of the size of the whole assembly is the N50 contig (the smaller the number the better). The N50 length is the length of this contig; a weighted median contig length. Questions - How do the contig metrics compare to the original reads? Now we have an assembly, albeit not a brilliant one due to us only having used 1 million reads, we can start to explore it. There is also a metaQUAST specifically for metagenome assemblies but it requires reference assemblies be provided. "],["15-Genome_binning.html", "Chapter 15 Genome binning 15.1 MetaBAT2 15.2 CheckM", " Chapter 15 Genome binning A metagenome assembly consists of contigs from many different genomes. At this stage we don't know which contigs are from which species. We could try to taxonomically classify each contig but there are 2 problems with this approach: Some contigs may be misclassified which can lead to multiple contigs from the same genome/organism being classified as various taxa. Databases are incomplete and so some contigs will not be classified at all (microbial dark matter). To alleviate these issues genomic binning can be carried out. This will cluster contigs into bins based on: Coverage: Contigs with similar coverage are more likely to be from the same genome. Composition: Contigs with similar GC content are more likely to belong to the same genome. Genomic binning has been used to discover many new genomes. Additionally, it makes downstream analyses quicker as the downstream steps will be carried out on the sets of bins rather than on one large metagenome assembly. Binning produces \"bins\" of contigs of various quality (e.g. draft, complete). These bins are also know as MAGs (Metagenome-assembled genomes). In other words a MAG is a single assembled genome that was assembled with other genomes in a metagenome assembly but later separated from the other assemblies. The term MAG has been adopted by the GSC (Genomics Standards Consortium). It is recommended to ensure you do not have a poor quality metagenome assembly. Binning requires contigs of good length and good coverage. Extremely low coverage and very short contigs will be excluded from binning. 15.1 MetaBAT2 We will use MetaBAT2 for our genome binning. It is a relatively new binning tool with three major upsides that makes it very popular: It has very reliable default parameters meaning virtually no parameter optimisation is required. It performs very well amongst genome binners. It is computationally efficient compared to other binners (requires less RAM, cores etc.) 15.1.1 MetaBAT2: Conda environment &amp; directory We will use the shotgun_meta conda environment to start for this chapter. Additionally, make a new directory and move into it. #Make directory mkdir -p ~/7-Binning/K1 #Move into it cd ~/7-Binning/K1 15.1.2 MetaBAT2: depth calculation To carry out effective genome binning MetaBAT2 uses coverage information of the contigs. To calculate depth we need to align the reads to the metagenome assembly. For the alignment we will use bwa. We need to index our assembly file prior to alignment. bwa index ~/6-Assembly/K1/final.contigs.fa Next we will align our trimmed paired reads we used to create the stitched reads. We will carry this out with the bwa mem command. bwa mem is a good aligner for short reads. If you are using long reads (PacBio or Nanopore) minimap2 will be more appropriate. bwa mem ~/6-Assembly/K1/final.contigs.fa \\ ~/2-Trimmed/K1_R1.fq.gz ~/2-Trimmed/K1_R2.fq.gz &gt; \\ K1.sam After alignment we need to get the file ready for the contig depth summarisation step. This requires converting the sam file to a bam (binary form of a sam file) file and then sorting the bam file. # Convert sam to bam file samtools view -bu K1.sam &gt; K1.bam # Created sorted bam file samtools sort K1.bam &gt; K1.sort.bam Now we can summarise the contig depths from the sorted bam files with MetaBAT2's jgi_summarize_bam_contig_depths command. jgi_summarize_bam_contig_depths --outputDepth K1.depth.txt K1.sort.bam You can have a look at the depth file and you will notice there are many contigs with low coverage (&lt;10) and of short length (&lt;1500). less K1.depth.txt To get a better look we will open the file in R and look at a summary of the file's table. Activate R: R Now in R we will read in the file and get a summary() of it. #Read in the table as an object called df (short for data frame) #We want the first row to be the column names (header=TRUE) #We do not want R to check the column names and &quot;fix&quot; them (check.names=FALSE) df &lt;- read.table(&quot;K1.depth.txt&quot;, header=TRUE, check.names=FALSE) #Create a summary of the data summary(df) We can see the numbers of the contigLen and totalAvgDepth are very low. However, this is most likely due to a bunch of short low coverage contigs which will be ignored by MetaBAT2. Therefore we will remove rows with information on contigs shorter than 1500 and rerun the summary. MetaBAT2's documentation dictates the minimum contig length should be &gt;=1500 with its default being 2500. #Set the new object &quot;df_min1500len&quot; as all rows #where the value in the column &quot;contigLen&quot; of &quot;df&quot; #Is greater than or equal to 1500 df_min1500len &lt;- df[df$contigLen &gt;= 1500,] #Summary of our new data frame summary(df_min1500len) That is looking better. The minimum average coverage for MetaBAT2 is 1 and our minimum is higher than that. Now you can quit R and continue. #quit R q() #On the prompt to save your workspace press &quot;n&quot; and then enter. Note: One of the reasons for our short contigs is that we only used a subset of our sequencing dataset for this tutorial due to time concerns. 15.1.3 MetaBAT2: run With our assembly and its depth information we can run MetaBAT2 for binning. #make a diretcory for the bins mkdir bins #Run MetaBAT2 metabat2 \\ --inFile ~/6-Assembly/K1/final.contigs.fa \\ --outFile bins/K1 \\ --abdFile K1.depth.txt \\ --minContig 1500 List the contents of the output directory and you'll see there are 2 files with the prefix of K1. These are the 2 bins that will hopefully contain 1 MAG (Metagenome-Assembled Genome). ls bins 15.2 CheckM CheckM is a useful tool to assess the quality of assembled bacterial and archaeal genomes. This can be used on assemblies produced from single cell, single isolate, or metagenome data. Additionally, it can be used to identify bins that are likely candidates for merging. This occurs when one genome has been separated into different bins. An important part of CheckM is the ubiquitous and single-copy genes it utilises. It has sets of these genes for different phylogenetic lineages. With these it can determine: What lineage a bin/MAG belongs to. Does it contain genes only found in Escherichia? How complete the bin/MAG is. A set of lineage specific genes should all be found in a genome belonging to the lineage (ubiquitous). What percentage of these lineage specific genes are present in the MAG? &gt;95% is very good &gt;80% is good &gt;70% is ok &lt;70% is poor to poorer. How contaminated is the bin/MAG? Only one copy of each gene should be present (single-copy). Are there any markers for other lineages present? 15.2.1 CheckM: Conda Due to program version conflicts we will use the checkm conda environment for this section. Open a new terminal and activate the checkm environment. . usecheckm Ensure you are in the correct directory. cd ~/7-Binning/K1/ 15.2.2 CheckM: run CheckM has many different commands. We will use one of the common workflows it provides called lineage_wf. This carries out five of its commands in a workflow (i.e. the next step uses output from the previous step). tree - Places bins in the reference genome tree. This reference tree comes with CheckM. tree_qa - Assess the phylogenetic markers found in each bin. lineage_set - Infers lineage-specific marker sets for each bin. analyze - Identifies marker genes in bins. qa - Assesses the bins for contamination and completeness. Run the CheckM command (this will take a while): checkm lineage_wf \\ --tab_table -f MAGS_checkm.tsv \\ -x fa \\ bins/ checkm_output The options we used are: --tab_table : Prints results to a tab separated table. -f : File name to print result to (if not specified results will go to stdout). -x : Suffix/extension of bin files. Other files are ignored in the specified bin directory. fa is used as our MetaBAT2 produce fasta files end in .fa. bins : The second last argument is the bin_dir, the directory containg all the bins in fasta format. checkm_output : The last argument is the directory to store the output to. This directory should not exist prior to running. 15.2.3 CheckM: output As we have only used a subset of data the results are not very good so let us look at premade results. These premade results were produced using the entire K1 dataset. First you will need to copy it over. cd ~/7-Binning mkdir K1_fullset cd K1_fullset ln -s /pub39/tea/nsc006/NEOF/Shotgun_metagenomics/binning/K1_fullset/* . Now we can look at the results table that is in your current directory. less MAGS_checkm.tsv Tip: The definition of the statistics can be found at: https://github.com/Ecogenomics/CheckM/wiki/Reported-Statistics One quick way to calculate the overall quality of a bin is with the following equation: \\[ q = comp - (5 * cont) \\] Where: q = Overall quality comp = Completeness cont = Contamination A score of 70-80% would be the aim. Therefore let us calculate this for the bins with R. First the tab delimited file has some issues for R so we'll make a comma separated version. cat MAGS_checkm.tsv | tr &quot;\\t&quot; &quot;,&quot; &gt; MAGS_checkm.csv Activate R: R Now to do the calculation in R for each bin: #Read in csv file as data frame df &lt;- read.csv(&quot;MAGS_checkm.csv&quot;, header=TRUE, check.names=FALSE) #Add a column for overall quality df$quality &lt;- df$Completeness - (5 * df$Contamination) #View the bin name, lineage, number genomes, #completeness, contamination, and the quality columns df[,c(1,2,3,12,13,15)] Questions: What lineages were assigned to the bins? How many genomes are present in the different bins? What is the completeness and contamination of the bins and how is the overall quality? With this information how successful was the binning? Was the binning able to create MAGs (i.e. one genome per bin)? Which bins look particularly good? Once you are happy you can quit R. q() It is always useful to know the quality of your bins so you know which are more reliable than others. With that information you can be more or less certain when concluding your findings. "],["16-Assembly_functional_annotation.html", "Chapter 16 Assembly functional annotation 16.1 Taxonomic annotation 16.2 Prokka 16.3 MinPath", " Chapter 16 Assembly functional annotation 16.1 Taxonomic annotation Taxonomic annotation of bins can be carried out with Kraken2. As we have already done this for the reads and taxonomic results from read and assembly approaches have similar performances we will not cover it here, and instead move straight onto functional annotation with Prokka. 16.2 Prokka We will carry out Prokka functional annotation using MetaWRAP. MetaWRAP is a very useful program with many tools for metagenome analysis but we will only use it to carry out functional annotations for now. 16.2.1 Prokka: conda and directories Now to activate a MetaWRAP environment in a new terminal (right click on the main screen background, choose Applications -&gt; Shell -&gt; bash). The first step is to use chos 8 which is a custom command for our cluster. It will Change the Unix OS to version 8 as version 7 (default in our cluster) is too old for some of the programs. chos 8 . usemetawrap Make a new directory and move into it. mkdir ~/8-metawrap_anno cd ~/8-metawrap_anno 16.2.2 Prokka: run Now we can annotate the bins. The below will take a long time to run (&gt;1 hour). Instead of running it skip onto the next step to copy pre-made output to continue with. This command is here so you know what to run in your own future analyses. metawrap annotate_bins \\ -b ~/7-Binning/K1/bins \\ -o ./K1 \\ -t 8 The options used are: -b: The directory containing the assembled bins in fasta format. -o: The output directory. -t: Number of threads to use for the command. 16.2.3 Prokka: output Like in previous steps we will copy over premade results for the full dataset. Note: This need to be done in one of you non metawrap terminals #Copy over directory cp -r /pub39/tea/nsc006/NEOF/Shotgun_metagenomics/prokka/K1_fullset ~/8-metawrap_anno #Change into copied directory cd ~/8-metawrap_anno/K1_fullset There are many directories but the one we are interested is bin_funct_annotations . This contains a GFF file for each bin. Look at the gff file for bin 1. less bin_funct_annotations/K1.1.gff The GFF file is a tab delimited file contain annotation information for the features in the assembly/bin. In this case it is a GFF3 file (most curent version of GFF). There is quite a lot of information contained in each row so instead of listing all the columns here please have a look at the official documentation: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md A quick thing we can do with these files is to see if any of the bins contain a specific annotation. For example, if we wanted to know if there were any ATP-binding proteins in any of the bins we could carry out the below command. grep &quot;ATP-binding protein&quot; bin_funct_annotations/K1*gff | less We can now view the lines containing \"ATP-binding protein\" with the start of the line containing the file name the line belongs to. What if you want to know about pathways? 16.3 MinPath We can use MinPath to predict MetaCyc metabolic pathways. These pathway are made up of sets of enzymes that catalyse various reactions. 16.3.1 MinPath: conda and directory Use the MetaWRAP environment in chos 8 for this section. Ensure you start in the ~/8-metawrap_anno/K1_fullset directory. 16.3.2 MinPath: EC extraction Before we can estimate the pathways we need to extract the EC numbers predictions from the GFF file. EC (Enzyme Commission) numbers are a numerical classification for enzymes based on the reaction they catalyse. Unless you know the EC scheme well they are generally not helpful by themselves. An example EC number is EC 3.1.1.The numbers represents the group the enzyme belongs to with the first number being the biggest group. From highest to the lowest grouping 3.1.1. represents: 3: Hydrolase enzymes. 3.1: Hydrolase enzymes that act on ester bonds. 3.1.1: Carboxylic Ester Hydrolases. With all that information we will extract the EC annotations from the GFF files. First we'll create a directory for the output and move into the GFF directory. #Create EC directory mkdir EC #Change into GFF directory cd bin_funct_annotations/ We will now use a loop with various parts to create an EC annotation file (.ec). Note: The lack of \\ at the end of most of the ines is intentional. The below is all one command over multiple lines but loops work slightly different and don't need \\ in certain parts. Ensure you do press enter at the end of each line. ls -1 *gff | sed &quot;s/.gff//&quot; | while read bin do cat ${bin}.gff | grep &quot;eC_number=&quot; | cut -f 9 | cut -f 1,2 -d &#39;;&#39; | \\ sed &quot;s/ID=//g&quot; | sed &quot;s/;eC_number=/\\t/g&quot; &gt; ../EC/${bin}.ec done That is quite a bit of code so we'll break it down with examples for you to run. Of course feel free to skip over the explanations of code sections you are already comfortable with. The first part lists all our .gff files on one line each (ls -1 *gff). Then the suffix .gff is substituted with nothing sed \"s/.gff//\". This gives us the name of each bin (e.g. K1.1, K1.2). Run this section of code to see that. #List all gff files on one (-1) each ls -1 *gff #List all the file prefixes (on one line each) ls -1 *gff | sed &quot;s/.gff//&quot; Next we loop through each file prefix and use the variable bin (arbitrarily chosen) which contains the file prefix. This is carried out with while read bin. All the lines between the do (start of the loop) and done (end of loop) are line run in the loop. Run the loop with an echo command to show we are using the ${bin} variable to specify the input and output files. ls -1 *gff | sed &quot;s/.gff//&quot; | while read bin do echo &quot;${bin}.gff ../EC/${bin}.ec&quot; done Now to look at the command within the loop: cat ${bin}.gff | grep \"eC_number=\" | cut -f 9 | cut -f 1,2 -d ';' | \\ sed \"s/ID=//g\" | sed \"s/;eC_number=/\\t/g\" &gt; ../EC/${bin}.ec A good way to figure out what a pipe workflow is doing is to to run the first part of workflow and then add section by section to see how it is affecting the output. Additionally, it is always good to run it on one file and head the output so we have a manageable amount of data to look at. We'll do that with the K1.1.gff file. Note: Remember we are adding head to the end for ease of viewing. Tips: Use the up arrow to go back to previously run commands that you can then edit. Remember the clear command. #Grab every line containing &quot;ec_number=&quot; cat K1.1.gff | grep &quot;eC_number=&quot; | head #Cut out the 9th column/field (-f) (i.e. only keep the 9th column) #cut uses tabs as the default column/field delimiter cat K1.1.gff | grep &quot;eC_number=&quot; | cut -f 9 | head #Cut out the 1st and 2nd field based on the delimeter &quot;;&quot; (-d &quot;;&quot;) cat K1.1.gff | grep &quot;eC_number=&quot; | cut -f 9 | cut -f 1,2 -d &#39;;&#39; | \\ head #Substitute (s/) ID= for nothing (/ID//) #/g indicates to carry out it out for every instance in every line #Default is to only the first instance in each line cat K1.1.gff | grep &quot;eC_number=&quot; | cut -f 9 | cut -f 1,2 -d &#39;;&#39; | \\ sed &quot;s/ID=//g&quot; | head #Substitute ;eC_number= for a tab (\\t) cat K1.1.gff | grep &quot;eC_number=&quot; | cut -f 9 | cut -f 1,2 -d &#39;;&#39; | \\ sed &quot;s/ID=//g&quot; | sed &quot;s/;eC_number=/\\t/g&quot; | head Finally the pipes output is written into a file: &gt; ../EC/${bin}.ec. The output .ec files are required for MinPath and are tab delimited with two columns: protein/sequence id annotation (in this case the EC number) 16.3.3 MinPath: run With our .ec files we can create our MetaCyc predictions. First we'll change directory into the EC directory. Then create an output directory for the MetaCyc predictions. cd ../EC mkdir ../MetaCyc Now we can loop through the file suffixes to run MinPath. ls -1 *ec | sed &quot;s/.ec//&quot; | while read bin do python /pub39/tea/nsc006/programs/MinPath/MinPath.py \\ -ec ${bin}.ec \\ -report ../MetaCyc/${bin}.minpath done A lot of output will be printed to screen but this can be ignored unless you see warnings. 16.3.4 MinPath: output First step is to change directory into the MetaCyc directory. #Change directory cd ../MetaCyc #List contents ls From the CheckM results we found that bin K1.22 was very good with a quality score &gt;96%. We will therefore have a look at its output. Have a look at the .minpath -report file for K1.22. less K1.22.minpath The file contains the following columns Pathway ID Pathway reconstruction: Only available for genomes annotated in MetaCyc database. Naive: Indicates if pathway was reconstructed by the naive mapping approach (1) or not (0). Minpath: Indicates if the pathway was kept (1) or removed (0) by MinPath. Fam0: The number of families involved in the pathway. Fam-found: Number of families in pathway that were annotated/found. Name: Description of pathway. Quit (q) less when you are happy. There are some quick things we can do in bash with these files. #Count number of pathways found in each bin with word count wc -l *minpath #Grab every line with &quot;PWY-6972&quot; from every file grep &quot;PWY-6972&quot; *minpath | less With this file you can then investigate what bins have which pathways. Additionally, with more samples analysed you can determine which samples have which pathways present. "],["17-Appendix.html", "A Next steps B Manuals C Obtaining Read Data", " A Next steps Below are some good links to start with before carrying out your own projects. A review paper on metagenome assembly approaches Genome-resolved metagenomics using environmental and clinical samples: https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbab030/6184411 bioBakery tools for meta'omic profiling https://github.com/biobakery/biobakery Assessing the performance of different approaches for functional and taxonomic annotation of metagenomes https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6289-6 MicrobeAnnotator: Easy-to-use pipeline for the comprehensive metabolic annotation of microbial genomes. https://github.com/cruizperez/MicrobeAnnotator MetaEuk: Functional annotation of eukaryotic metagenome https://github.com/soedinglab/metaeuk _ Methods for Metagenomic data visualisation and analysis https://www.researchgate.net/publication/318252633_Methods_for_The_Metagenomic_Data_Visualization_and_Analysis B Manuals Conda: https://conda.io/projects/conda/en/latest/user-guide/getting-started.html FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ MultiQC: https://multiqc.info/ Trim Galore: https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/ Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml samtools: http://www.htslib.org/ BBTools: https://jgi.doe.gov/data-and-tools/bbtools/ Kraken2: https://github.com/DerrickWood/kraken2/wiki/Manual Krona: https://github.com/marbl/Krona/wiki/KronaTools Bracken: https://ccb.jhu.edu/software/bracken/index.shtml?t=manual LEfSe: https://huttenhower.sph.harvard.edu/lefse/ HUMAnN 3.0: https://huttenhower.sph.harvard.edu/humann/ MetaPhlAn 3.0: https://huttenhower.sph.harvard.edu/metaphlan/ Biobakery: https://github.com/biobakery/biobakery hclust2: https://github.com/SegataLab/hclust2 MegaHit: https://github.com/voutcn/megahit BWA: https://github.com/lh3/bwa minimap2: https://github.com/lh3/minimap2 MetaBAT2: https://bitbucket.org/berkeleylab/metabat/src/master/ CheckM: https://github.com/Ecogenomics/CheckM/wiki PhyloPhlAn: https://github.com/biobakery/phylophlan/wiki Prokka: https://github.com/tseemann/prokka MinPath: https://github.com/mgtools/MinPath/blob/master/readme MetaCyc: https://metacyc.org/ C Obtaining Read Data The following commands can be used to obtain the sequence data used in this practical, directly from the EBI metagenomics site. It is worth noting that these are the full set of data, not like the miniaturised version you have used in the tutorial. wget -O K1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_1.fastq.gz wget -O K1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505102/ERR505102_2.fastq.gz wget -O K2_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_1.fastq.gz wget -O K2_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505104/ERR505104_2.fastq.gz wget -O W1_R1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_1.fastq.gz wget -O W1_R2.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR505/ERR505090/ERR505090_2.fastq.gz "]]
