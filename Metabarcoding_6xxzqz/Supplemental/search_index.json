[["01-Metabarcoding_supplemental.html", "Metabarcoding for diet analysis and environmental DNA - supplemental material Chapter 1 Introduction Table of contents", " Metabarcoding for diet analysis and environmental DNA - supplemental material Katy Maher, Helen Hipperson and Ewan Harney 2025-08-08 Chapter 1 Introduction This bookdown is supplemental to the main one. It contains some additional code and examples of tools that may prove useful in addition to the core DADA2 pipeline. Ensure you have the metabarcoding environment activated for using all of these materials. As a reminder, the command to do this (if you are not still in the R session) is: . usemetabarcoding Table of contents Normalising sequence data Exporting data Classifying ASVs with GenBank and BLAST Assigning taxa with MEGAN Further blast and megan approaches Summary This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Normalisation.html", "Chapter 2 Normalising your data 2.1 Relative abundance 2.2 Rarefaction 2.3 Variance stabilising transformation using DESeq2", " Chapter 2 Normalising your data As mentioned in the main tutorial document there are several different methods for normalising your data for downstream analysis. Here we will look at three different methods of normalising your data for downstream analysis. To summarise so you have several methods in one place, we will also include the normalisation method that we used in the main tutorial. Remember these are just a few examples of how to normalise your data and we recommend you read the recent literature and think about the properties of your own dataset when deciding the best way to proceed. 2.1 Relative abundance The transform_sample_counts function transforms the sample counts from a taxa abundance matrix using a user-provided function. The counts of each sample will be transformed individually. In this case we will transform the raw samples into relative abundances by dividing individual ASV counts by the total ASV counts in a sample. phylo.prop &lt;- transform_sample_counts(phylo, function(otu) otu/sum(otu)) 2.2 Rarefaction Rarefaction involves randomly removing reads until you reach a number often equal to or less than the number of reads in the smallest sample. Here we rarefy to an even depth using the minimum number of reads found in a sample as specified by the sample.size=min(sample_sums(phylo)) argument. The argument rngseed = 1 sets a random number seed used for random number generation, which is then used when subsampling a random number of reads from each sample. By setting the random seed you can make this process reproducible. We have set the argument replace as replace = F. From the phylo.rarefied R help page \"Two implications to consider are that (1) sampling with replacement is faster and more memory efficient as currently implemented; and (2), sampling with replacement means that there is a chance that the number of reads for a given OTU in a given sample could be larger than the original count value, as opposed to sampling without replacement where the original count value is the maximum possible.\" phylo.rarefied &lt;- rarefy_even_depth(phylo, rngseed = 1, sample.size=min(sample_sums(phylo)), replace = F) 2.3 Variance stabilising transformation using DESeq2 Variance stabilising transformation borrows methods from transcriptomic research using the DESeq2 package. The workflow below is based on the workflow for variance stabilised data suggested here. First we load the DESeq2 library. library(DESeq2) Now transform the phyloseq object into a format appropriate for DESeq2. ds &lt;- phyloseq_to_deseq2(phylo, ~SITE) The function estimateSizeFactors is used to estimate the size factors for a DESeqDataSet - for more information on the normalisation methods used by DESeq2 please read the DESeq2 paper and manual. ds&lt;-estimateSizeFactors(ds) For our dataset we get the error message \"Error in estimateSizeFactorsForMatrix(counts(object), locfunc = locfunc, : every gene contains at least one zero, cannot compute log geometric means\" This indicates we have many zeros in our count data (which is common in amplicon studies). There are several ways to deal with this problem. One way is to calculate size factors separately using a zero-tolerant variant of geometric mean. For more details of this method see here. # calculate geometric means prior to estimation of size factors gm_mean &lt;- function(x, na.rm=TRUE){ exp(sum(log(x[x &gt; 0]), na.rm=na.rm) / length(x)) } geoMeans &lt;- apply(counts(ds), 1, gm_mean) We can now proceed and calculate size factors and dispersions of our dataset, before calculating the variance stabilizing transformation and transforming the count data with getVarianceStabilizedData. ds &lt;- estimateSizeFactors(ds, geoMeans = geoMeans) ds &lt;- estimateDispersions(ds) ds_vst&lt;-getVarianceStabilizedData(ds) We then convert the transformed dataset back into a phyloseq object for further analysis. ds_count_phylo &lt;- otu_table(ds_vst, taxa_are_rows=T) sample_info_tab_phy &lt;- sample_data(meta.rmlow) ds_phylo &lt;- phyloseq(ds_count_phylo, sample_info_tab_phy) This is just one way to deal with the problem of zero counts in our dataset. For more discussion on using DESeq2 for metabarcoding data and dealing with zero count data please check out the following links: phyloseq FAQ DESeq2 with phyloseq "],["03-Exporting_data.html", "Chapter 3 Exporting data 3.1 Fasta file 3.2 Sequence count matrix 3.3 Table of taxon names", " Chapter 3 Exporting data It is useful to be able to export our ASV sequences and a matrix table of counts from DADA2/R as we might want to visualise or analyse these using other software. Our ASV sequences and counts per sample are stored in the object seqtab.nochim. The ASVs are not named, so first let's name them (ASV_1, ASV_2, etc.). # The column names of seqtab.nochim are actually the ASV sequences, # so extract these and assign them to `mifish_seqs` mifish_seqs &lt;- colnames(seqtab.nochim) # Make a new variable for ASV names, `mifish_headers`, #with length equal to the number of ASVs mifish_headers &lt;- vector(dim(seqtab.nochim)[2], mode=&quot;character&quot;) # Fill the vector with names formatted for a fasta header (&gt;ASV_1, &gt;ASV_2, etc.) for (i in 1:dim(seqtab.nochim)[2]) { mifish_headers[i] &lt;- paste(&quot;&gt;ASV&quot;, i, sep=&quot;_&quot;) } 3.1 Fasta file Now we have our sequences and names as variables we can join them and make a fasta file. mifish_fasta &lt;- c(rbind(mifish_headers, mifish_seqs)) write(mifish_fasta, &quot;MiFish_ASVs.fa&quot;) You should now have this fasta file in your working directory on the server. 3.2 Sequence count matrix Next make a table of sequence counts for each sample and ASV. # First transpose the `seqtab.nochim` and assign this to the variable `mifish_tab` mifish_tab &lt;- t(seqtab.nochim) # Name each row with the ASV name, omitting the &#39;&gt;&#39; used in the fasta file row.names(mifish_tab) &lt;- sub(&quot;&gt;&quot;, &quot;&quot;, mifish_headers) write.table(mifish_tab, &quot;MiFish_ASV_counts.tsv&quot;, sep=&quot;\\t&quot;, quote=F, col.names=NA) You should now have an ASV by sample matrix with sequence counts in a tab-separated value (tsv) file in your working directory. 3.3 Table of taxon names Lastly, if we've used dada2 to assign taxonomy we can make a table of taxon names for each ASV. # Replace the row names in `taxa` with the ASV names, # omitting the &#39;&gt;&#39; used for the fasta file. rownames(taxa) &lt;- gsub(pattern=&quot;&gt;&quot;, replacement=&quot;&quot;, x=mifish_headers) write.table(taxa, &quot;MiFish_ASV_taxonomy.tsv&quot;, sep = &quot;\\t&quot;, quote=F, col.names=NA) You should now have a tsv file of taxonomic assignments for each ASV in your working directory. "],["04-BLAST.html", "Chapter 4 GenBank and BLAST 4.1 BLAST againt the nt database 4.2 BLAST against a custom database", " Chapter 4 GenBank and BLAST GenBank and BLAST are both hosted by the NCBI (Nationl Center for Biotechnology Information). GenBank is an annotated database of all publicly available DNA sequences, more than 200 million. BLAST is a program that compares sequences to those in databases such as GenBank to find regions of similarity. Therefore we can use BLAST to find sequences similar to our ASVs in GenBank and use that to infer the taxonomy in our dataset. This is not a fool-proof way to know exactly what was in our samples, however, as there are mis-annotated records in GenBank, which can occur if an organism is misidentified before being sequenced, or if the sequence was contaminated or erroneous. Although GenBank holds millions of sequences it is not comprehensive, and it might be the case that an ASV in our sample is simply not represented in the database. It is possible to use BLAST through the web interface either pasting in sequences one by one or uploading a fasta file. However, with a few hundred ASVs from a typical metabarcoding project it is more efficient to use a downloaded copy of the GenBank nucleotide (nt) database and a standalone command line version of the BLAST software. See here for information on downloading BLAST software and databases. Many institutions will have servers with the BLAST software and databases communally available as this saves storage space compared to many users having their own copies, so it is worth asking if this is available where you are based. As the BLAST databases are large and the searches can take a while to run the commands below are not intended to be run on the web VNC, but are provided in case they are useful for you for your own data. BLAST is used on the linux command line, rather than in R as we have been working in all the earlier steps of the workbook. 4.1 BLAST againt the nt database To perform a blastn search of our MiFish ASVs against the nt database the command would be: blastn -query MiFish_ASVs.fa -task blastn -db nt \\ -evalue 0.001 -outfmt 6 -out MiFish_ASVs_blast_out.txt The options specified are: -query filename.fa fasta file of ASV sequences -task blastn perform a blastn search -db nt search against the nucleotide (nt) database -evalue 0.001 threshold E-value for returning BLAST hits -outfmt 6 generate the results in a tabular format The text file produced will be a table of BLAST hits for all ASVs where a match could be found within the threshold E-value. The table will have 12 columns including the ASV name, the database sequence match name, the percent identity of the match, the length of the alignment, the E-value and the bit-score. These last two values are measures of how well our sequences match, with a smaller E-value and a larger bit-score indicating better matches. Here's what the first few lines of the output file looks like: We have many hits for ASV_1. The first two hits have 100% sequence identity over the whole length (167 bp) of our amplicon. The other hits shown have 1 mismatch giving them a 99.4% sequence identity. How do we decide which match is best given the high similarity to many sequences? We could simply take the top hit for each ASV, but an alternative way is to use a 'lowest common ancestor' algorithm to assign taxonomy. This is implemented in the program taxonomizr. 4.2 BLAST against a custom database If you have your own fasta file of curated reference sequences you can also use BLAST to match your ASVs to this. First make a BLAST database from the fasta file: makeblastdb -in My_12S_Fish_sequences.fa \\ -parse_seqids \\ -dbtype nucl \\ -out My12S This will generate all of the database files needed to run blastn against the sequences in 'My_12S_Fish_sequences.fa'. To do this the command would be: blastn -query MiFish_ASVs.fa -task blastn \\ -db My12S -evalue 0.001 -outfmt 6 \\ -out MiFish_ASVs_blast_My12S_out.txt "],["05-MEGAN.html", "Chapter 5 Taxonomizr 5.1 Filtering BLAST results 5.2 Importing BLAST results into R and taxonomizr", " Chapter 5 Taxonomizr Taxonomizr is an R package which can be used to parse BLAST result files in order to assign taxonomy to NCBI accession numbers or taxonomic IDs. It has the ability find agreement between the taxonomic hits using a Lowest Common Ancestor (LCA) algorithm. The LCA considers all significant BLAST hits and, if these are to reference sequences from multiple species, assigns the taxonomy of the node that lies above all of those species. For example, if an ASV matched equally well to reference sequences of Neolamprologus longior and Neolamprologus gracilis it would be assigned to the genus level as Neolamprologus sp. Similarly, if an ASV matched equally well to reference sequences of Neolamprologus longior and Ophthalmotilapia ventralis it would be assigned at the family level to Cichlidae as both of these species are within that family. Taxonomizr includes a function for downloading the latest taxonomy data from NCBI and uses this to match NCBI accession numbers (the second column in our blast output table) to taxonomic classes. As these files are large this is not intended to be run on the web VNC, but the workflow is provided in case it is useful for your own data. 5.1 Filtering BLAST results We already restricted the BLAST hits in our output file by specifying a maximum E-value threshold. It is also useful to further filter out sequences with a low percentage identity or a short alignment length as these are likely to be spurious matches to our ASVs. We can do this easily on the linux command line using an awk command (awk is a programming language useful for text and file processing). Let's filter to keep only hits with a percent identity of at least 90% and an alignment length of at least 100 bp. awk &#39;$3 &gt;= 90&#39; MiFish_ASVs_blast_out.txt | \\ awk &#39;$4 &gt;= 100&#39; &gt; MiFish_ASVs_blast_out_filtered.txt 5.2 Importing BLAST results into R and taxonomizr First we need to download the taxonomizr accession file (remember do not run this on the web VNC this is just for your own analysis). # load the taxonomizr library library(taxonomizr) # download the taxonomizr accession file prepareDatabase(&#39;accessionTaxa.sql&#39;) This command will check to see if you have the accession file already downloaded, it will download it if it does not find the file. You should only need to run this once when running your analysis. If you wanted to update the accession file, if for example you are analysing a new set of data some months later then you will first have to delete/rename the old file and run this command again. We now need to read in our blast file into R. # Read in the filtered blast result file blastResults&lt;-read.table(file = &quot;MiFish_ASVs_blast_out_filtered.txt&quot;, header=FALSE, stringsAsFactors=FALSE) We can now optionally choose to filter the blast results to keep only the top user specified percent of blast hits (suggested values: 1-10). # Filter file by top percent value of bit score as specified in topperc (e.g. 2%) # Note that the top percent value is converted to a fraction and subtracted from 1: # thus a top percent value of 2 becomes 0.98 topperc&lt;-2 topPercent &lt;- data.frame(blastResults %&gt;% group_by(V1) %&gt;% arrange(V1, desc(V12)) %&gt;% filter(V12 &gt; max(V12*(1 - (topperc/100))))) We will then extract the ASVs and accession number columns from our top percent filtered blast results. # Split the top percent values in to ASVs and accessions asvs&lt;-topPercent[,1] accessions&lt;-topPercent[,2] We now have everything we need to run our taxonomizr taxonomy assignment. First we assign taxonomic IDs to our NCBI accession numbers using the accessionToTaxa function. We can then use the function getTaxonomy to appoint the taxonomy for those IDs. # Get the taxonomic information for top percent accessions from accessionTaxa.sql taxaId&lt;-accessionToTaxa(accessions,&quot;./../accessionTaxa.sql&quot;) taxa&lt;-getTaxonomy(taxaId,&#39;./../accessionTaxa.sql&#39;, desiredTaxa=c(&quot;kingdom&quot;, &quot;phylum&quot;, &quot;class&quot;, &quot;order&quot;, &quot;family&quot;, &quot;genus&quot;, &quot;species&quot;)) To generate an agreement among our taxonomy assignments to each ASV we can use the function condenseTaxa. This will collapse the taxonomy assignments to the lowest taxonomic rank shared by all the blast hits. # LCA algorithm and merge back to top percent ASVs taxon_path &lt;- condenseTaxa(taxa,asvs) Finally we can write our taxonomic information for each ASV to an output file. # Write output write.table(&quot;taxonomizr_taxon_path.tsv&quot;, sep=&quot;\\t&quot;, quote = FALSE) "],["06-blast2megan.html", "Chapter 6 More BLAST and taxonomizr 6.1 Introduction to blast2taxonomizr 6.2 Running blast in array mode 6.3 Taxonomizr 6.4 Combining output with DADA2 output", " Chapter 6 More BLAST and taxonomizr 6.1 Introduction to blast2taxonomizr We may want to scale up the approaches described in chapters 04 and 05 to allow us to process larger data sets more rapidly. In this chapter we explain two ways you could do this: Using Slurm job arrays to simultaneous blast a split fasta file Running taxonomizr and formatting the output to provide LCA information We run through some hypothetical examples and draw your attention to some of the key bits of code. This workflow is based on an automated command line workflow called blast2taxonomizr which is designed for use on a specific HPC (the University of Sheffield's Bessemer cluster), but the scripts associated with blast2taxonomizr can be easily modified for use on your own cluster. These steps are not intended to be run on the web VNC, but the workflow is provided in case it is useful for your own data. 6.2 Running blast in array mode NCBI nt is an enormous database containing tens of millions of sequences. If you have a long list of ASVs, 'blasting' hundreds or thousands of sequences against nt can take many hours or even days. To speed things up we can use the Slurm workload manager (used by many HPCs) to split blast into an array job and run several blast searches simultaneously. Many HPCs provide their own information about how to use slurm effectively (see for example the University of Sheffield's HPC documentation on advanced job submission with slurm). Below we outline the main steps required to use a slurm job array to process a large number of sequences: 6.2.1 Split the input file Say we have an fasta file 'asv.fasta' with 1000 sequences, and we want to split it into 10 smaller files (with 100 sequences each) that can be run simultaneously. We can use awk to split the file into chunks like so: ### use awk to split a fasta file awk &#39;BEGIN {n=0;} /^&gt;/ {if(n%100==0){file=sprintf(&quot;chunk%d.fa&quot;,n);} print &gt; file; n++; next;} { print &gt;&gt; file; }&#39; &lt; asv.fasta You don't need to fully understand how this is splitting the file, but it's worth noting a couple of things: /^&gt;/ : the characters between two forward slashes are what we are tell awk to look for (what is known as a regular expression) - in this case a '&gt;' at the start of a line (used by fasta files to denote the sequence name and description), if(n%100==0) : the '100' is how many sequences will be included in each chunk (you can set it to any number), file=sprintf(\"chunk%d.fa\",n) : this is how we define how the output files will be called. in the example it is set to 'chunk%d', where '%d' is the line number of the original file (starting at 0). So our chunk files will be called 'chunk0.fa', 'chunk100.fa' all the way up to 'chunk900.fa', 6.2.2 Make a list of the subfiles Now we will make a text file containing a list of all the chunks we want to run on the job array. This is easily achieved with the ls command. It's also worth double checking how many chunk files there are in the list, as we will need that information in the following steps: ### use ls to make a list of subfiles ls chunk*fa &gt; list_of_chunks.txt ### count the number of lines in the file with wc -l (word count - line) wc -l list_of_chunks.txt 6.2.3 Making the array script Now we have split our fasta file up, we can move on to the array. First, we will have to make a simple bash script that reads in the information from the 'list_of_chunks.txt' file, and uses that as an input for blast. Below are the main commands you would need to include: # create a variable FASTA corresponding to the information on a numbered line of list_of_chunks.txt FASTA=$(sed &quot;${SLURM_ARRAY_TASK_ID}q;d&quot; &lt; (cat list_of_chunks.txt)) # Run blastn using the newly defined FASTA variable. Make sure the path to the nt database is correct! blastn -query ${FASTA} \\ -task blastn \\ -db /path/to/ncbi/nt \\ -out ${FASTA}_blast.out.tab \\ -outfmt 6 When the job array is submitted this script will be run as many times as specified (it will be 10 times in our example). For each numbered array job (1 to 10): FASTA=$(...): a new variable called 'FASTA' is created based on some information we will provide sed \"${SLURM_ARRAY_TASK_ID}q;d\": it is equal to the information on a single line (whose number is determine by which array job is running) &lt; (cat list_of_chunks.txt): this file is the source of the information Thus for array job 1 FASTA takes the value of line 1 in 'list_of_chunks.txt' (line 1 reads 'chunk0.fa'), while in array job 2 FASTA takes the value of line 2 in 'list_of_chunks.txt' (chunk100.fa). For each job in the array, the unique value of variable ${FASTA} is then used when calling blastn. If you plan to use the command line version of MEGAN, we recommend altering the -outfmt argument slightly (see section 6.3.1 Preparing Blast for MEGAN for more details). When making a script remember that you will have to include the shebang and set various other sbatch options. If blast is available through a conda environment you will also have to load this as well. Once this is done, you can save the script. We could call it something like 'blast_array.sh'. Depending on how permissions are applied in your HPC you may have to make the script executable using the chmod command (see this explainer on chmod). A script file can be made executable be running: # make a file executable chmod u+x blast_array.sh 6.2.4 Running the array script Finally we can submit the job as an array to slurm. Notice that we have to specify how many jobs our array is composed of. This must be equal to the number of lines in 'list_of_chunks.txt'. # submit script as an array job to slurm sbatch --array=1-10 blast_array.sh Once the job has finished, you should have 10 'chunk%d.fa_blast.out.tab' files containing the blast results. The final step is to concatenate these files together with the cat command: # concatenate file together cat chunk*.fa_blast.out.tab &gt; all_blast.out.tab 6.2.5 Arrays in blast2taxonomizr The blast array in blast2taxonomizr makes use of two scripts, a preparatory script that splits the input and makes a list of files (01A_run_prep_for_blast.sh), and then the array itself (01B_run_blastn_array.sh). These scripts do a few additional things compared with the steps listed above, such as making sub-directories to store the 'chunk%d.fa' input files and 'chunk*.fa_blast.out.tab' output files. Please take a look at the readme and have a look inside the scripts themselves for more information. 6.3 Taxonomizr In Chapter 5 we provided information about running taxonomizr in R, but it is also possible to install a command line version of MEGAN on your cluster and automate the procedure of finding the Lowest Common Ancestor (LCA). Please speak to your HPC administrator for advice on installing new software. 6.3.1 Preparing Blast for taxonomizr Taxonomizr requires a blast tab delimited file as input (-outfmt 6). However, it also expects the ncbi subject sequence id (column 2 in the blast tab file) to be a simple accession number (e.g. 'MT539258.1'), yet by default, the output will contain additional gene identifier information (e.g. 'gi|1846592020|gb|MT539258.1|'). Although this information can be filtered out afterwards (e.g. using awk), you can also amend the -outfmt argument so that only the accession is included: # Run blastn with a custom list of column headers blastn -query ${FASTA} \\ -task blastn \\ -db /path/to/ncbi/nt \\ -out ${FASTA}_blast.out.tab \\ -outfmt &quot;6 qseqid saccver pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen staxid ssciname scomnames sblastname sskingdoms stitle&quot; In this example of blast we specify exactly which column headers to include. Columns 1 and 3-12 are the default values, but we have used saccver (subject accession version) rather than sseqid (subject sequence ID) in column 2 to be compatible with taxonomizr. This handy Guide to blastn output format 6 has more information about the default and optional columns in blast output format 6. It is possible to specify additional columns such as sscinames and sskingdoms which will provide the scientific names and the kingdom of the ncbi subject sequence (as long as the ncbi taxa database files 'taxdb.btd' and 'taxdb.bti' are present in the current working directory). This taxonomic information can provide a useful sanity check when looking at blast results. 6.3.2 Running taxonomizr Once we have a filtered blast output file prepared, we can use taxonomizr (as in the previous chapter). In this case we use a bash script to prepare and/or filter our input file before passing the prepared file to the taxonomizr R script. The script expects either the blast.out.tab created using the command above or a series of 'chunk*.fa_blast.out.tab' output files as created in the blast arrary method. In the case that the array script was used, these will be combined using 'cat'. cat chunk* &gt; all_blast.out.tab As explained in Chapter 5.1, it is also useful to filter blast results (using awk) for a minimum percentage identity and a minimum length before running taxonomizr. It is also recommended that a minimum percentage identity threshold and a top percent value is used for the taxonomizr LCA algorithm to improve taxonomic assignment. If there are any blast hits you want to exclude from your blast results you can do this. For example to remove any 'uncultured eukaryote' samples: grep -v &quot;uncultured eukaryote&quot; all_blast.out.tab &gt; filtered_blast.out.tab After you have filtered your blast output file as desired, you can run your taxonomizr R script from within your bash script, e.g.: Rscript 02_taxonomizr_lca.R As explained in Section 6.2.3 Making the array script, you must include certain information in your bash script like the the shebang, and it is useful to provide other sbatch options. You may have to make the script executable with chmod. Once your script is saved and executable, run it like so: # Run a script sbatch my_script.sh 6.4 Combining output with DADA2 output The final bash script in the blast2megan pipeline is '03_run_make_summary_files.sh'. This script is simply a wrapper for an R script called '03_make_summary_files.R' that combines taxonomic assignment results from blast2taxonomizr with sequence data and ASV counts from DADA2 to create several summary files, including: ASV_taxa_seq_counts.tsv: a complete summary of taxonomic results (LCA taxon, taxon rank, and taxon path), sequence, and count results, ps_taxamat.tsv : ASV taxonomic results in matrix format for phyloseq, ps_countmat.tsv : ASV counts results in matrix format for phyloseq, ps_phylogeny.rds : phylogenetic tree for phyloseq prepared according to protocol of Callahan et al. 2016, see subsection Construct the phylogenetic tree. The second, third and fourth files are designed to be inputs for the popular community analysis R package phyloseq. If you are interested in understanding how it works, take a look at the R code in 03_make_summary_files.R. Among other things the script uses the R function merge to join together different data tables based on shared information (either the ASV sequence or the ASV number) and assigns column headers and rownames to the merged data (turning data tables into matrices). Notice that the script requires 'taxonomizr_summary_out.tsv' and 'taxonomizr_taxon_path_us.tsv' (the output from the previous step, which it expects to find in the 'blast_out' directory) as well as the DADA2 output files '06_ASV_seqs.fasta' and '06_ASV_counts.tsv' (which it expects to find in the 'working_data' directory). "],["07-Summary.html", "Chapter 7 Summary", " Chapter 7 Summary Do the results you obtained with this pipeline make sense? In a real scenario, you (as the owner of the dataset) are the one with the deepest knowledge of the data you are analysing, so it is upon you to answer this question. This answer needs to be inferred by gathering all the information you can on the samples. If the answer is ‘no,’ you can consider the following points. Are there any missing taxa you know should be there? These taxa may be present but not being detected for a few reasons, first of all, are these represented in the taxonomic database you are using? If no, you should try a different approach. If they are, you can try with different taxonomic assignment methods. If this still does not work, you can change the method to pick up the representative sequence and/or the clustering at the earlier stages of the analysis. The failure could be right at the beginning of the project design. It is worth checking if the PCR primers you are using can amplify these species. Are the primers known to impose bias on the mix of species you are amplifying? Are the primers unable to amplify some taxon? The literature may be able to help you on this topic. We hope that this workflow will prove useful to you when you analyse your own research data. We have covered some of the key methods and considerations for analysing metabarcoding data. However, every project is different and you still need to think about what the most appropriate way might be for analysing your particular data. For a full and complete understanding, you should also read the original publication associated with each program. "]]
