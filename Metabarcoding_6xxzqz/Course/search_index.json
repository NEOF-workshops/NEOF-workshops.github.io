[["01-Metabarcoding.html", "Metabarcoding for diet analysis and environmental DNA Chapter 1 Introduction Table of contents", " Metabarcoding for diet analysis and environmental DNA Katy Maher and Helen Hipperson 2025-09-24 Chapter 1 Introduction Sequencing DNA barcodes from mixed sources of DNA is an increasingly used way to survey biodiversity, whether analysing dietary content from faecal-derived DNA or monitoring aquatic species from water-derived DNA. This course will give an overview of metabarcoding with different barcoding genes to target particular taxa. Using an example data set, we will go from raw sequence data through to assigning taxonomy to the sequence variants and some examples of downstream community analysis. Sessions will start with a brief presentation followed by self-paced computer practicals guided by an online interactive book. The book will contain theory and practice code. At the end of the course learners will be able to: Utilise and understand the use of dada2 with metabarcoding data. Import and quality assess sequences and generate ASVs with dada2. Assign taxonomies to ASVs. Produce alpha and beta diversity metrics &amp; plots. There are supplemental materials including normalisation, exporting data, classifying ASVs with GenBank and BLAST, and assigning taxa with Taxonomizr. These can be run through after the course by the learner. Table of contents Background Introduction to R and DADA2 Cluster intro Loading your data into R and dada2 Primer removal Checking the quality of your data Cleaning your data Identifying ASVs Assigning taxonomy Further analysis Running the pipeline with a different marker Guide for different marker This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Example_dataset.html", "Chapter 2 Background", " Chapter 2 Background Advances in sequencing technology mean that it is now feasible to characterise eDNA and diet content through DNA-based approaches by simultaneously sequencing short standardised DNA sequences (DNA barcodes) from a variety of environmental samples, a process known as DNA metabarcoding. The dataset we will use today to work through our metabarcoding analysis is from a study by Doble et al. 2019 on tropical fish communities from Lake Tanganyika. The study's authors collected water eDNA samples from sites along the shore of Lake Tanganyika in Tanzania. Samples were then filtered, extracted and made into individual libraries using a two-step PCR method to attach indexes and Illumina adapters. Four different primer sets were used to assess the community structure of fishes in Lake Tanganyika. We will examine the two shorter primer sets today: MiFish‐U (Miya et al. 2015) these primers target a hypervariable region of the 12S rRNA gene. 12S‐V5 (Riaz et al. 2011) these primers target a different part of the 12S region. Both these primer sets target sections of the mitochondrial 12S rRNA gene. The 12S gene occupies 1/16th of the mitochondrial genome. It is the mitochondrial homologue of the prokaryotic 16S and eukaryotic nuclear 18S ribosomal RNAs. Both genes were sequenced using a single Illumina MiSeq 300 cycle sequencing run (2x150bp paired end sequencing). The subset of samples you will analyse today were sampled in triplicate from sites along the shore of Lake Tanganyika in 2017. Also included is a filter control, an extraction control and 5 aquarium samples. The aquarium samples contained five cichlid species endemic to the lake. "],["03-R_DADA2_intro.html", "Chapter 3 Introduction to DADA2 and R 3.1 DADA2 3.2 R", " Chapter 3 Introduction to DADA2 and R 3.1 DADA2 DADA2 is an open-source software package for modeling and correcting Illumina-sequenced amplicon errors. It has several advantages over other amplicon pipelines: Resolution: DADA2 infers amplicon sequence variants (ASVs) from amplicon data rather than operational taxonomic units (OTUs) clustering based methods (we will discuss what these are and how they differ below). Accuracy: DADA2 reports fewer false positive sequence variants than other methods. It incorporates quality information, quantitative abundances, sequence differences and parameterises its error model using the data provided. Comparability: The ASVs output by DADA2 can be directly compared between studies, without the need to reprocess the pooled data. Computational Scaling: The compute time and memory requirements are lower than some other methods. 3.1.1 OTUs vs ASVs OTUs (Operational Taxonomic Units) are generated by clustering reads into groups based on similarity. A threshold, such as 97% similarity, is used to cluster sequences together often with the hope that all those sequences will belong to the same taxonomic group. Because of this clustering step different datasets cannot be compared. Commonly used software that are used for clustering OTUs include: UCLUST VSEARCH CD-Hit SWARM ASVs (amplicon sequence variants) are generated using the following methods. First the number of times an exact sequence has been sequenced is calculated. This is then combined with an error model unique for the sequencing run to determine the probability that a particular read is not found in the dataset due to a sequencing error. Reads can then be filtered out according to this calculation with the aim to remove sequencing errors and leave only true sequences. ASVs are able to be compared across studies using the same amplicon sequence as exact sequences are used rather than the clusters generated by OTU analysis. Here we will use the DADA2 software to analyse our data, using an ASV approach. This is a popular and well-established tool for this approach, although other pipelines/softwares are available that may run on different platforms or have particular advantages for certain studies or users. See Hakimzadeh et al. 2023 for a comprehensive review of metabarcoding analysis pipelines. 3.1.2 Workflow The workflow we will be following is summarised below: Paired end data is loaded into the R environment Primers are removed using the software cutadapt The data is then quality examined by plotting the error quality profiles using DADA2 Filter and trim the files to remove poor quality sequences Generate an error model, dereplicate reads and infer ASVs from the clean data Merge the paired end reads Make an ASV matrix Remove potential chimeric reads from the dataset Assign taxonomy Multivariate analysis 3.2 R R is an open-source free language and environment which was designed for statistical computing and graphics. It is possible to install R on many UNIX platforms, Windows and MacOS. In addition to its base environment, that contains capabilities for data manipulation, calculation and graphical display, there are many packages (including DADA2) that can be installed and contain additional functions, data and code for analysing, manipulating and plotting different types of data. You do not need to understand all the code you will be running today but a basic foundation in R can be useful when you come to analysing your own data and running further analysis. Here are some useful links to get you started if you want to learn more about R after this course. Tutorial: https://swirlstats.com/ R Website: https://www.r-project.org/ Cran Website: https://cran.r-project.org/ Rstudio Website: https://www.rstudio.com/ If you are not familiar with R we have decribed some of the common terminology and put a few hints to help you understand how to run a few basic R commands below. Feel free to skip this section and move onto the next chapter if you have used R before and are comfortable with the basics. 3.2.1 The console window The console window is the place where the user can type R commands to submit and then view the results, just like the terminal window in linux. R shows it is ready to accept commands by showing a &gt; prompt. You can then submit a command using the enter button. If you haven't completed a command (for example you forget to close a set of parentheses) the + symbol will show on a new line until you finish the command. 3.2.2 Your working directory Your working directory is the folder you are currently working on on your computer. If you set this when you read in or write an output to a file R will automatically look for and save files in this directory. You can set your working directory using: setwd(&quot;/path/to/working_directory/&quot;) 3.2.3 Packages Packages contain additional functions, data and code for analysing, manipulating and plotting different types of data. Many common packages will be installed as default when you install R. Other more specialised packages, such as the DADA2 package, must be installed by the user. Packages found on The Comprehensive R Archive Network (CRAN), which is R's central software repository, can be installed easily using the following command. install.packages(&quot;package_name&quot;) Every time you reload R you will need to load the packages you need if they are not one of the ones installed by default. To do this type: library(&quot;package_name&quot;) 3.2.4 Functions Functions are a set of commands that are organised to perform a specific task. There are a lot of functions that come pre-installed in packages in R and others can be installed as parts of packages. It is also possible for users to create their own functions. For example a specific function can be called like this, we will use the sum function: sum(1,2) # which sums 1+2 and prints 3 to the console window. And to write your own function you would type something like this: function_name &lt;- function(arg_1, arg_2, ...) { Function body } 3.2.5 Assigning values to variables A variable provides a means of accessing the data stored in Rs memory. Data can be assigned to a variable using either &lt;- or =. For example: x&lt;-sum(1,2) If you run this command and then type x into the R console 3 will be printed to the screen. You can then use this variable in another function and assign to another variable if desired: y&lt;-sum(x,x) y # y would be assigned the value 6 and be printed to the console When you have a large script containing many commands it is beneficial to give variables logical names which can help you remember what they contain and help you call them quickly when needed. 3.2.6 A few useful commands and tips rm(list=ls()) can be used to clear your environment when you start a new analysis. This will delete all the variables you have stored in Rs memory so be careful when you use this. The read.table() can be used to read in a file. R likes to work with simple formats such as .csv and .txt files. write.table() can be used to write a data frame or a matrix to a file. Just like linux, remember you can press the up arrow on your keyboard to list previous commands run in the current R session. To get more information and help on the function you want to use you can call using ?_function.name_ e.g.: ?sum "],["04-Cluster_Introduction.html", "Chapter 4 Cluster Introduction 4.1 Logon instructions 4.2 The Terminal Window", " Chapter 4 Cluster Introduction 4.1 Logon instructions For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you were sent. You will now be in a logged-in Linux VNC desktop with two terminals. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance. If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal. How to zoom with your browser You may need to zoom out with your browser so you can see the full webVNC window. Chrome: Click on the three dots in vertical line ( ) on the top left for a dropdown menu which includes zoom options. Edge: Click on the three horizontal lines ( ) on the top left for a dropdown menu which includes zoom options. Firefox: Click on the three dots in horizontal line ( ) on the top left for a dropdown menu which includes zoom options. These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Mint Linux The following link is a guide to install Mint Linux: https://linuxmint-installation-guide.readthedocs.io/en/latest/ If you have a windows machine and would like to install linux on it you can install Windows Subsystem for Linux (WSL). The following link is a guide to install linux via (WSL): https://learn.microsoft.com/en-us/windows/wsl/install 4.2 The Terminal Window In our case the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences it may look different. Already there is useful information for us on the terminal window. nsc206: This is the login name, also known as the username. In this case nsc206 is a demonstrator's account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into. ada02: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux (OS) and others '~' is a shortcut to the user's home directory. Everything after the '$' is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen, choose Terminal. "],["05-Loading_data.html", "Chapter 5 Loading data into R and DADA2 5.1 Jupyter 5.2 Workshop notebook 5.3 Loading DADA2 5.4 Loading our data", " Chapter 5 Loading data into R and DADA2 Before we can start we first need to make a directory which will be used to contain all the files you generate throughout this workshop. To do this type the following commands. #Change directory to home cd ~ #Make a directory in home called &quot;Metabarcoding&quot; mkdir Metabarcoding #Change directory to &quot;Metabarcoding&quot; cd Metabarcoding You will need to activate the metabarcoding conda environment before continuing. Carry this out with the following command. Note: Ensure you include the dot and space (. ) at the start before usemetabarcoding . usemetabarcoding 5.1 Jupyter Jupyter-notebook is a nice browser based method to write, edit, and run code. It was initally created for Python coding, but has since branched out to many other languages, such as R. We are using it in this workshop for a variety of its properties: It is popular and well maintained. It is lightweight. Other heavier weight programs, such as RStudio, would struggle in our HPC due to the graphical and CPU load. It is interactive and displays code output. It allows for easier annotation, editing, and debugging than the command line. It provides a graphical interface for changing directories and choosing files. Before carrying out any analysis we will go through a quick tutorial of jupyter-notebook. 5.1.1 Open Jupyter-notebook The first step is to open jupyter-notebook. Run the below command in your (meta_barcode) environment. jupyter-notebook This will open jupyter-notebook in firefox. We won't need to access the linux terminal anymore. Leave the terminal running jupyter-notebook and full screen your firefox so you should see something like below. 5.1.2 Create R notebook The next step is to create a R notebook. Click on the \"New\" button towards the top right, right of the \"Upload\" button. From the dropdown click \"R\" below \"Python 3 (ipykernel)\". This will open up a new R notebook like below. 5.1.3 Cells and code Jupyter-notebook uses cells (the gray boxes) to separate code. This is very useful to compartmentalise our code. There will already be one cell. Within the cell, type in the below commands. 1+1 2-3 When pressing enter in cells it will create a new line. To run all commands in a cell press CTRL + enter. Run your current cell and you should see something like below. 5.1.4 Create new cells You can create new cells by 2 different means. Press the + button on the tool bar (between the floppy disk ( ) and scissors( ). ). This will add a cell below your currently selected cell. Click on the Insert button and use the dropdown to add a cell above or below your currently selected cell. Tip: Hover over the toolbar icons to display a text based description of its function. With that knowledge add a second cell below the first cell. Add the following code to your second cell but do not run it. num_1 &lt;- 3 num_2 &lt;- 10 Tip: Notice there are green lines around your selected cell. Insert a third cell and add the following code to it. Do not run the code. num_1 * num_2 5.1.5 Running code Try to run the code in the third cell. There should be an error as we have not created the objects num_1 &amp; num_2. We have only written the code for these objects but not run them. We can run all the code in a notebook starting from the first cell to the last cell. To run all cells from the start: Click on the \"Cell\" button. Click \"Run All\" from the drop-down options. You should then see something like the below in your notebook. There is no output printed for cell 2 because we are assigning variables. However, the correct output for Cell 3 is below it. This is because the variables were assigned in cell 2 before cell 3 was run. 5.1.6 Saving the file As with RStudio and other good coding interfaces we can save our notebook. First we should rename the file. Rename the notebook to \"jupyter_tut\": Click on the name of the notebook, currently called \"Untitled\". This is at the very top of the notebook, right of the Jupyter logo. A pop-up called \"Rename Notebook\" will appear. Change the Name to \"jupyter_tut\". Click \"Rename\". Now we can save the file. Two methods to save are: Click the floppy disk ( ) on the toolbar. Click on the \"File\" button. Click \"Save and Checkpoint\" from the dropdown options. 5.1.7 Title cells with markdown We will be using multiple notebooks in this workshop. We will also have multiple sections per notebook. It will be useful to create header cells with markdown to create visual separation of the different sections. To add a header cell to the top of our notebook: Create a new cell at the top of the notebook. Click on the \"Code\" drop down and select \"Markdown\". The \"Heading\" option no longer works. Add the following to the \"Markdown\" cell to create a first level header. Ensure you have a space between the # and header text (\"Tutorial\"). # Tutorial Great, we can now add nice headers in our notebooks. Save the notebook once more before carrying on to the next section. Markdown You won't need to know more about Markdown but if you are interested please see the Markdown guide. 5.1.8 Close the notebook To close the notebook: Click on \"File\". From the dropdown options click \"Close and Halt\". When you are back in the file explorer page you may not yet see the new file you saved. If so, you will need to refresh the page with the Refresh button ( ) towards the top right. With that quick tutorial of jupyter-notebook we can start our metabarcoding analysis. 5.1.9 Video tutorial 5.2 Workshop notebook Create a new R notebook called \"DADA2_analysis_pipeline_MiFish-U_primer_set\". Add a markdown cell with the first level header: # MiFish-U primer set analysis using DADA2 5.3 Loading DADA2 To start with we will need to load a few packages (see section 3.2.3 for a description of what a package is if you are not familiar with R) which we will need for running the first part of our analysis. First add a new code cell and type the following and run the cell to load the following package libraries. library(dada2) library(Biostrings) library(ShortRead) 5.4 Loading our data 5.4.1 Saving the paths for our input and output directories We are now nearly ready to input our data but first we need to set the main paths we will be using which contain the input files (the raw sequencing data files) and where we want to save the output files we generate (the 'Metabarcoding' directory we made above). In a new code cell type the following and run: input.path &lt;- &quot;/pub14/tea/nsc206/NEOF/metabarcoding_workshop&quot; output.path &lt;- &quot;~/Metabarcoding&quot; In a new code cell we can now check what files are contained in our input directory. Type and then run the following: list.files(input.path) You should see a list of paired fastq sequencing files listed. Forward and reverse fastq filenames have format: SAMPLENAME_L001_R1_001.fastq and SAMPLENAME_L001_R2_001.fastq. This shows that we have set our input path correctly. There is also a directory called 'taxonomy' and a csv file of sample information, both of which we will use later. From now on you will get less instructions on your notebook structure. Please create your own coding and markdown cells where you think appropriate. 5.4.2 Inputting the forward and reverse reads We will assign the input path and specify all files which end in _L001_R1_001.fastq as our forward reads (stored as the variable fnFs) and _L001_R2_001.fastq as our reverse reads (stored as the variable fnRs). fnFs &lt;- sort(list.files(input.path, pattern = &quot;_L001_R1_001.fastq&quot;, full.names = TRUE)) fnRs &lt;- sort(list.files(input.path, pattern = &quot;_L001_R2_001.fastq&quot;, full.names = TRUE)) "],["06-primer_removal.html", "Chapter 6 Identifying and removing primers 6.1 Primer orientation checking 6.2 Cutadapt", " Chapter 6 Identifying and removing primers Now you will carry out primer removal. DADA2 requires the primers you used to be trimmed off the forward and reverse of your reads. We will use the software cutadapt for this from within R. Read in your forward and reverse primer sequences for our MiFish-U primerset. The MiFish-U primers do not contain any degenerate bases but cutadapt is able to handle any degenerate bases you may have in primer sequences for your own projects. FWD &lt;- &quot;GTCGGTAAAACTCGTGCCAGC&quot; REV &lt;- &quot;CATAGTGGGGTATCTAATCCCAGTTTG&quot; We have written the primer sequences out below with space dividers to make it easier to type in. Remember to remove the spaces when you type this into R! Forward primer GTC GGT AAA ACT CGT GCC AGC Reverse primer CAT AGT GGG GTA TCT AAT CCC AGT TTG 6.1 Primer orientation checking It might be useful to check the orientation of the primers on our sequences so we know the best way to remove them. We know how our libraries were created and sequenced so we are expecting that the forward primer should be located in a forward orientation in the forward read and the reverse primer should be located in the forward orientation in the reverse read. For your own data you might be less certain as to which orientation your primers are in your reads. 6.1.1 Primer orientations We use the following code to make a list of all orientations of our primer sequences (forward, complement, reverse and reverse complement). allOrients &lt;- function(primer) { # Create all orientations of the input sequence require(Biostrings) dna &lt;- DNAString(primer) # The Biostrings works w/ DNAString #objects rather than character vectors orients &lt;- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), RevComp = reverseComplement(dna)) return(sapply(orients, toString)) # Convert back to character vector } FWD.orients &lt;- allOrients(FWD) FWD.orients # print all orientations of the forward primer to the console REV.orients &lt;- allOrients(REV) REV.orients # print all orientations of the reverse primer to the console 6.1.2 N filter It is a good idea to pre-filter your data before searching for the primer sequences to remove Ns (ambiguously called bases) from the dataset and improve mapping of the primers. To do this we can use the filterAndTrim function. For more information on filterAndTrim and the parameters you can specify type into the console. ?filterAndTrim (Close the pop-up window). For now we are only filtering Ns but we will come back to this function later after removing our primers. Specify the path to a new output directory to contain the N filtered files. We will call this directory filtN, and a copy of each of the forward and reverse fastq files with any 'N' containing sequences removed will be written here. fnFs.filtN &lt;- file.path(output.path, &quot;filtN&quot;, basename(fnFs)) fnRs.filtN &lt;- file.path(output.path, &quot;filtN&quot;, basename(fnRs)) Then run filterAndTrim with maxN = 0 (sequences with more than 0 Ns will be discarded) and multithread=TRUE (all input files are filtered in parallel, which is faster). filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE) For this function we have specified: the path to our raw forward read fastqs fnFs the path to write our N-trimmed forward fastqs fnFs.filtN the path to our raw reverse read fatsqs fnRs the path to write our N-trimmed reverse fastqs fnRs.filtN and the function-specific options for maxN and multithread This will take a few minutes to run. Wait for filterAndTrim to finish running before running the next step. This is indicated by the colour of the circle next to R on the right-hand side of the screen. Grey=running, clear=idle: 6.1.3 Primer orientations count The following function will use all possible primer combinations to count the number of times a primer is found in the forward and reverse read in each orientation. primerHits &lt;- function(primer, fn) { # Counts number of reads in which the primer is found nhits &lt;- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE) return(sum(nhits &gt; 0)) } rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]])) The [[1]] part of the rbind command runs the function on the first file in our list. To specify the second file you would change this to [[2]]. However, we do not need to run this command more than once (which is good as it can take a little while to run!) because we know that the libraries for each sample were made using the same protocol. Therefore, all the samples should look similar. We can see that although most of our primers are found as expected in the forward orientation on our forward/reverse read in this sample, we have a few sequences that also contain the reverse complement. This is likely due to the fact that there is some variation in the length of this amplicon between species so in some cases the sequencing machine has cycled through the full region and into the the forward/reverse primer. 6.2 Cutadapt Now that we know the orientation of our primers we can use the software cutadapt to remove the primers from our samples. In the below part we will use the system2 command to allow us to run a shell command from within R. See https://cutadapt.readthedocs.io/en/stable/ for more information on how to install and run cutadapt. 6.2.1 Cutadapt program path Specify the path to the cutadapt software so R knows where to find and run it. cutadapt &lt;- &quot;/pub14/tea/nsc206/mambaforge/envs/qiime2-2023.5/bin/cutadapt&quot; 6.2.2 Output paths Specify the path to the output files where we want to put the cutadapt output files. path.cut &lt;- file.path(output.path, &quot;cutadapt&quot;) if(!dir.exists(path.cut)) dir.create(path.cut) fnFs.cut &lt;- file.path(path.cut, basename(fnFs)) fnRs.cut &lt;- file.path(path.cut, basename(fnRs)) 6.2.3 Primer orientations options Here we specify the options needed by cutadapt in order to trim the forward orientation of the forward and reverse primer off the forward and reverse read and the reverse complement off the end of the forward and reverse reads. FWD.RC &lt;- dada2:::rc(FWD) REV.RC &lt;- dada2:::rc(REV) # Trim FWD and the reverse-complement of REV off of R1 (forward reads) R1.flags &lt;- paste(&quot;-g&quot;, FWD, &quot;-a&quot;, REV.RC) # Trim REV and the reverse-complement of FWD off of R2 (reverse reads) R2.flags &lt;- paste(&quot;-G&quot;, REV, &quot;-A&quot;, FWD.RC) What do the -g -a -G and -A parameters mean? (Hint: use the following command to check the helpfile for cutadapt) system2(cutadapt, args = &quot;--help&quot;) This should open the cutadapt help information in the Linux terminal session rather than Jupyter-Notebooks directly. 6.2.4 Other cutadapt options In addition to trimming the primers from the reads we will also specify a couple of extra useful parameters. --discard-untrimmed this tells cutadapt to discard any read where the primers haven't been trimmed off. This is especially important for our data as our files at the moment contain sequences amplified using both MiFish-U and 12S-V5 primer sets. We only want to keep sequences matching the MiFish-U primer set for this analysis. --minimum-length 60 discard reads shorter than 60bp. This will remove unexpected short reads and help speed up further analysis. 6.2.5 Run cutadapt Run cutadapt. (Note. The cutadapt step is time intensive so this might take a little while to run, probably about 15 minutes, and a lot of output will be written to the Linux terminal screen while it is running). for(i in seq_along(fnFs)) { system2(cutadapt, args = c(R1.flags, R2.flags, &quot;-n&quot;, 2, # -n 2 required to remove FWD and REV &quot;-o&quot;, fnFs.cut[i], &quot;-p&quot;, fnRs.cut[i], # output files fnFs.filtN[i], fnRs.filtN[i], # input files &quot;--discard-untrimmed&quot;, &quot;--minimum-length 60&quot;)) } 6.2.6 Primer check We can now check whether all the primers have been removed using the primerHits function we specified earlier. rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]])) We now have no primers remaining in our file. "],["07-quality_checks.html", "Chapter 7 Checking the quality of your data 7.1 Forward read quality 7.2 Reverse read quality", " Chapter 7 Checking the quality of your data Those of you who attended our Introduction to sequencing data and quality control course would have used FastQC to check the quality of the data. DADA2 has its own quality control option, which plots a similar read length by quality figure. 7.1 Forward read quality To plot the quality of the forward reads: Import the cutadapt files Extract the sample names Run the plotQualityProfile function # Specify the paths and file names of the forward and reverse primer cleaned files cutFs &lt;- sort(list.files(path.cut, pattern = &quot;_L001_R1_001.fastq&quot;, full.names = TRUE)) cutRs &lt;- sort(list.files(path.cut, pattern = &quot;_L001_R2_001.fastq&quot;, full.names = TRUE)) # Extract sample names get.sample.name &lt;- function(fname) strsplit(basename(fname), &quot;_&quot;)[[1]][2] sample.names &lt;- unname(sapply(cutFs, get.sample.name)) head(sample.names) # check the quality for the first file plotQualityProfile(cutFs[1:1]) The features of the plot include: Gray-scale heatmap: Shows the frequency of each quality score along the forward read lengths Green line: The median quality score Orange lines: The quartiles Red line: Situated at the bottom of the plot, it represents the proportion of reads of that particular length. Its y-axis values are on the right side of the plot. The quality is very good for our forward reads. You can also see that the majority of the forward reads are ~130bp long after having the primers removed with cutadapt. 7.2 Reverse read quality Now check the quality of the reverse file in the same way plotQualityProfile(cutRs[1:1]) The reverse reads also look like they are good quality. To check the quality of the second and third fastq files we would type. plotQualityProfile(cutFs[2:3]) plotQualityProfile(cutRs[2:3]) "],["08-clean.html", "Chapter 8 Cleaning your data", " Chapter 8 Cleaning your data We will now filter our data to remove any poor quality reads. First set the path to a directory to store the filtered output files called filtered. filtFs &lt;- file.path(path.cut, &quot;../filtered&quot;, basename(cutFs)) filtRs &lt;- file.path(path.cut, &quot;../filtered&quot;, basename(cutRs)) Now run filterAndTrim. This time we use the standard filtering parameters: maxN=0 After truncation, sequences with more than 0 Ns will be discarded. (DADA2 requires sequences contain no Ns) truncQ = 2 Truncate reads at the first instance of a quality score less than or equal to 2 rm.phix = TRUE Discard reads that match against the phiX genome maxEE=c(2, 2) After truncation, reads with higher than 2 \"expected errors\" will be discarded minLen = 60 Remove reads with length less than 60 (note these should have already been removed by cutadapt) multithread = TRUE input files are filtered in parallel out &lt;- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), truncQ = 2, minLen = 60, rm.phix = TRUE, compress = TRUE, multithread = TRUE) out Some samples have very low read numbers after this filtering step. These could be poor quality samples but we also have negatives controls in this dataset so we would expect these to contain zero or very few reads. "],["09-ASV_identification.html", "Chapter 9 Identification of ASVs 9.1 Generate an error model 9.2 Dereplication 9.3 Inferrence of ASVs 9.4 Merging paired end reads 9.5 Making our ASV matrix 9.6 Chimera detection and removal 9.7 Sequence tracking sanity check", " Chapter 9 Identification of ASVs 9.1 Generate an error model First we need to model the error rates of our dataset using both the forward and reverse reads. Each dataset will have a specific error-signature with errors introduced by PCR amplification and sequencing. errF &lt;- learnErrors(filtFs, multithread = TRUE) errR &lt;- learnErrors(filtRs, multithread = TRUE) We can use the plotErrors function to check the estimated error rates. plotErrors(errF, nominalQ = TRUE) You will see the Warning messages: 1: Transformation introduced infinite values in continuous y-axis 2: Transformation introduced infinite values in continuous y-axis Do not worry about these warning messages. This is a message from the plotting function to let you know that there were some zero values in the data plotted (which turn into infinities on the log-scale). This is expected, it results from the fact that not every combination of error type (e.g. A-&gt;C) and quality score (e.g. 33) is observed in your data, which is normal. Interpreting the plots. The error rates for each possible transition (e.g. A→C, A→G) are shown Red line - expected based on the quality score. (These are plotted when nominalQ = TRUE is included in the plot command) Black line - estimate Black dots - observed What we are expecting to see here is that the observed match up with estimates. Here we can see that the black dots track well with the black line. We can also see that the error rates drop with increasing quality score as we would expect. Sanity checks complete, we can proceed with the analysis. If you are worried about what your error plots look like when you fit your own dataset, one possible way to to improve the fit is to try increasing the number of bases the function is using (the default is 100 million). 9.2 Dereplication The next step is to dereplicate identical reads. This is a common step in many workflows used for processing amplicons. This saves time and processing power as identical reads are collapsed together. For example instead of processing 100 identical sequences, only one is processed but the original number (i.e. 100) is associated with it. exists &lt;- file.exists(filtFs) # check that all the samples are still present after filtering derepFs &lt;- derepFastq(filtFs[exists], verbose=TRUE) derepRs &lt;- derepFastq(filtRs[exists], verbose=TRUE) # Name the derep-class objects by the sample names names(derepFs) &lt;- sample.names[exists] names(derepRs) &lt;- sample.names[exists] 9.3 Inferrence of ASVs Now we are ready to infer the ASVs in our dataset. To do this DADA2 uses the error models created above to infer the true sample composition (follow this link for more details). Here we will run the inference algorithm on single samples to save time but it is also possible to run samples together in pseudo-pools to increase the ability to identify ASVs of low abundance. Low abundance ASVs in a sample may be filtered out when run separately but if they are found in higher numbers in another sample then the chance of that ASV being real increases. Pseudo-pooling increases the chances of these \"real\" low abundance ASVs being retained within samples. Whether or not to use the pseudo-pooling option will depend on your dataset and experimental design (see https://benjjneb.github.io/dada2/pseudo.html#Pseudo-pooling for more information). dadaFs &lt;- dada(derepFs, err = errF, multithread = TRUE) dadaRs &lt;- dada(derepRs, err = errR, multithread = TRUE) It is important to note that you want to run the error and inference steps on datasets generated from a single Illumina run. Data generated from different runs can have different error structures. 9.4 Merging paired end reads Up until now we have carried out all filtering, error correction and inference on the forward and reverse reads separately. It is now time to merge the two files. By default the minimum overlap allowed between the two samples is 12 bp. mergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE) 9.5 Making our ASV matrix Now it is time to make the counts table. Below we use the dim() function to print the dimensions, i.e. number of rows and columns. Each column represents a single ASV and each row is an individual sample. seqtab &lt;- makeSequenceTable(mergers) dim(seqtab) Q. How many ASVs are in our matrix? 9.6 Chimera detection and removal The last step in generating our ASV matrix is to detect and remove any chimeric sequences. Chimeric sequences are formed when two or more biological sequences are joined together. This is fairly common in amplicon sequencing. DADA2 uses a method whereby it combines the left and right segments of abundant reads and compares these with lower abundant sequences. Any low abundant sequences that match are removed. seqtab.nochim &lt;- removeBimeraDenovo(seqtab, method=&quot;consensus&quot;, multithread=TRUE, verbose=TRUE) dim(seqtab.nochim) Q. How many ASVs remain after filtering out chimeras? Although this is a large proportion of sequence variants it should be a smaller proportion of the total sequences. sum(seqtab.nochim)/sum(seqtab) In these data ~ 17% of the merged sequence reads were identified as chimeric. Check the range of ASV lengths: table(nchar(getSequences(seqtab.nochim))) Sequences that are much longer or shorter than the expected amplicon length could be the result of non-specific priming, and can be removed from your sequence table (e.g. seqtab.len &lt;- seqtab[,nchar(colnames(seqtab)) %in% 150:180]). This is analogous to “cutting a band” in-silico to get amplicons of the targeted length, e.g. the above command would retain amplicons between 150 bp and 180 bp and remove others. This is dependent on the amplicon and you should only remove these if you do not expect any length variation, or after further investigation of what these short or long sequences are. For this example we will proceed without any amplicon length filtering. 9.7 Sequence tracking sanity check The last thing to do in this section is to track the number of sequences through the pipeline to check whether everything has run as expected and whether there are any steps where we lose a disproportionate number of sequences. If we end up with too few reads to run further analysis we can use this table to identify any step which might require further investigation and optimisation. getN &lt;- function(x) sum(getUniques(x)) track &lt;- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim)) colnames(track) &lt;- c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoisedF&quot;, &quot;denoisedR&quot;, &quot;merged&quot;, &quot;nonchim&quot;) rownames(track) &lt;- sample.names track "],["10-Assign_taxonomy.html", "Chapter 10 Assigning taxonomy", " Chapter 10 Assigning taxonomy To assign taxonomy we will use a custom reference database containing fish sequences available for the MiFish-U amplicon region for species found in Lake Tanganyika and its broader catchment area. taxa &lt;- assignTaxonomy(seqtab.nochim, &quot;/pub14/tea/nsc206/NEOF/metabarcoding_workshop/taxonomy/MiFish_Reference_Database_taxonomy.fasta&quot;, multithread=TRUE, verbose = T) taxa.print &lt;- taxa rownames(taxa.print) &lt;- NULL head(taxa.print) This displays the taxonomic assignment of the first six ASVs at taxonomic levels from kingdom to species. The second ASV has not been assigned to any taxon in the reference database. Of the other five, two have been assigned at the species level, two at the genus level and one at the family level. "],["11-Further_analysis.html", "Chapter 11 Further analysis 11.1 R packages for further analysis 11.2 Rarefaction curves 11.3 Alpha diversity 11.4 Beta diversity", " Chapter 11 Further analysis In this section we will briefly discuss a few basic types of multivariate data analysis and data visualisation which are often used in metabarcoding studies. It is important to remember that there are several different ways to plot and analyse your data and we have presented only a few examples here. It might be that for your own data set and questions it would be better to approach your analysis in a different way. First we will load in a table which contains the metadata. meta&lt;-read.csv(&quot;/pub14/tea/nsc206/NEOF/metabarcoding_workshop/sample_info.csv&quot;, row.names = 1) Before we start we will clean up the dataset. Our negative controls seem to be mostly clean of contaminants (with only 0 and 4 reads assigned to them). We will remove these for further analysis. Here we remove rows 25 and 31 which contain the filter (S62) and extraction (S72) control samples from our ASV table and metadata table. seqtab.rmcontrol&lt;-seqtab.nochim[-c(25,31),] meta.rmcontrol&lt;-meta[-c(25,31),] If you are concerned about contaminants in your own analysis then it could be useful to explore the R package decontam which is designed to identify and filter contaminants. We will also remove samples which appear to have failed/have very low sequence numbers in our dataset. #Check number of sequences per sample rowSums(seqtab.rmcontrol) # S40, S41, S50 all have low sequence counts. So we will remove these rows. seqtab.rmlow&lt;-seqtab.rmcontrol[-c(13,14,23),] meta.rmlow&lt;-meta.rmcontrol[-c(13,14,23),] # Print the minimum sequence number in one sample. min(rowSums(seqtab.rmlow)) # The lowest number of sequences in one sample is now 26667. 11.1 R packages for further analysis We will use three more handy R packages in order to further explore our data, phyloseq and vegan to explore the divesrity in our data, plus the visualisation package ggplot2 for graphics. Load these libraries before proceeding with the steps below. library(phyloseq) library(vegan) library(ggplot2) 11.2 Rarefaction curves The more deeply we sequence a sample the more species we discover, until this accumulation levels off as we have found all or as many ASVs/species as we are going to find. This becomes a problem as samples are often sequenced at different depths so will have reached different points in the curve. This is a common difficulty as pooling and sequencing equal amounts of each sample can be tricky. One way to visualise this is to plot a rarefaction curve for each sample. rarecurve(seqtab.rmlow, step=100, col=meta.rmlow$COLOUR, lwd=2, ylab=&quot;ASVs&quot;, label=F) # add a vertical line to represent the fewest sequences in any sample abline(v=(min(rowSums(seqtab.rmlow)))) You can see that each sample levels off at a different sequencing depth and each sample has been sequenced to different depths. This also gives us an indication of how many ASVs are in each sample, with the aquarium samples here (dark blue lines) amongst the least diverse, as we'd expect. Rarefaction is one of the common approaches people use to correct for differences in library sequencing depth between samples and was originally proposed for traditional ecological studies (Sanders 1968). It involves randomly removing reads until you reach a number often equal to or less than the number of reads in the smallest sample. Random subsampling can result in a loss of data and generate artificial variation. Due to these concerns other methods of transformation have been suggested. Other methods of normalisation include normalising counts as a proportion of the total library size (McMurdie and Holmes 2014), centered log-ratio transformations (Gloor et al. 2017), geometric mean pairwise ratios (Chen et al. 2018), and also variance stabilising transformations and relative log expressions (Badri et al. 2018). Choosing the type of normalisation method to use for your own dataset is not trivial and the most appropriate method can vary between studies and datasets. Further reading is recommended. A couple of papers to get you started are listed below: McMurdie and Holmes (2014) Willis (2019) Cameron et al. (2021) 11.3 Alpha diversity Measures of alpha diversity are used to describe diversity within a sample. 11.3.1 Phyloseq creation We will use the R package phyloseq to plot alpha diversity. For this example we will proceed with unnormalised data. It is generally recommended not to normalise count data before calculating alpha diversity measures in the phyloseq FAQ. First make a phyloseq object. To do this we first read in our ASV, taxonomy, and metadata tables before making the plyloseq object phylo. seqtab.rmlow2&lt;-t(as.data.frame(seqtab.rmlow)) phylo_asv &lt;- otu_table(seqtab.rmlow2, taxa_are_rows=TRUE) phylo_tax &lt;- tax_table(taxa) phylo_samples &lt;- sample_data(meta.rmlow) phylo &lt;- phyloseq(phylo_asv, phylo_tax, phylo_samples) sample_names(phylo) rank_names(phylo) sample_variables(phylo) 11.3.2 Alpha diversity metrics The two alpha diversity metrics we will plot are: 1. Shannon's diversity index A measure of diversity where a higher number means higher diversity. Shannon's index accounts for the abundance and evenness of the features present. If richness and evenness increase the diversity score increases. Values can range from one (in case of a single dominant species) to the total number of all species (in case of all species having equal abundance). Equation: \\[ H = -\\sum_{i=1}^{S} p_i lnp_i \\] H = Shannon diversity index pi = is the proportion of species i S = Number of species 2. Simpson diversity index A measure of diversity based on number of features present and the relative abundance of each feature. If richness and evenness increase the diversity score increases. The values range from 0 (no diversity) to 1 (infinite diversity). Equation: \\[ D = 1 - {\\sum_{i=1}^{S} {p_i} ^{2}} \\] D = Simpson diversity index pi = is the proportion of species i S = Number of species 11.3.3 Alpha plots Plot the two alpha diversity metrics. plot_richness(phylo, measures=c(&quot;Shannon&quot;, &quot;Simpson&quot;), color = &quot;SITE&quot;) plot_richness(phylo, x=&quot;SITE&quot;, measures=c(&quot;Shannon&quot;, &quot;Simpson&quot;), color = &quot;SITE&quot;) + geom_boxplot() The first figure plots the diversity measure per sample and colours the output by site. The second figure combines the replicates to plot as a boxplot. 11.4 Beta diversity Beta diversity compares the difference in diversity between two sites, or to put it another way it calculates the number of species that are not the same in the two sites. 11.4.1 Normalisation We will normalise the data before running the beta diversity calculation. We will transform the data into proportions to be used for Bray-Curtis distances. ps.prop &lt;- transform_sample_counts(phylo, function(otu) otu/sum(otu)) 11.4.2 NMDS We then generate and plot the NMDS (Non-metric MultiDimenstional Scaling) using Bray-Curtis distances. Equation: \\[ d_jk = \\sum\\frac{|x_{ij}-x_{ik}|} {(x_{ij}+x_{ik})} \\] ord.nmds.bray &lt;- ordinate(ps.prop, method=&quot;NMDS&quot;, distance=&quot;bray&quot;) plot_ordination(ps.prop, ord.nmds.bray, color=&quot;SITE&quot;, title=&quot;Bray NMDS&quot;) In general the samples do seem to cluster roughly by site in the NMDS plot. 11.4.3 PERMANOVA We will then calculate the Bray-Curtis distances using the distance function and perform a PERMANOVA (permutational multivariate analysis of variance) using the adonis function from Vegan to check whether the separation of samples by site is significantly different. bray.dist&lt;-distance(ps.prop, method=&quot;bray&quot;) sampledf &lt;- data.frame(sample_data(phylo)) adonis2(bray.dist ~ SITE, data = sampledf) The PERMANOVA results suggest that there is a statistical difference in communities between sites. 11.4.4 Taxonomic bar chart Lastly, let's plot the proportion of ASV sequences within each sample that belong to different taxonomic families. plot_bar(ps.prop, fill = &quot;Family&quot;)+ geom_bar(aes(color=Family, fill=Family), stat=&quot;identity&quot;, position=&quot;stack&quot;)+ facet_grid(~SITE, scales = &quot;free&quot;, space = &quot;free&quot;) We can see that the five aquarium samples contain mostly Cichlidae sequences, apart from a very small number of unassigned (NA) and misassigned (there were only cichlid fish in the aquarium) sequences. Site 11 has more than 50% unassigned sequences in all three replicate samples. All of the samples containing &gt; 10% Procatopodidae sequences are grouped together in the NMDS plot (especially sites 19 &amp; 20, but also two samples from site 2, and the one sample from site 18), suggesting this may be in part driving the observed patterns of community similarity. "],["12-Analysing_another_marker.html", "Chapter 12 Analysing the second primer pair 12.1 The primer sequences 12.2 Questions", " Chapter 12 Analysing the second primer pair The sequencing data files we have analysed for the marker MiFish-U also contain sequences for a second marker, 12S-V5. Both of these primer pairs amplify up short regions of the 12S gene. As shown below, the 12S-V5 region neighbours that amplified by the MiFish-U primers, and has a shorter expected length. It can be common to use multiple genes or multiple markers of the same gene in metabarcoding studies. Depending on the taxa that you are targeting, multiple primer pairs can complement each other by increasing taxonomic coverage or resolution. 12.1 The primer sequences Below are the forward and reverse primer sequences for 12S-V5. FWD2 &lt;- &quot;ACTGGGATTAGATACCCC&quot; REV2 &lt;- &quot;TAGAACAGGCTCCTCTAG&quot; Here are the primer sequences again, but formatted so they are a little easier to read. When you type them into R though you should do this with no spaces, as above. 12S-V5 FWD: ACT GGG ATT AGA TAC CCC 12S-V5 REV: TAG AAC AGG CTC CTC TAG There is also a fasta reference database file for 12S-V5, which is here: /pub14/tea/nsc206/NEOF/metabarcoding_workshop/taxonomy/12S-V5_Reference_Database_taxonomy.fasta Run through the pipeline for the 12S-V5 primer pair, starting from identifying and removing the primers and through to the further diversity analyses. Start by making a new notebook called \"DADA2_analysis_pipeline_12S-V5_primer_set\". Try to work through this on your own using the commands run for the MiFish-U primers as a guide. You will be able to copy and paste commands from your MiFish-U notebook. However, if you do get stuck with any of the code then you can look at the next section for help. The questions below will hopefully help guide your interpretation of the different processing steps and output. Answers/discussion can also be found in the next section under each of the relevant headings. 12.2 Questions Primer orientation checking Do we see the same pattern of the orientation of the 12S-V5 primers in the forward and reverse reads as we did for the MiFish-U primers? If not, why do you think this is? Checking the data quality Is the sequence data of similar quality to the MiFish reads? Are there any differences you can notice between the quality plots for the MiFish-U and 12S-V5 sequences? Cleaning the data How many of the samples have fewer than 1000 clean reads remaining after quality trimming? Are these the same samples with low sequence numbers as those for the MiFish primers? Making the ASV matrix and chimera removal How many ASVs are identified for the 12S-V5 primer data, and how many non-chimeric ASVs? Does this number differ from the number for the MiFish data, and if so why do you think this could be? Sequence tracking sanity check How many samples have low sequence numbers, and does this include our two negative samples (S62 and S72)? Assigning taxonomy How many of the top six ASVs have species-level taxonomic assignments? Alpha diversity How do the measures of alpha diversity compare between MiFish-U and 12S-V5, in terms of level of diversity and variability within sampling site? Does our filter negative control sample have low or high diversity compared to the actual samples? Beta diversity How does the proportion of taxonomically unassigned reads compare between the 12S-V5 and MiFish ASVs? Does the negative filter control sample resemble any of the other samples? "],["13-Others_marker_steps.html", "Chapter 13 Guide to analysing the second primer pair 13.1 Loading the data 13.2 Identifying and removing primers 13.3 Checking the quality of your data 13.4 Cleaning your data 13.5 Identification of ASVs 13.6 Assigning taxonomy 13.7 Further analysis 13.8 Rarefaction curves 13.9 Alpha diversity 13.10 Beta diversity", " Chapter 13 Guide to analysing the second primer pair 13.1 Loading the data Load the DADA2 packages needed for analysis as before. The two primer pairs were multiplexed together for each sample, so our input path and lists of file names and locations (fnFs and fnRs) remain the same. 13.2 Identifying and removing primers Now to carry out primer removal. DADA2 requires the primers you used to be trimmed off the forward and reverse of your reads. We will use the software cutadapt again for this, which will this time identify and remove the 12S-V5 primers and discard any other sequences (including those we have already analysed for MiFish-U). Read in your forward and reverse primer sequences for the 12S-V5 primer set, but give them different variable names. The 12S-V5 primers do not contain any degenerate bases but cutadapt is able to handle any degenerate bases you may have in your own primer sequences. FWD2 &lt;- &quot;ACTGGGATTAGATACCCC&quot; REV2 &lt;- &quot;TAGAACAGGCTCCTCTAG&quot; Here are the primer sequences again, but formatted so they are a little easier to read. When you type them into R though you should do this with no spaces, as above. 12S-V5 FWD: ACT GGG ATT AGA TAC CCC 12S-V5 REV: TAG AAC AGG CTC CTC TAG 13.2.1 Primer orientation checking Let's check the orientation of these primers. We know how our libraries were created and sequenced so we are expecting that the forward primer should be located in a forward orientation in the forward read and the reverse primer should be located in the forward orientation in the reverse read. For your own data you might be less certain as to which orientation your primers are in your reads. We can use the allOrients function again to make a list of all orientations of our primer sequences (forward, complement, reverse and reverse complement, again make sure to give these R variables a unique name). Note that you can copy this from your previous notebook. FWD2.orients &lt;- allOrients(FWD2) FWD2.orients # print all orientations of the forward primer to the console REV2.orients &lt;- allOrients(REV2) REV2.orients # print all orientations of the reverse primer to the console We have already filtered any Ns from our raw data when we processed the MiFish primer sequences, so we can use those output files to find and trim the 12S-V5 primer sequences. fnFs.filtN &lt;- file.path(output.path, &quot;filtN&quot;, basename(fnFs)) fnRs.filtN &lt;- file.path(output.path, &quot;filtN&quot;, basename(fnRs)) The primerHits function will use all possible 12S-V5 primer combinations to count the number of times a primer is found in the forward and reverse read in each orientation. rbind(FWD2.ForwardReads = sapply(FWD2.orients, primerHits, fn = fnFs.filtN[[1]]), FWD2.ReverseReads = sapply(FWD2.orients, primerHits, fn = fnRs.filtN[[1]]), REV2.ForwardReads = sapply(REV2.orients, primerHits, fn = fnFs.filtN[[1]]), REV2.ReverseReads = sapply(REV2.orients, primerHits, fn = fnRs.filtN[[1]])) The [[1]] part of the rbind command runs the function on the first file in our list, and it will take a little while to run. As with MiFish, we have both primers found as expected in the forward orientation on our forward and reverse read in this sample. However, there are also the same number or more hits for the reverse complements of the primer sequences. This is actually a bit of an odd looking sample as there are very few reads (&lt; 2,000) matching the primer sequence in the forward orientation, so let's check another sample to get a better idea of the 12S-V5 primers in these data. rbind(FWD2.ForwardReads = sapply(FWD2.orients, primerHits, fn = fnFs.filtN[[5]]), FWD2.ReverseReads = sapply(FWD2.orients, primerHits, fn = fnRs.filtN[[5]]), REV2.ForwardReads = sapply(REV2.orients, primerHits, fn = fnFs.filtN[[5]]), REV2.ReverseReads = sapply(REV2.orients, primerHits, fn = fnRs.filtN[[5]])) Here we can see that (a) the reverse complement of the forward primer sequence is frequently seen in the reverse reads, and (b) the reverse complement of the reverse primer is frequently seen in the forward reads. The 12S-V5 forward primer is actually situated in the same location as the MiFish-U reverse primer (see the figure in section 13), so in this case we are seeing it in both the 12S-V5 and the MiFish-U sequences. The expected length of the 12S-V5 amplicons is ~106 bp, and hence the Illumina reads have sequenced through the full amplicon and into the reverse primer. This explains the numbers, and neither of these are problematic for cutadapt to identify and trim these primers to leave us with the 12S-V5 sequences for further analysis. 13.2.2 Cutadapt Copy and paste the path to cutadapt into your new notebook. Specify the path to a new output directory ('cutadapt2') where we will put the 12S-V5 cutadapt output files. path.cut2 &lt;- file.path(output.path, &quot;cutadapt2&quot;) if(!dir.exists(path.cut2)) dir.create(path.cut2) fnFs.cut2 &lt;- file.path(path.cut2, basename(fnFs)) fnRs.cut2 &lt;- file.path(path.cut2, basename(fnRs)) Here we specify the options needed by cutadapt in order to trim the forward orientation of the forward and reverse primer off the forward and reverse read and the reverse complement off the end of the forward and reverse reads. FWD2.RC &lt;- dada2:::rc(FWD2) REV2.RC &lt;- dada2:::rc(REV2) # Trim FWD and the reverse-complement of REV off of R1 (forward reads) R1.flags2 &lt;- paste(&quot;-g&quot;, FWD2, &quot;-a&quot;, REV2.RC) # Trim REV and the reverse-complement of FWD off of R2 (reverse reads) R2.flags2 &lt;- paste(&quot;-G&quot;, REV2, &quot;-A&quot;, FWD2.RC) In addition to trimming the primers from the reads we will also specify a couple of extra useful parameters. --discard-untrimmed this tells cutadapt to discard any read where the primers haven't been trimmed off. This is especially important for our data as our files at the moment contain sequences amplified using both MiFish-U and 12S-V5 primer sets. We only want to keep sequences matching the 12S-V5 primer set for this analysis. --minimum-length 60 discard reads shorter than 60bp. This will remove unexpected short reads and help speed up further analysis. Run cutadapt. (Note. The cutadapt step is time intensive so this might take a little while to run, probably about 15 minutes). for(i in seq_along(fnFs)) { system2(cutadapt, args = c(R1.flags2, R2.flags2, &quot;-n&quot;, 2, # -n 2 required to remove FWD and REV &quot;-o&quot;, fnFs.cut2[i], &quot;-p&quot;, fnRs.cut2[i], # output files fnFs.filtN[i], fnRs.filtN[i], # input files &quot;--discard-untrimmed&quot;, &quot;--minimum-length 60&quot;)) } We can now check whether all the primers have been removed using the primerHits function we specified earlier. rbind(FWD2.ForwardReads = sapply(FWD2.orients, primerHits, fn = fnFs.cut2[[1]]), FWD2.ReverseReads = sapply(FWD2.orients, primerHits, fn = fnRs.cut2[[1]]), REV2.ForwardReads = sapply(REV2.orients, primerHits, fn = fnFs.cut2[[1]]), REV2.ReverseReads = sapply(REV2.orients, primerHits, fn = fnRs.cut2[[1]])) We now have no primers remaining in our file. 13.3 Checking the quality of your data Those of you who attended our quality Introduction to sequencing data and quality control course we used FastQC to check the quality of our data. DADA2 has its own quality control option, which plots a similar read length by quality figure. To run first import the cutadapt files and then run the plotQualityProfile function # Specify the paths and file names the forward and reverse primer cleaned files cutFs2 &lt;- sort(list.files(path.cut2, pattern = &quot;_L001_R1_001.fastq&quot;, full.names = TRUE)) cutRs2 &lt;- sort(list.files(path.cut2, pattern = &quot;_L001_R2_001.fastq&quot;, full.names = TRUE)) # Extract sample names get.sample.name &lt;- function(fname) strsplit(basename(fname), &quot;_&quot;)[[1]][2] sample.names &lt;- unname(sapply(cutFs2, get.sample.name)) head(sample.names) # check the quality for the first file plotQualityProfile(cutFs2[1:1]) To interpret this plot, the gray-scale heatmap shows the the frequency of each quality score along the forward read length.The green line is the median quality score and the orange lines are the quartiles. The red line at the bottom of the plot represents the proportion of reads of that particular length. The overall quality is good for our forward reads, as for the MiFish sequences. The majority of the reads are just over 100bp long, as we expect with this shorter amplicon. 13.4 Cleaning your data We will now filter our data to remove any poor quality reads. First set the path to a directory to store the filtered output files called filtered2. filtFs2 &lt;- file.path(path.cut2, &quot;../filtered2&quot;, basename(cutFs2)) filtRs2 &lt;- file.path(path.cut2, &quot;../filtered2&quot;, basename(cutRs2)) Now run filterAndTrim, using the standard filtering parameters: maxN=0 After truncation, sequences with more than 0 Ns will be discarded. (DADA2 requires sequences contain no Ns) truncQ = 2 Truncate reads at the first instance of a quality score less than or equal to 2 rm.phix = TRUE Discard reads that match against the phiX genome maxEE=c(2, 2) After truncation, reads with higher than 2 \"expected errors\" will be discarded minLen = 60 Remove reads with length less than 60 (note these should have already been removed by cutadapt) multithread = TRUE input files are filtered in parallel out2 &lt;- filterAndTrim(cutFs2, filtFs2, cutRs2, filtRs2, maxN = 0, maxEE = c(2, 2), truncQ = 2, minLen = 60, rm.phix = TRUE, compress = TRUE, multithread = TRUE) out2 Some samples have low read numbers. These could be poor quality samples or the negatives controls. Some of the samples with low read numbers are different to those with low reads for MiFish-U. This might be because (through sample composition or by chance) the PCR or sequencing has worked better for one primer combination than the other for the same sample. 13.5 Identification of ASVs 13.5.1 Generate an error model First we need to model the error rates of our dataset using both the forward and reverse reads. Each dataset will have a specific error-signature with errors introduced by PCR amplification and sequencing. errF2 &lt;- learnErrors(filtFs2, multithread = TRUE) errR2 &lt;- learnErrors(filtRs2, multithread = TRUE) We can use the plotErrors function to check the estimated error rates. plotErrors(errF2, nominalQ = TRUE) Interpreting the plots, as a reminder: The error rates for each possible transition (e.g. A→C, A→G) are shown Red line - expected based on the quality score Black line - estimate Black dots - observed The error plots look very similar to those for the MiFish-U sequences - the observed dots track well with the expected line, and the error rates drop with increasing quality scores. Sanity checks complete, we can proceed with the analysis. 13.5.2 Dereplication The next step is to dereplicate identical reads, after first checking again that all the samples are present after filtering. exists2 &lt;- file.exists(filtFs2) # check that all the samples #are still present after filtering derepFs2 &lt;- derepFastq(filtFs2[exists2], verbose=TRUE) derepRs2 &lt;- derepFastq(filtRs2[exists2], verbose=TRUE) # Name the derep-class objects by the sample names names(derepFs2) &lt;- sample.names[exists2] names(derepRs2) &lt;- sample.names[exists2] 13.5.3 Inferrence of ASVs Now we are ready to infer the ASVs in our dataset. dadaFs2 &lt;- dada(derepFs2, err = errF2, multithread = TRUE) dadaRs2 &lt;- dada(derepRs2, err = errR2, multithread = TRUE) 13.5.4 Merging paired end reads It is now time to merge the two files. By default the minimum overlap allowed between the two samples is 12 bp, and no mismatches are permitted. mergers2 &lt;- mergePairs(dadaFs2, derepFs2, dadaRs2, derepRs2, verbose=TRUE) 13.5.5 Making our ASV matrix Now it is time to make the counts table. Each column represents a single ASV and each row is an individual sample. seqtab2 &lt;- makeSequenceTable(mergers2) dim(seqtab2) There are 583 ASVs in our matrix. 13.5.6 Chimera detection and removal The last step in generating our ASV matrix is to detect and remove any chimeric sequences. seqtab2.nochim &lt;- removeBimeraDenovo(seqtab2, method=&quot;consensus&quot;, multithread=TRUE, verbose=TRUE) dim(seqtab2.nochim) We have 212 ASVs remaining after filtering out chimeras. There are fewer ASVs for the 12S-V5 data than for the MiFish data. This could be as it is a shorter amplicon and therefore has less potential for variation, or potentially the primers could be less efficient at/have more mismatches when amplifying some taxa. sum(seqtab2.nochim)/sum(seqtab2) In these data ~ 7.5% of the merged sequence reads were identified as chimeric. CHeck the range of ASV lengths: table(nchar(getSequences(seqtab2.nochim))) 13.5.7 Sequence tracking sanity check The last thing to do in this section is to track the number of sequences through the pipeline to check whether everything has run as expected and whether there are any steps where we loose a disproportionate number of sequences. If we end up with too few reads to run further analysis we can use this table to identify any step which might require further investigation and optimisation. We will generate the number of sequences using the getN function. You can copy and paste this from the previous notebook. getN &lt;- function(x) sum(getUniques(x)) track2 &lt;- cbind(out2, sapply(dadaFs2, getN), sapply(dadaRs2, getN), sapply(mergers2, getN), rowSums(seqtab2.nochim)) colnames(track2) &lt;- c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoisedF&quot;, &quot;denoisedR&quot;, &quot;merged&quot;, &quot;nonchim&quot;) rownames(track2) &lt;- sample.names track2 There are five samples with fewer than 2000 reads. However, our eDNA filter negative control (S62) has &gt;100000 sequences. 13.6 Assigning taxonomy To assign taxonomy we will use a custom reference database containing fish sequences available for the 12S-V5 amplicon region for species found in Lake Tanganyika and its broader catchment area. taxa12S &lt;- assignTaxonomy(seqtab2.nochim, &quot;/pub14/tea/nsc206/NEOF/metabarcoding_workshop/taxonomy/12S-V5_Reference_Database.fasta&quot;, multithread=TRUE, verbose = T) taxa12S.print &lt;- taxa12S rownames(taxa12S.print) &lt;- NULL head(taxa12S.print) For this marker all of the first six ASVs have been assigned to class Actinopteri, four to the family-level (all Cichlidae), and one to the species-level (Pseudosimochromis babaulti). 13.7 Further analysis As the 12S-V5 sequences are all from the same samples then we can use the same meta data table as for the MiFish analysis. Before we start we will clean up the dataset. One of our negative controls seems to be mostly clean of contaminants (S72, with only 9 reads assigned). We will remove this from further analysis, but retain our filter control negative (S62) for further investigation. meta&lt;-read.csv(&quot;/pub14/tea/nsc206/NEOF/metabarcoding_workshop/sample_info.csv&quot;, row.names = 1) seqtab2.rmcontrol&lt;-seqtab2.nochim[-31,] meta2.rmcontrol&lt;-meta[-31,] We will also remove samples which appear to have failed/have very low sequence numbers in our dataset. #Check number of sequences per sample rowSums(seqtab2.rmcontrol) # S16, S25, S40 and S43 all have low sequence counts. So we will remove these rows. seqtab2.rmlow&lt;-seqtab2.rmcontrol[-c(1,4,13,16),] meta2.rmlow&lt;-meta2.rmcontrol[-c(1,4,13,16),] # Print the minimum sequence number in one sample. min(rowSums(seqtab2.rmlow)) # The lowest number of sequences in one sample is now 26841. 13.8 Rarefaction curves The more deeply we sequence a sample the more species we discover, until this accumulation levels off as we have found all or as many ASVs/species as we are going to find. This becomes a problem as samples are often sequenced at different depths so will have reached different points in the curve. This is a common difficulty as pooling and sequencing equal amounts of each sample can be tricky. One way to visualise this is to plot a rarefaction curve for each sample. Note: remember to load the required packages first. rarecurve(seqtab2.rmlow, step=100, col=meta2.rmlow$COLOUR, lwd=2, ylab=&quot;ASVs&quot;, label=F) # add a vertical line to represent the fewest sequences in any sample abline(v=(min(rowSums(seqtab2.rmlow)))) You can see that each sample levels off at a different sequencing depth and each sample has been sequenced at different depths. Overall the number of ASVs is lower, but as with MiFish the ASV numbers level off for almost all samples before the minimum sequencing depth is reached. 13.9 Alpha diversity Measures of alpha diversity are used to describe diversity within a sample. First make a phyloseq object. To do this we first read in our ASV, taxonomy and metadata tables before making the plyloseq object phylo2. seqtab2.rmlow2&lt;-t(as.data.frame(seqtab2.rmlow)) phylo_asv2 &lt;- otu_table(seqtab2.rmlow2, taxa_are_rows=TRUE) phylo_tax2 &lt;- tax_table(taxa12S) phylo_samples2 &lt;- sample_data(meta2.rmlow) phylo2 &lt;- phyloseq(phylo_asv2, phylo_tax2, phylo_samples2) sample_names(phylo2) rank_names(phylo2) sample_variables(phylo2) Plot the two alpha diversity metrics. plot_richness(phylo2, measures=c(&quot;Shannon&quot;, &quot;Simpson&quot;), color = &quot;SITE&quot;) plot_richness(phylo2, x=&quot;SITE&quot;, measures=c(&quot;Shannon&quot;, &quot;Simpson&quot;), color = &quot;SITE&quot;) + geom_boxplot() The first figure plots the diversity measure per sample and colours the output by site. The second figure combines the replicates to plot as a boxplot. We can see that both the Shannon and Simpson diversity is lower for 12S-V5, and variability within sampling sites is generally higher. The filter negative control has very low diversity (only S41 is lower). 13.10 Beta diversity Beta diversity compares the difference in diversity between two sites, or to put it another way it calculates the number of species that are not the same in the two sites. We will normalise the data before running the beta diversity calculation. We will transform the data into proportions to be used for Bray-Curtis distances. ps.prop2 &lt;- transform_sample_counts(phylo2, function(otu) otu/sum(otu)) We then generate and plot the NMDS (Non-metric MultiDimenstional Scaling) using Bray-Curtis distances. ord.nmds.bray2 &lt;- ordinate(ps.prop2, method=&quot;NMDS&quot;, distance=&quot;bray&quot;) plot_ordination(ps.prop2, ord.nmds.bray2, color=&quot;SITE&quot;, title=&quot;Bray NMDS 12S-V5&quot;) This is a very different pattern to that for the MiFish data - all of the lake site samples are clustered together, as our the aquarium samples, and the filter control is very distant to both of these groupings. Before deciding how to proceed let's plot the proportion of ASV sequences within each sample that belong to different taxonomic families. plot_bar(ps.prop2, fill = &quot;Family&quot;)+ geom_bar(aes(color=Family, fill=Family), stat=&quot;identity&quot;, position=&quot;stack&quot;)+ facet_grid(~SITE, scales = &quot;free&quot;, space = &quot;free&quot;) There are a far higher proportion of ASVs unassigned with this primer combination (the grey NA portion of the bars). The filter control sample has almost entirely unassigned ASVs. Given the low diversity, high differentiation from other samples and lack of ASVs of the fish sequences of interest we can remove this negative control, and re-plot the NMDS. Let's remove the aquarium samples as well, and re-plot the community similarity for the lake filter samples. #To remove specific samples from a phyloseq object SamplesToRemove &lt;- c(&quot;S62&quot;, &quot;S63&quot;, &quot;S64&quot;, &quot;S65&quot;, &quot;S66&quot;, &quot;S67&quot;) phylo2.nofilt &lt;- prune_samples(!(sample_names(phylo2) %in% SamplesToRemove), phylo2) #Transform the data into proportions and re-plot NMDS ps.prop.nofilt &lt;- transform_sample_counts(phylo2.nofilt, function(otu) otu/sum(otu)) ord.nmds.bray.nofilt &lt;- ordinate(ps.prop.nofilt, method=&quot;NMDS&quot;, distance=&quot;bray&quot;) plot_ordination(ps.prop.nofilt, ord.nmds.bray.nofilt, color=&quot;SITE&quot;, title=&quot;Bray NMDS 12S-V5 lake samples&quot;) Lastly, we can calculate the Bray–Curtis distances using the distance function and perform a PERMANOVA (permutational multivariate analysis of variance) using the adonis function from Vegan to check whether there is any significantly different separation of samples by site. bray.dist.nofilt&lt;-distance(ps.prop.nofilt, method=&quot;bray&quot;) sampledf.nofilt &lt;- data.frame(sample_data(phylo2.nofilt)) adonis2(bray.dist.nofilt ~ SITE, data = sampledf.nofilt) The PERMANOVA results suggest that there is also a statistical difference in communities between sites with the 12S-V5 data. "],["14-Appendix.html", "A Mamba installs A.1 Mamba installation and environment A.2 Jupyter-notebook", " A Mamba installs A.1 Mamba installation and environment Mamba is a reimplementation of conda. It is a great tool for installing bioinformatic packages including R packages. Mamba github: https://github.com/mamba-org/mamba The best way to use Mamba is to install Miniforge. It has both Conda and Mamba commands. Miniforge installation: https://github.com/conda-forge/miniforge Mamba guide: https://mamba.readthedocs.io/en/latest/user_guide/mamba.html To create the mamba environment meta_barcode run the below commands in your bash. You will need to have installed mamba first. #meta_barcode mamba create -n meta_barcode mamba activate meta_barcode #Install packages from bioconda channel mamba install -c bioconda bioconductor-dada2 bioconductor-biostrings bioconductor-shortread \\ cutadapt bioconductor-phyloseq bioconductor-deseq2 blast megan #Install packages from conda-forge channel mamba install -c conda-forge r-vegan r-ggplot2 r-irkernel A.2 Jupyter-notebook If you are running this on your own computer you can use RStudio. However, you can also use Jupyter-notebook if you are using an HPC or prefer it. If using bash you will need to create an environment with Jupyter-notebook. Ensure you are in the (base) mamba environment. mamba create -n jupyter mamba activate jupyter mamba install -c anaconda jupyter mamba deactivate To run Jupyter-notebook with your meta_barcode environment you can run the following. #Activate you meta_barcode env mamba activate meta_barcode #Run jupyter-notebook (may be a slightly different path) ~/mamba/envs/jupyter/bin/jupyter-notebook "]]
