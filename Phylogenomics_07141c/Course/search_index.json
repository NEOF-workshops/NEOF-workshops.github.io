[["01-Intro.html", "Phylogenomics Chapter 1 Introduction", " Phylogenomics Gavin Gouws, Matthew Gemmell, Helen Hipperson, Ewan Harney, Kathryn Maher 2024-01-22 Chapter 1 Introduction With the proliferation of Next Generation Sequencing, data sets for the study of phylogenetic relationships are growing increasingly larger, and data sets of genome-wide markers from a multitude of taxa are now readily attainable. This allows us to address questions around the evolution, biogeography and accurate delineation of biodiversity. The tools used to conduct typical phylogenetic analyses have advanced to accommodate these larger data sets, opposed to studies of single genes, or a few data partitions or sequence alignments that were common a decade ago. This online workshop combines introductory lectures and self-paced practical. This involves working through this interactive workbook, completing bioinformatics exercises and analyses on a webVNC. Instructors are on hand to provide support and guidance. In this practical session we will introduce you to analysing SNP datasets in phylogenetic and species delimitation contexts. Specifically, we will go through: Background Cluster Introduction Sequence quality control Preparing data in Stacks Model selection, tree reconstruction and support evaluation Hypothesis and topology testing Dating phylogenies Species delimitation with the Multispecies Coalescent At the end of the workshop you will be familiar with: Sequence and other data formats used in phylogenomic analyses The RADseq procedure and the data produced The basic quality control of Illumina data Locus reconstruction, SNP genotyping and preparing a sequence alignment from such data Evaluating models of nucleotide or sequence evolution Constructing and determining the statistical support for phylogenetic trees Manipulating trees to test alternative hypotheses Determining the times of divergence of branches in the tree Evaluating species boundaries and testing taxonomic hypotheses This workflow and these tools will serve as a basis (with additional consideration and more intensive analysis) for analysing and publishing your own data. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Background.html", "Chapter 2 Background 2.1 Example data: ddRADseq data of manta and devil rays", " Chapter 2 Background In this workshop, we will work through examples of typical phylogenomic and species delimitation analyses that can be performed using large datasets of genome-wide genetic variation. Single Nucleotide Polymorphisms (SNPs) obtained through next-generation sequencing will be used. We will examine raw data, perform routine quality control on the sequences, compile the SNP data for analysis. We'll then proceed to construct, evaluate, manipulate and test phylogenetic trees, date phylogenies, and examine gene tree-species tree conflicts and determine species boundaries in a multi-species coalescent framework. The supplemental workbook contains an example of an alternative data set and different implementations of some of these analyses. 2.1 Example data: ddRADseq data of manta and devil rays For all the exercises in the main workbook, we will use various subsets of the data generated by Hosegood et al. (2020). These authors used a reduced-representation, genome-wide sequencing approach (double-digest Restriction-site Associated DNA sequencing or \"ddRAD\") to generate SNP data to investigate manta rays and devil rays across their global distribution. The aim of the study was to: Determine the phylogenetic relationships among species groups. Test the validity of the current taxonomy. To examine species delimitation in order to identify meaningful taxonomic or biological units for conservation. Their study included 116 individuals, representing all species of Mobula known at the time of sampling, and an additional five representatives of the outgroup, Rhinoptera bonasus. All data were generated using Illumina sequencing, with the study combining SNP data generated through a pilot MiSeq sequencing run with data generated in a larger HiSeq run. The authors compiled two data sets, using different filtering thresholds. This provided different levels of missing data, and 1762 and 7926 SNPs were available for analysis in each. We'll take a more straightforward approach and use various subsets of the original data set. This will allow us to run analyses in a timely manner for the workshop. We will demultiplex samples using sequence reads from one multiplexed sample pool, identify variants and prepare a sequence alignment using data from five samples. Then we will proceed to construct and examine a phylogenetic tree of 22 individuals. This will be followed by manipulating the tree to test alternative hypotheses and to date cladogenic events. Finally, we'll look at two species delimitation analyses using a much smaller subset of the data. Some of the figures from the Hosegood et al. (2020) study are presented below to give an indication of the taxonomic, geographic and analytical scope of their study, and of the results they reported. The results we will produce in this workshop will differ because we will be using different subsets of the data, we will filter and treat the data differently, and - in some cases - will use slightly different approaches. "],["03-Cluster_Introduction.html", "Chapter 3 Cluster Introduction 3.1 Logon instructions 3.2 The Terminal Window 3.3 Accessing the example data", " Chapter 3 Cluster Introduction 3.1 Logon instructions For this workshop, we will be using Virtual Network Computing (VNC). Use the webVNC link that you were sent and log in to the VNC through a browser. You will now be logged in to a Linux VNC desktop. You should see something like the image below, with two terminals. If there is only one, don't panic - this is fine. If you do not see something similar, please ask for assistance. If the VNC is taking up too much or too little space of your browser, you can use the zoom function of your browser to adjust the size. You can also resize the terminal window by dragging the corner in or out. Ensure you can see one whole terminal. Many of these instructions will not work outside of this webVNC and the environments created within it. If you would like to install your own Linux OS on your desktop or laptop, we would recommend MintLinux. The following link is a guide to install MintLinux: https://linuxmint-installation-guide.readthedocs.io/en/latest/ 3.2 The Terminal Window We will use the terminal window as our shell to interpret and pass our commands to the kernel. The terminal window itself should look like the picture below. It might be slightly different, depending on your system and preferences. Already, there is useful information for us in the terminal window. nsc206: This is the login name, also known as the username. In this case nsc006 is a demonstrator's account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into. ada02: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux OS and others '~' is a shortcut to the user's home directory. Everything after the '$' is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen (the grey bit) and choose Terminal 3.3 Accessing the example data Before we can start, we first need to copy the material for the workshop into our home directory. Type the following command to get to your home directory. # Change directory to home cd ~ Then copy the directory and files into your home directory. This may take a few minutes to copy across. cp -r /pub14/tea/nsc206/NEOF/phylogenomics/ . Your home directory will now contain a directory called 'phylogenomics'. This will include subdirectories with the example data files and some supporting files that we need. All the files you will generate throughout this workshop will be written to this directory or sub-directories within it. You will then need to activate the phylogenomics conda environment before continuing. Carry this out with the following command: Note: Ensure you include the dot and space (. ) at the start before typing usephylogenomics. . usephylogenomics The environment has loaded successfully if the word 'phylogenomics' appears in in parentheses (brackets) at the start of the new line (before the the log-in, machine and directory names, and command line). The environment on the webVNC was previously installed with all the software and packages you need, using mamba. Mamba is a re-implementation of conda and is a relatively straightforward environment and package manager. More information on mamba is provided in the Appendix, along with commands for setting this environment up on your own computer or cluster (if you are allowed to). Navigate to your 'phylogenomics' directory. cd ~/phylogenomics You are now ready to start examining and analysing the data! "],["04-Quality_Control.html", "Chapter 4 Quality Control 4.1 Workshop data 4.2 Manta- and devil-ray ddRAD data set 4.3 Quality assessment 4.4 Quality Control 4.5 Post QC Check", " Chapter 4 Quality Control This tutorial will give hands-on experience with the quality control of Illumina sequence data. We will first assess the quality of the RADseq data produced through an Illumina sequencing run before proceeding to some quality control. 4.1 Workshop data Before we can carry on with this section, navigate into the sub-directory where the raw data are located. # Check that you are still in the &#39;phylogenomics&#39; directory pwd # Then navigate to the sub-directory cd raw_data # You can navigate to the sub-directory directly, regardless of where you are with this command cd ~/phylogenomics/raw_data 4.2 Manta- and devil-ray ddRAD data set This dataset contains one set of sequence files, the forward (R1) and reverse (R2) read files of 'Lib1_001_', that were demultiplexed from the paired-end Illumina sequencing run. These are compressed (gzipped) fastq files, as indicated by the '.gz' extension. The files contain sequence reads of many RADtags (double-digested DNA sequence fragments) that were isolated and sequenced from a number of individuals of different manta ray and devil ray species, and multiplexed (combined) into a single pool for sequencing. These are some of the original raw data from Hosegood et al. (2020), but the files have been randomly \"thinned\" (they contain about 10% of the original data) to enable us to complete the exercises in a timely manner. List the contents of the current directory to confirm that the two sequence files are there. There should also be two additional files ('manta_samples.barcodes' and 'TruSeq3-PE.fa'), which we will use later. ls Have a look at the structure of the reverse (R2) fastq file: Remember: You can use 'Tab' to auto-complete and save yourself the typing. zcat Lib1_001_R2.fastq.gz | head zcat is a command for viewing the contents of compressed files (equivalent to using cat for other files) and the pipe | symbol then passes this output to head to display only the first few lines of the file. Fastq files contain a sequence header line, the nucleotide sequence, a quality header line (containing only a '+' symbol) and the quality scores for each nucleotide. We have opened the R2 file, because there is an important detail that is not present in the R1 file, which we will come back to later. Note and remember the 'GTATGCCG' sequence string at the end of the sequence header line. 4.3 Quality assessment We’ll run this raw sequence data through fastQC, a tool for conducting quality assessments on Illumina data, to summarise the data quality. First, make a directory for the output. mkdir fastqc_output Now we can run fastQC. Note: The command below (and others in the workbook) can be run over one line by excluding the \\ (a bash escape) Alternatively, you can type \\ (with a mandatory preceding space) during a command and then press the enter key. The next line on the command line will start with '&gt;'. This will allow you to continue typing the command on the line. This can be used to type one command over multiple lines, which is useful for clarity and for long commands. Please ask if you would like a demonstration of the bash escape. fastqc -o fastqc_output \\ Lib1_001_R1.fastq.gz Lib1_001_R2.fastq.gz Normally, when you run a command it will run the process in the foreground. This means your command line will be locked while the process is running. You will only be able to type a new command in the terminal once the foreground process is finished. This is normally wanted, but not always - for example, when you run a process like firefox. To run a process in the background, so that you can type and run new commands, you can add the symbol &amp; to the end of a command. We will use this for running firefox. Please follow this link for more information on foreground and background processing. Using firefox, have a look at the output html reports and answer the questions further below. To look at the R1 fastQC output: firefox fastqc_output/Lib1_001_R1_fastqc.html &amp; You may see a warning message on your terminal, but you can press enter to continue to type in the terminal. To look at the R2 fastQC output: firefox fastqc_output/Lib1_001_R2_fastqc.html &amp; How many total reads are there in each of the files? 53,260 165,017 250,000 What is the length of the reads? 75 100 125 151 In the read 2 file, at what base position does the quality of all the reads go below 28 (i.e., the position where not all of the box plot is in the green)? 104-105 114-115 124-125 It does not Are there any over-represented sequences or sequences with adapter content? Yes No In this case, the reads seem to be of generally very high quality. This is likely as these reads are still relatively short, considering the Illumina sequencing kits used. Longer sequences can be of poorer quality, especially towards the end of the reads, and reads from the sequencing of shorter fragments may contain adapter sequence. Unfortunately, data from a sequencing run may often not look so nice and will require quality trimming and filtering. For most datasets: The quality decreases towards the end of the reads The R2 reads have poorer quality than the R1 reads Depending on what has been sequenced and the sequencing kit used, the reads may be a range of sizes compared to being the same size. However, most of the reads will be towards the long end of the range. Even if data does look very good, we would carry out quality control to remove any poor data masked by the high-quality data and to remove any adapter sequences. The application we will use in the next chapter has an internal quality filter. However, it is good practice to work through QC to get a feel for what you may lose from your data set prior to processing it further. This may put your mind at ease, indicate that you may need to proceed with caution or that you should perhaps revisit your libraries if this was a pilot sequencing run. 4.4 Quality Control Quality control generally comes in two forms: Trimming: This involves cutting off bits of sequence. This is typically in the form of trimming off low quality bases from the end of reads, and trimming off adapters or primers at the start of reads. Filtering: This occurs when entire reads are removed. A typical occurrence of this is when a read is too short, as we do not want reads below a certain length, or if the read has an excess of uncertain or poor quality base-calls. To carry this out, we are going to use trimmomatic. To run trimmomatic with the above sequence data we will use the command below. trimmomatic \\ PE -phred33 \\ Lib1_001_R1.fastq.gz Lib1_001_R2.fastq.gz \\ Lib1_1_out_paired.fastq Lib1_1_out_unpaired.fastq \\ Lib1_2_out_paired.fastq Lib1_2_out_unpaired.fastq \\ ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 \\ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:30 MINLEN:50 The parameters in the bash script are: PE: Our input data consists of paired-end reads. -phred33: Type of Phred quality encoding. In this case, it is phred33 as our fastq files (like most Illumina data) use the Sanger encoding. Next, the raw input forward (1) and then reverse (2) read files that we want to perform quality control on are specified. On the next line, the output files for read 1 are specified, first for paired reads and second for unpaired reads. Unpaired reads come about when one read from a pair is filtered out but the other one is not. We normally ignore this unpaired file after the trimming. However, we must specify this for trimmomatic to run. Next, the output files for read 2 are specified, first paired then unpaired. ILLUMINACLIP: These settings are used to find and remove Illumina adapters. First, a fasta file of known adapter sequences is given ('TruSeq3-PE.fa'), followed by the number of mismatches allowed between the adapter and read sequence and then thresholds for how accurate we want the alignment between the adapter and read sequence to be. The 'TruSeq3-PE.fa' file is provided in your 'raw_data' directory. LEADING: The minimum quality value required to keep a base at the start of the read. TRAILING: The minimum quality value required to keep a base at the end of the read. SLIDINGWINDOW: This specifies to scan the read quality over a given window (4 bp here), cutting when the average quality is below a certain Phred score (30 here). MINLEN: This specifies the minimum length of a read to keep; any reads shorter than 50 bp are discarded. Once the command is run, it will give you some summary output to the screen. Based on this output, approximately what percentage of paired reads were kept? 95 66 15 5 4.5 Post QC Check To see the impact of our quality control, we will run fastQC on the trimmomatic output, specifically the two paired read files, and send the output to the 'fastqc_out' directory we created earlier. fastqc -o fastqc_output \\ Lib1_1_out_paired.fastq Lib1_2_out_paired.fastq We can open the fastQC output with the following commands for the R1 and R2 files: firefox fastqc_output/Lib1_1_out_paired_fastqc.html fastqc_output/Lib1_2_out_paired_fastqc.html &amp; To see if the quality of the reads has improved and to evaluate what we have lost, let’s answer the questions below and compare these to the answers to the questions about the fastQC raw read assessment above. How many total reads are there in each of the files? 53 260 165 017 250 000 513 778 What is the length of the reads? 35-76 50-76 75-100 50-125 What is the average GC content (%) of the R2 reads? 40 43 45 50 Some things to note: There is now a range of read lengths. This reflects the trimming of regions of poor quality at the ends of reads through the sliding window. Depending on the downstream application, you may need to truncate reads to an equal length. We won't do that here, as we can still construct loci with fragments that are overlapping between the R1 and R2 reads. Shorter reads may not overlap, but we may still be able to \"stack\" them within a RADlocus to assemble alleles and loci (see the next section). The number of reads has significantly decreased due to quality control. This is expected, but will need to be taken into account. If you do not have enough reads to create sufficient depth or coverage to confidently reconstruct loci and call variants, you will need to be less stringent with the quality filtering you implement in the next section. What is the minimum number of reads required for downstream analysis? Unfortunately, this depends on what you intend to do with your sequence data. For genome assemblies, the general rule of thumb is you want at least 20 times as many bases in the reads as would be in the final assembly (based on the genome size of the organisms). For calling SNPs, you need less coverage of the genome, but you do want a sufficiently large enough number of reads of the same RADtag to be able to confidently reconstruct your loci and genotype your samples. R2 quality is still slightly worse than R1 quality towards the end of the reads. Again, this is normal, and is mostly an artifact of the Illumina chemistry. You could be more stringent, but you may be removing too many reads (including potentially good forward reads). In this section, we ran fastQC and trimmomatic on a single set of paired read files. The output of a sequencing run on your library may produce several paired read-files, as would the sequencing of multiple libraries. You may wonder how you would run fastQC and trimmomatic on multiple sets of fastq files. The use of wildcards and loops to accomplish this is explained in the Appendix. "],["05_Preparing_data_with_Stacks.html", "Chapter 5 Preparing data in Stacks 5.1 Stacks 5.2 Demultiplexing data with process_radtags 5.3 Building the catalogue loci with denovo_map.pl 5.4 Preparing a sequence alignment file in populations", " Chapter 5 Preparing data in Stacks 5.1 Stacks Stacks is a very popular and widely-used application for building loci from short-read sequence data, identifying Single Nucleotide Polymorphisms (SNPs) and genotyping individuals for population genomic and pedigree/relatedness studies, as well as for building linkage maps. It can also prepare SNP data for phylogenetic analyses, as we will do here. Stacks was principally developed for Illumina RADseq data, but can support other GBS or re-sequencing data sets, and (with a little manipulation) data from other platforms. It incorporates many elements of the typical tools (like bwa, SAMtools, bcftools and vcftools) used in other SNP/variant calling pipelines in a simple, single-access application. It also offers the choice of assembling loci using a reference genome, through a de novo assembly procedure (which we will use below) or through a hybrid of the two. We will use various subsets of the data from the Hosegood et al. (2020) paper to demonstrate demultiplexing the data produced by the Illumina sequencing run, reconstructing loci and identifying variants, and preparing the SNP data for phylogenetic analyses. 5.2 Demultiplexing data with process_radtags We will take a quick look in the directory we worked in previously to confirm that the files we need for Stacks are there: cd ~/phylogenomics # List the files that are present ls -l raw_data/ Along with the output you generated in the previous section, you should still have the two sequence data files, the read 1 and read 2 ('Lib1_001') files with '.fastq.gz' extensions, and the barcode file ('manta_samples.barcodes'; see below), an important file for this section. process_radtags is used to demultiplex the individual sequence reads from a sequence data set containing pooled samples and assign these reads to individual samples. It performs QC at the same time, removing low quality sequence reads, Illumina adapters, reads where the restriction enzyme cut-sites are not detected, and reads with any uncalled/ambiguous bases. process-radtags requires a barcode file to demultiplex the sequences, identifying the individual samples through a unique barcode or index (or combinations of barcodes/indices), which are either incorporated at the start of the sequence read or annotated in the sequence header. Let's have a look at the barcode file for the sample data we want to demultiplex. less raw_data/manta_samples.barcodes This is a tab-delimited file, with two columns listing our barcodes or indexes and the samples identified by these barcode combinations in a third column. Our samples are (mostly) identified by numbers here, but you could use any name or alphanumeric identifier. If you had additional combinatorial barcodes, there will be additional columns for any additional barcodes. You could also just have a single barcode or column. For your own study, you may have many different sequence pools coming off the Illumina platform and you will need to compile barcode files to demultiplex each of the sets of sequences further to get the data for each of your samples. Press 'q' to exit the less view and return to the command line. Before we run process_radtags, create a directory to receive the demultiplexed data: mkdir working_data We can now execute process_radtags. The command below contains arguments specific to the way this library was indexed. The Stacks website and manual provide detailed examples of code for dealing with various indexing schemas and barcode combinations for single- and paired-end reads. In this case, the library was indexed and each sample is identified using a combination of two different barcodes, which are included in the P1 and the P2 adapters, respectively. The barcoded P1 adapter is appended to the start of the sequence that becomes the R1 read and the barcoded P2 adapter to the start of the R2 read. However, the full sequencing output has already undergone one demultiplexing step, using the P2 barcodes, as they came off the sequencer, and these read files and samples were all indexed with the same P2 barcode. This barcode is now included in the sequence index line (sequence header) in the fastq file, as we noted in the previous chapter. We can also inspect the sequence data itself to see the barcodes. zcat Lib01_001_R1.fastq.gz | grep &quot;TGCA&quot; I this command, we are using zcat as before, piping the output and then using grep (for \"grab regular expression\"), which is essentially a search function that will pull out each line in the file that contains our search term TGCA. This is the enzyme cutsite overhang for the restriction enzyme SbfI. This overhand is where the the P1 adapter is ligated to start the R1 read. The cutsite should may be bolded or in colour in your output. Most of the matches will be near the start of the sequences, but there may be other random nucleotide stretches \"deeper\" in the sequences. The nucleotides to the left of those near the start are our barcodes and those to the right are the sequence data of the RADtag fragment. The following command will demultiplex these read files: process_radtags -1 raw_data/Lib1_001_R1.fastq.gz \\ -2 raw_data/Lib1_001_R2.fastq.gz \\ -P -i gzfastq \\ -b raw_data/manta_samples.barcodes \\ -o working_data/ \\ --inline-index --renz_1 sbfI --filter_illumina \\ -c -q -r In this instance, the arguments used are: -1 and -2 specify the path to and the names of the raw R1 and R2 sequence data files, respectively. -P indicates that these are read files from paired-end sequencing. -i denotes the input file type. If left out, Stacks will guess the input file type and will use gzipped fastq ('.fastq.gz') as the default if it can't. -b: This specifies the path to and the filename of the barcode file. -o specifies the path to output the demultiplexed data. --inline_index indicates that the barcode used to identify the samples is situated in the sequence itself (inline) at the start of the R1 reads (from the P1 adapter), and in the fastq sequence index (sequence header) in the R2 reads (from the P2 adapter). --renz_1 defines one of the restriction enzymes used (SbfI), which will have the cut-site overhang sequence from the P1 adapter on the R1 reads. Ordinarily, one would also specify the second restriction enzyme (SphI) used in the double digest for the cut-site overhang for the P2 adapter with --renz_2, but we will not do this here, as this enzyme cut-site has already been removed, along with the barcode and Illumina adapters in the first demultiplexing stage. -c cleans the data, removing any reads with uncalled/ambiguous bases. -q: This is a quality filter and discards reads below a given sequence quality (Phred) score. -r: This is an option to rescue sequence reads and avoid them being removed by allowing a specified number of nucleotide mismatches (the default is one) between the specified barcodes and those observed in the reads or between the observed enzyme cut-sites (overhangs) and those expected. --filter_illumina removes reads that were marked as 'failed' by Illumina's internal quality filter. WARNING: The different modules and versions of Stacks can be little tricky or inconsistent with respect to whether one needs a full path to the specific files (like here) or whether you could just specify a path to a directory (-p) containing your data. It is not always as straightforward as the manual suggests. If this does not work in your own analysis, try specifying the paths to the data in other ways. Often it is easiest to just work in the directory in which your data are located and direct the output elsewhere. Execute the script (it should take about a minute to run), and let's consider the terminal output. How many reads were retained and assigned to individual samples? 40739 61572 392829 500000 What percentage of reads where not assigned because the RAD cutsite (the correct overhang for enzyme SbfI) could not be found? 0% 1.1% 8.0% 12.3% What percentage of reads failed to pass the quality filters and were discarded? 0.0% 0.4% 8.0% 13.0% What percentage of the (500 000) reads were not assigned because the barcodes in the sequence could not be matched to any in the barcode file? 0% 1.1% 8% 13.0% Why do you think these barcodes were not found? Remember that many (up to 96) individuals were multiplexed and included in this pair of read files. We have only specified a few (25) of the potential barcode combinations to identify samples, in order to keep the processing time short for the workshop. We have also thinned the data set. We can peek into the 'working_data' directory to see what files were produced, and also have look at the process_radtags log, which will give us more detailed information on how the reads were demultiplexed. # List the contents of the &#39;working_data&#39; subdirectory ls -l working_data # View the contents of the &#39;process_radtags&#39; log less working_data/process_radtags.raw_data.log For each sample (ours are identified by a number), four gzipped fastq sequence files were produced. The 'Sample'.1.fq.gz and 'Sample'.2.fq.gz files contain the paired forward and reverse reads, respectively, that were assigned to that sample. The 'Sample'.rem.1.fq.gz includes the forward reads that were assigned to that sample, but the matching reverse reads could not be assigned or were discarded because of quality issues, and vice versa for 'Sample'.rem.2.fq.gz. We normally would not need these 'remainder' files again. The process_radtags log expands the summary that we saw in the terminal and indicates at a sample or barcode-combination level how many reads were retained or lost/unassigned because of quality or mismatched barcodes and enzyme cutsites. It also lists and counts potential barcodes that were observed in the data, given the mismatching limits we specified, that were not listed or assigned in the barcode file. If you need to demultiplex several pairs of read files, as you typically would in a RADseq study, the Appendix gives some guidance on how to do this with loops. With our sequence data demultiplexed and assigned to samples, we can now begin to reconstruct loci and call our SNPs. 5.3 Building the catalogue loci with denovo_map.pl We'll use the Stacks pipeline denovo_map.pl to reconstruct loci, identify variants and prepare data for downstream analysis. This pipeline combines several of the core modules of Stacks in a single execution. These modules are 'ustacks', which \"stacks\" up sequence reads to identify alleles and compile loci in individual samples, 'cstacks', which combines the loci from each of the individuals into a catalogue of loci for all the included samples, 'sstacks', which refers the samples back to the catalogue to identify shared loci, and 'tvs2bam', which then transposes the data from being arranged by sample to being arranged by locus. It then uses 'gstacks' to create contigs from paired-end reads, call SNPs and, importantly for phylogenetic applications, compile and combine the SNPs into haplotypes. The 'populations' module is used to generate some basic population-level summary and diversity statistics, which are probably more relevant for population genomics, but this module is also used to prepare files for other applications (which we will do below), and is included in the pipeline by default as a filtering tool and as a controller for which samples are processed down the pipeline. For demonstrating denovo_map.pl, we'll use a subset of five samples from the Hosegood et al. (2020) study. These have already been demultiplexed and quality-filtered in process_radtags. Navigate to the directory where this data are located. cd ~/phylogenomics/demultiplexed_data List the files that are present: ls -l You should see 10 fastq files, the R1 ('.1.fq.gz') and R2 ('.2.fq.gz') reads for each of five samples, and a 'popmap' (populations map) file ('5samples.popmap'). Have a look at the 'popmap' file: less 5samples.popmap This is also a tab-delimited file, with the names of the sequence files (without the R1, R2, 1 or 2 indicators, and without file extensions) of the samples that we want to analyse in the left column, and a second column on the right for assigning these samples to populations or species. These assignments are important for the calculation of statistics in the 'populations' module, defining units for downstream analysis and implementing data filtering options. Here, the population map contains five individuals, each representing a different species of Mobula. We can now create a directory to receive the output... mkdir ../denovo_map ...and execute the pipeline: denovo_map.pl --samples . \\ --popmap 5samples.popmap \\ -o ../denovo_map/ \\ -T 8 -d \\ -M 3 -n 3 \\ -p 1 -r 1 \\ --paired Was this successful? Did it look like all the expected information appeared as output in the terminal? Let's take a look at the arguments we used in this script: --samples specifies the path to where the sequence reads are located, with . indicating the present directory. --popmap specifies the path to and the name of the 'popmap' file. -o: This is the path to direct your output to. -T specifies the number of threads to use. -M and -n are parameters used to specify the maximum number of mismatches (base differences) allowed between sequence reads when building loci within samples in 'ustacks' (-M) and between individuals when merging loci in the catalogue (-n) in 'cstacks'. These are not arbitrary and need to be optimised for each data set. An overview of one optimising procedure is given in the Appendix. We are using the parameters that Hosegood et al. (2020) optimised for the full data set. -p is a data filtering option and specifies the minimum number of populations that a locus must be present in in order to be included and be processed. To maximise the number of loci retained in this initial pass (we will filter this further below), we are specifying a minimum number of one population. Note: each individual is a unique taxon in this analysis and we don't have \"populations\" per se, but in terms of the terminology each individual here is regarded as a different 'population'. You could well have multiple individuals representing a species or 'population'. Depending on your objectives you may choose to treat your samples individually as different terminal representatives of the same species or collectively. -r: Another data filtering option, specifying the minimum percentage (as a float) of individuals in a population that a locus needs to be present in to be retained. We have set this at 100% here, as we only have one individual in each of our populations and we don't want to exclude any of these. --paired assembles contigs for each locus from the paired-end reads after compiling the RADseq loci. Importantly, - -d: 'Dry run' gives you the option of checking whether the command will execute without actually processing any of your called data, allowing you to check for typos, incorrect paths or any other errors. denovo_map.pl is the most time-consuming of the processes we can run in Stacks, and it is good to see that it is working properly before committing time to it. You can check the 'denovo_map.log' for errors and issues, although the output to the terminal should have indicated these. less ../denovo_map/denovo_map.log Now, remove -d from the code above and execute it again. Remember that you can use the up key to scroll through your Linux history to save you from retyping code. Now would be a good time to go make yourself a cup of tea; it will take Stacks about 12 minutes to take the set of sample data sequentially through the different modules. As each module executes, a report for each sample will be sent to the terminal. Once this is done, we can have a look at the files that denovo_map.pl produces. ls ../denovo_map For each of our samples, four '.tsv.gz' (tab-delimited) output files are produced. These contain: 1) The alleles that are constructed from each RADtag locus ('XXX.alleles.tsv.gz'), 2) The assembled loci for each sample ('XXX.tags.tsv.gz'), 3) The loci that were matched to the catalogue ('XXX.matches.tsv.gz'), 4) and the final SNP genotype calls ('XXX.snps.tsv.gz'). You can consult the Stacks manual to see exactly what information is contained in each. There are also individual log files for each of the Stacks modules, which are synthesised in the 'denovo_map.log'. There are a few things that we would want to note or consider from this output. These may determine how we proceed with our analyses. A few MCQs will guide you through these. less ../denovo_map/denovo_map.log What is the lowest depth of coverage for our samples? (Scroll to the bottom of the ustacks section) 231.19x 140.44x 99.05x 45.78x 37.49x As a rule of thumb, we would like at least 30x depth coverage for SNP genotyping in a typical spatial population genomics study and higher coverage for the accuracy needed to build high-resolution pedigrees. For a phylogenomic study, this would also depend on the taxonomic scope and sampling. The coverage in this case is excellent for these samples. How many loci are in the catalogue? (Scroll to the bottom of the cstacks section). 5612 27619 52291 75125 These represent the total number of loci present across all five samples, before one starts filtering to see which are shared among different sample combinations. With more divergent taxa added to the set of samples, we may see many more loci in the catalogue (depending, of course, on our sequencing yield and coverage), but we may see a large attrition of data that can actually be informative. 5.4 Preparing a sequence alignment file in populations We'll use the 'populations' module of Stacks to produce Phylip files for downstream analysis. This data format was developed for the programme PHYLIP, and is a common format for all forms of phylogenetic data and accepted by many applications. While the 'populations' module and the filtering and file-creation arguments can be included directly in the denovo_map.pl bash script above, we'll use the 'populations' module separately to demonstrate the effect of two different filtering strategies. The choice between these two (or of any other along a continuum) will depend on your study, research questions, taxonomic sampling and the data itself. In the first instance, we will minimise the missing data in our data matrix by only including SNPs that are found in all our taxa. The script is as follows: populations -P ../denovo_map/ -M 5samples.popmap \\ -t 8 -p 5 -R 1.0 \\ --write-single-snp --phylip In this command, -P indicates the path to where the files were saved by process_radtags.pl above. -M points to the population map. -t: The number of threads to use for parallel processing -p and -R are the parameters used for data filtering (similar to that above), where -p (an integer) is the minimum number of populations that a SNP locus needs to be present in to be included, while -R (a float) is the minimum percentage of individuals across the full sample that a locus must be present in for it to be maintained. A -p of 5 and a -R of 1.0 (100%) will ensure that the retained loci are present in all five of our taxa. write-single-snp is called to prevent linked loci from being included in the data set. Multiple SNPs on the same RADtag or locus are much more likely to be linked than to not be. This command will retain only the first SNP from each RADtag. You can also use write-random-snp to choose a random SNP from each fragment, but this may make exact replication of your analysis difficult. --phylip specifies the output format. This particular argument will include SNPs in the haplotype that are fixed within populations (i.e., species here), but variable between them. Other options allow for including sites that are variable between, as well as within populations (--phylip-var), or all sequences and variable sites (--phylip-var-all). Take a look at the Phylip file that was produced. An explanation and example of the Phylip format can be found here. less ../denovo_map/populations.fixed.phylip How many SNP characters are in the data matrix and present in all five taxa? 5 1677 2517 8435 In the second scenario, we'll attempt to maximise the number of loci in the data matrix, but this could introduce a lot of missing data: populations -P ../denovo_map/ -M 5samples.popmap \\ -t 8 -p 2 -R 0.4 \\ --write-single-snp --phylip Here, we have specified that a locus only needs to be present in two individuals or species (-p 2) and in 40% (-R 0.4) of the overall sample (i.e., those two individuals) to be included. Once executed, open the Phylip file. You can use the same code as above, as the previous output will be overwritten. How many SNP characters are included in the data matrix now? 5 1678 2517 8436 Now that we have compiled the Phylip data matrix, we will proceed with generating, investigating and further interrogating phylogenetic trees. If you are interested in exploring SNP identification, variant calling, genotyping, variant filtering and subsetting further, consider registering for the NEOF Population Genomics course. Please visit the NEOF Training website periodically to see what courses are on offer. "],["06-Phylogeny.html", "Chapter 6 Model selection, tree reconstruction and support evaluation 6.1 Estimating a best-fit model of nucleotide or sequence evolution 6.2 Constructing a maximum-likelihood phylogenetic tree 6.3 Calculating nodal support", " Chapter 6 Model selection, tree reconstruction and support evaluation We will use IQ-TREE to construct and evaluate a phylogenetic tree. IQ-TREE is a popular package that uses a very fast and efficient algorithm to infer phylogenomic trees under a maximum-likelihood framework. It also incorporates modules for evaluating and selecting models prior to tree reconstruction, and for bootstrapping to determine nodal support. These modules, ModelFinder, for evaluating models, and UFBoot for evaluating support, are substantially faster than many alternative, stand-alone packages, such as jModelTest or the bootstrapping procedure that is used in RAxML, another popular and widely-used, likelihood-based, phylogenetic-reconstruction package. The latest version, IQTREE-2, can be installed in Linux, Windows and Mac systems, and offers a range of new tree-search options and sequence evolution models. There is also a IQ-TREE web-server to which jobs can be submitted. Navigate to the 'iqtree' directory: cd ~/phylogenomics/iqtree We will produce a phylogenetic tree of ten species of manta- and devil-rays, including two representatives of each of eight Mobula species and two former Manta species, and two Rhinoptera bonasus individuals as outgroup representatives. The data were produced exactly as in the previous chapter, following the second approach in which we maximised the number of SNP loci included. We are deliberately including a sparse data set (which many deeper phylogenetic data sets are likely to be) in order to potentially introduce uncertainty into our reconstruction and support evaluation. If we were to use the first approach and eliminate all missing data, we would have been left with only 307 characters to analyse, opposed to 1 678 from the previous exercise! This demonstrates quite nicely how the number of RAD loci and SNPs that are common to all samples in the Stacks catalogue decreases and missing data increase when one adds more distantly-related species to a dataset. In this case, we have added more species, representatives of species that previously belonged to another genus, Manta, and a more distantly-related outgroup. Take a look at the 'popmap' used to generate this data set to get a feel for the individuals and taxa included. less phylogeny.popmap Check that you have the Phylip file ('manta_phylogeny.phylip') that we will use for the analysis. ls -l You can take a look at the header of this file to see the dimensions (taxa X characters) of the matrix. head -n 1 manta_phylogeny.phylip We will now work through the various steps of tree construction. Although many of the steps can be combined in a single command and IQ-TREE analysis, we will estimate a best-fit model of nucleotide-evolution, construct a phylogenetic tree and evaluate bootstrap support separately in order to pause and consider the procedure and results. 6.1 Estimating a best-fit model of nucleotide or sequence evolution \"All models are wrong, but some are useful\" --- George E. P. Box IQ-TREE supports and tests the fit of well over 100 variations of models of sequence evolution, including traditional rate-matrix (base substitution) base frequency models and newer Lie Markov models. It can also accommodate advanced partition models and mixture models, should these be needed for your data. To evaluate and choose a model, we will use this code: iqtree -s manta_phylogeny.phylip -m MFP In this, - -s denotes our data input file. IQ-TREE can support sequence alignments in PHYLIP, fasta, NEXUS and a number of other formats. - -m indicates that we wish to use ModelFinder (MFP) to find the most appropriate model. This analysis should take about two minutes to run and will log useful output to the terminal. This will also be saved as a log file ('manta_phylogeny.log'). For each of the evaluated models, it reports the log-likelihood (-lnL; the likelihood of observing the data given this particular model and the topology of a preliminary tree constructed using parsimony), and the Akaike Information Criterion (AIC), the corrected Akaike Information Criterion (AICc) and Bayesian Information Criterion (BIC) scores used in comparing the models. Our choice is based on the BIC, with the model minimising the BIC favoured. You can set this if you want to use one of the other criteria. Once IQ-TREE has identified the optimal model, it will generate a number of of candidate trees to fit, and then estimate and optimise the parameters of our chosen model, which is the final output in the terminal. Open the log-file and scroll through the output: less manta_phylogeny.phylip.log Which is the optimal model for this data set, as selected by the BIC? K2P+ASC TVM+F+ASC An overview of the various models of sequence or nucleotide evolution can be found here. At the base are a number of simple or straight-forward models (with very few parameters), but these are made increasingly more complex as more parameters are allowed to vary or are added. Older software packages only implemented simpler models, but with increasing computing power, we can now make the models a lot more complicated and specific. This is often needed for our larger and more complex data sets. The model chosen here is a simple two-parameter model. The base frequencies are all equal (at 0.25), and we have differing substitution rates (1.000 and 5.574) for transitions and transversions. In this instance, a correction for ascertainment bias (the '+ASC') has also been included. SNP data (like morphological data) are biased in that we only include variable data and have no constant sites or characters in our data sets. This needs to be considered when calculating and evaluating tree statistics and branch lengths. 6.2 Constructing a maximum-likelihood phylogenetic tree We will now construct a maximum-likelihood phylogenetic tree, implementing the chosen model and the parameters we have estimated. iqtree -s manta_phylogeny.phylip -m K2P+ASC -o Rhinoptera_bonasus1 -redo In this command, - -s again refers to the input data file. - -m invokes the model we want to use. We can specify MFP if we want to search for a model and construct the tree in one command. We have already performed this step, so here we will just specify the model that was selected above (K2P+ASC). - -o: We use this to specify our outgroup, ' Rhinoptera_bonasus1 ' in this case. If you do not specify an outgroup, IQ-TREE will regard the tree as unrooted, but will use the first taxon/entry in your alignment to root it for display purposes. - -redo: This is a neat safety feature built into IQ-TREE to prevent you accidentally overwriting previous work. If you have run analyses on a particular data file and generated log and output files before and you execute a command without -redo, it will issue an error warning and not execute. You are welcome to try the above command without -redo to see what happens. The report and output of the analysis will be written to the output file 'manta_phylogeny.phylip.iqtree'. You can open this to see the summary of the data and the parameters that were used, the tree statistics (likelihood score and length) and the tree itself. You may need to resize your terminal window or it might spread each branch of the tree over multiple lines of the terminal and make it hard to interpret. If this happens, resize your window and execute the command again. less manta_phylogeny.phylip.iqtree The tree is also presented in Newick format at the bottom of the file. The parentheses (brackets) and commas indicate the relationships and branching patterns in the tree, while the numbers indicate the branch lengths, as proportional mutations along that branch to the next node or to the taxon (at the tips). Newick annotation is a widely-used format and is accepted by most phylogenetic applications that would need a tree topology as input, and there are several editors (e.g., FigTree) available to manipulate tree topologies in this format, and to save the tree graphically for editing and publication. A separate Newick treefile ('manta_phylogeny.phylip.treefile') is written by IQ-TREE and can be used for this purpose. A few things are apparent from this tree. As expected, the two representatives of each species group together, except for Mobula japanica (we'll come back to this), but those species belonging to the former genus Manta are nested within the genus Mobula. Let's examine the support for this and other relationships. 6.3 Calculating nodal support We'll use ultrafast bootstrap approximation, developed by Minh et al. (2013) and implemented in UFBoot, to calculate support for our nodes and relationships in the tree. This approach is orders of magnitude faster than non-parametric (i.e., \"traditional\") bootstrapping, is less sensitive to model violations and produces less biased estimates of support. We will execute the command as before, with one addition: iqtree -s manta_phylogeny.phylip -m K2P+ASC \\ -o Rhinoptera_bonasus1 \\ -bb 10000 -redo We specify (with the -bb) the number of bootstrap replicates we want to perform; 1000 is recommended as a minimum. Once complete, we can open the updated 'manta_phylogeny.phylip.iqtree' file to see the resulting tree with the bootstrap support values appended. less manta_phylogeny.phylip.iqtree The interpretation of ultrafast bootstrap approximation support is slightly different to that of traditional bootstrapping, and the support produced by the two methods can't really be compared. We only consider UFBoot support &gt; 95% as being meaningful, whereas non-parametric bootstrap values &gt; 75% are generally accepted as supported (because this metric underestimates support). Nevertheless, our tree shows that most of the relationships are well-supported, including the placement of the two Manta species within Mobula, but we'll test that further in the next chapter. By way of comparison, the figure below indicates the slight difference in support generated by non-parametric bootstrapping. This analysis used only 100 replicates and took nearly 45 minutes on this modest data set, compared to the two minutes it took to perform 10 000 UFBoot replicates. "],["07-Topology_testing.html", "Chapter 7 Hypothesis and topology testing", " Chapter 7 Hypothesis and topology testing In this section, we'll use IQ-TREE to test some alternative hypotheses to the tree we generated in the previous chapter. We can constrain the tree (i.e., move around or swap branches) to represent relationships under any alternative hypothesis that we want to test - statistically - against the most likely tree. These constrained trees can be informed by any taxonomic arrangement, biogeographic scenario or patterns of phenotypic or character evolution. Prior to conducting the analysis, you would need to edit the Newick tree file (or compile a new one), such that the topology reflects that which you want to propose. In most applications, the topology along would suffice; it is very seldom that you would need to propose branch lengths for the relationships that you want to test. This is a relief, because that can be quite tricky, and is done in this analysis anyway (see below) - should you need these later! We have already edited a few trees and placed them in a file to analyse. You can view this file (you should still be in the 'iqtree' subdirectory): less manta_tests.treefile If you would like to challenge yourself, you are welcome to grab a notepad and try to reconstruct the topologies from the notation. If not, these are the topologies we would like to test: The first tree is only a slight modification of the tree we recovered before, in that we are forcing the two Mobula japanica samples to form a clade of their own. In the previous tree, one of Mobula japanica samples was a sister-taxon to the two Mobula mobular individuals, with some support (74%), with the other Mobula japanica individual outside of this; these species are considered synonyms, so this relationship is not that surprising. In the second tree, we are testing whether the former genus Manta could be considered as a separate, monophyletic sister genus to Mobula. In the last example, we are testing a fanciful biogeographic hypothesis, using distribution information retrieved from here. We are proposing that an ancestor with a circum-global distribution gave rise to new species in the Atlantic Ocean, including Mobula hypostoma. One of these, Manta alfredi, with an Atlantic and Indo-West Pacific (IWP) distribution, then gave rise a clade of species with a Red Sea, Indo-Pacific (IP) distribution, including a Mobula munkiana in the Eastern Pacific. Within each of these 'alternative hypothesis' trees, we have tried to keep the relationships, branching order and patterns (other than the obvious rearrangements and constraints) as close as possible to the original tree. When conducting tree topology tests, IQ-TREE will perform RELL replicates (analogous to bootstraps) for a number of the common topology test statistics. These include the bootstrap proportion test, the KH test (Kishino &amp; Hasegawa, 1989), the SH test (Shimoadaira &amp; Hasegawa, 1999) and Strimmer &amp; Rambaut's (2002) expected likelihood weights (ELW) test. You could optionally include various weighted (KH and SH) tests and an approximately unbiased (AU) test (Shimodaira, 2002). We'll include the default array of tests, as well as the AU test, and execute with the following command: iqtree -s manta_phylogeny.phylip -z manta_tests.treefile \\ -zb 10000 -au -te manta_phylogeny.phylip.treefile --redo In this script, - -s opens our data file, as before. - -z: This is the treefile containing the topologies that we want to test and compare. - -zb indicates how many RELL replicates we wish to perform. We have specified 10000, as this is the minimum recommended for the approximately unbiased test. - -au is included to include the approximately unbiased test. - -te specifies a user-defined tree to use to estimate model parameters. IQ-TREE will essentially execute the full model selection and tree reconstruction process anew on each execution. Here, we will just specify the tree we had generated previously to reduce (highly unlikely) chance that another topology would be obtained with a different starting tree for the model selection. - --redo: As before, we release the safety feature. After this has completed running (after about two minutes), we can consult the standard logfile and evaluate our results. less manta_phylogeny.phylip.iqtree Scroll towards the bottom. You'll find the results under the USER TREES subheader. A table will include the log-likelihood for each of the three alternative topologies, the difference in likelihood ('deltaL') compared to our earlier most-likely tree, the RELL bootstrap proportion, the p-values for the KH, SH and AU tests, and the expected likelihood weights (ELW). It is also indicated as to whether a particular tree is statistically excluded as a reasonable alternative to the original tree. Looking at these results: Which tree has the lowest likelihood score? (Keep in mind that these are log-likelihoods which will be negative) Tree 1 Tree 2 Tree 3 Which one, if any, of these trees is an acceptable alternative hypothesis to the original tree? None of them Tree 1 Tree 2 Tree 3 If of interest, you can consult the file 'manta_phylogeny.phylip.trees' to see that branch lengths were appended to each of our three proposed user trees. These are the branch lengths that would maximise the likelihood of that topology, given our data and the model that best fits our data. less manta_phylogeny.phylip.trees In the next chapter, we will attempt to date divergences (cladogenic events) in the tree. "],["08-Dating_phylogenies.html", "Chapter 8 Dating phylogenies 8.1 Setting up the date file 8.2 Running the analysis", " Chapter 8 Dating phylogenies We will use IQ-TREE to date the divergences in our tree. IQ-TREE implements the least-square dating (LSD2) dating procedure and algorithm of To et al. (2016). This approach and IQ-TREE allow us to date the phylogeny using tip dates, e.g., known sampling dates of the terminal taxa as one would have in a study of viral evolution, or by using ancestral dates. This procedure will work as long as you can place a date or age for the most-recent common ancestor (MRCA) on one node of the tree. This date or age can be derived from fossil data or some biogeographic, geological or historical event. 8.1 Setting up the date file For our data and situation, we will use the ancestral dating approach. As a first step, we will need to prepare a date file specifying the date record of the ancestral nodes or at least one node in the tree. This has been prepared already. # Check whether you are in the &#39;iqtree&#39; directory pwd # Open the dating file less manta_phylogeny.dating Again, this is a tab-delimited file. In the left column, we have the list of taxa that share a MRCA at the node with the age - in millions of years before present (hence specified as a negative value) - in the right column. In setting this up, it is important the the taxa names match those in our alignment file ('manta_phylogeny.phylip') exactly. These should be separated by commas and there should be no spaces (as spaces can also be used to delimit fields). Part of the reason for the taxonomic uncertainty in this group is that it has undergone very rapid and recent speciation, and Hosegood et al.'s (2020) results also suggested this. Poortvliet et al. (2015) provided a dated phylogeny, based on mitogenomes, for the manta- and devil-rays, and discussed the fossil evidence. Fossils of extant manta- and devil-rays are recent and only date back to between 11.6 and 3.6 million years ago (Mya). Fossil teeth of Mobula mobular and Mobula japanica date to between 11.6 and 3.6 Mya. The first fossils of Manta appeared 4.8 Mya. Based on molecular and fossil evidence, it is thought that Manta alfredi and Manta birostris diverged within the last one million years. Based on this, we can specify: 1) The age of the MRCA of Manta birostris and Manta alfredi to be 1 million years or to be at least 4.8 million years, and 2) The age of the MRCA of Mobula mobular and Mobula japanica to be at most 11.6 million years. All three cases are included in the date file ('manta_phylogeny.dating'), but we will comment out (using '#') those that we don't want to constraints that we don't want to use. Annotation with # By putting a '#' in front of a piece of code, text or data, we turn that into annotation. This is not read or executed as a command, but is rather a handy tool for making notes in code (such as comments or reminders to ourself as to what a command does, what settings to use or how to change parameters). It is also useful for \"blocking out\" bits of code or data that we don't want to run or use. It is much easier to just remove the '#' when we need to than to try to remember what we deleted before or to have multiple copies of essentially the same script. 8.2 Running the analysis 8.2.1 Dating our tree Starting the analysis in IQ-TREE is straightforward: iqtree -s manta_phylogeny.phylip --date manta_phylogeny.dating --date-tip 0 --redo In this command: -s points to our sequence alignment (as before). --date specifies our date file. --date-tip: This indicates that all our terminal taxa are extant and present at the present date (\"time zero\"). You could optimistically have ancient DNA from an extinct taxon or a mixture of contemporary tip dates and ancestral dates in a viral study. Remember --redo. Our output and log files from the previous chapter are still in the folder. This will run through the full analysis as we did before. It will use ModelFinder to find the optimal model and parameters of sequence evolution and find a maximum-likelihood tree, with the branch lengths representing the number of substitutions per site. For the dating, it will rescale the branches, using our time constraints, to build a time-calibrated tree. Once run, a few results are output to the terminal. These include the 'tMRCA', which is the age of the root of the tree, and a substitution rate at the root rate as the number of subtitutions per site along that branch of the tree. Alongside the typical IQ-TREE output we looked at in the last section, this analysis will also produce three 'timetree' files: ls -l These are: 'manta_phylogeny.phylip.timetree.lsd', which reports the output of the LSD2 process, 'manta_phylogeny.phylip.timetree.nex', the time-tree in a nexus tree format, and 'manta_phylogeny.phylip.timetree.nwk', the time-tree in newick format. We will use a web-browser and IcyTree to examine our time-calibrated tree. Right-click the grey are of your terminal and select Firefox. This will open the web browser. Navigate to 'icytree.org'. Use the File button at the top to navigate to and load your nexus tree. You are welcome to play around with the various functions and style of the tree, but to see the dates of our nodes and divergences, click the Style button, then Internal node text and the Date. Do not close Firefox quite yet. We can now take a quick look to see if changing one of the ancestral nodal constraints or dates changes the remaining dates in the tree. We'll use nano, a Linux text editor, to quickly edit the 'manta_phylogeny.dating' file. nano manta_phylogeny.dating This will open the file in what looks like another terminal within your terminal. You can edit in this as you would a normal text document. Delete the '#' from the second line, and place a '#' at the start of the first line. The age of Manta used in the analysis will now be 4.8 million years rather than one million, as the first line will be ignored. Press 'ctrl + x'. Press 'y' to save the changes. Press 'Enter' to use the existing file name (note the use of annotations rather than saving lots of different files under different names). Now you can execute the analysis again! iqtree -s manta_phylogeny.phylip --date manta_phylogeny.dating --date-tip 0 --redo Open another tab in Firefox, open IcyTree and the latest nexus tree file (it will have the same name) as you did above. Display the dates on the nodes. By flipping between the tabs you can compare the trees to see if there are any obvious or large differences or if the dating was fairly robust to these changes. 8.2.2 Getting confidence intervals around our dates IQ-TREE uses a procedure that resamples and combines a mixed Poisson and log-normal distribution of the relaxed clock model. As it is not published as yet, it is probably best to use this with caution and to keep an eye on the literature. Unless we are at the end of the day or a session. iqtree -s manta_phylogeny.phylip --date manta_phylogeny.dating --date-ci 100 --redo The only code different from the above is the command --date-ci, which specifies how many resampling replicates we want to use (100). We can set the standard deviation of the log-normal distribution, but increasing this can make the confidence intervals wider. We will use the default (0.2), so we will not set --clock-sd here. Once this is run, you can again use IcyTree to display the 95% confidence intervals around the dates we calculated before. Are these wide or our estimated dates quite accurate. They're unlikely to be, given the limitations of the current analysis and the sparse data file. You have now dated your phylogeny, and can use these dates to examine various evolutionary, historical or biogeographic scenarios. In the next chapter, we will return to testing species models, specifically using the Multispecies Coalescent. "],["09-Delimiting_species.html", "Chapter 9 Species delimitation with the Multispecies Coalescent 9.1 beast2 9.2 Gene tree - species tree conflicts 9.3 Bayes Factor Delimitation", " Chapter 9 Species delimitation with the Multispecies Coalescent 9.1 beast2 In this section, we are going to use beast2 ('Bayesian evolutionary analsis by sampling trees') to explore species delimitation and taxonomic hypotheses further through the Multispecies Coalescent (MSC). beast2 is a large, but versatile, package that applies Bayesian approaches in a multitude of different phylogenetic, demographic and species delimitation analyses. Many different add-ons or plug-ins, such as those that we will be using, are developed and maintained separate to the main package. We'll conduct two different analyses, both using the add-on package SNAPP (SNP and AFLP Package for Phylogenetic analysis). More information on the package can be found here. We will also need the plug-in MODEL-SELECTOR. These have already been installed on the webVNC. INSERT A LITTLE BACKGROUND TO THE MSC The application of the MSC used in SNAPP is based on an algorithm developed by Bryant et al. (2012) specifically for biallelic markers that allows the inference of a species tree directly from the markers themselves. It evaluates lineage and allele probabilities on a tree directly, rather than imputing a gene-tree for each marker and then integrating these individual trees, which is computationally-demanding for many genomic data sets. The key assumptions are that the markers are biallelic and the genealogies of each marker are independent (i.e., the markers are unlinked). The vast majority of our SNP markers are be biallelic, and we'll assume they are unlinked, given that we only used one SNP from each RADtag when compiling the data. 9.2 Gene tree - species tree conflicts We'll first use SNAPP to examine gene tree - species tree conflicts or concordance. This will be particularly useful to determine whether taxonomic separation mirrors actual divergence replicated across many markers or whether there are conflicting, alternative relationships from the underlying \"gene trees\". It provides a picture of the uncertainty of the topology of the \"true\" species tree. The underlying conflicts could result from incomplete lineage sorting, hybridization, introgression, horizontal gene transfer, gene duplication or some analytical artifacts. It provides another means to assess the confidence we have in our species tree, our taxonomy (or another hypothesis we are testing) and provides some nice figures for publication or animations for presentations! 9.2.1 Setting up the analysis For this exercise, we'll use a compiled data set from the Hosegood et al. (2020) study, containing a number of samples of Mobula japanica and Mobula mobular, as well as one sample of Manta alfredi. _Mobula japanica is now considered a synonym of Mobula mobular. The data set contains samples from across the distribution of the former Mobula japanica. This choice of taxa is explained in the next analysis. This file has been prepared exactly as we did for the earlier files, excluding all missing data. The alignment has been compiled and saved in the nexus format for use in beast2. The nexus format is another widely-used, cross-platform format for alignments of sequences (or other data sets) for phylogenetic analysis. You can find more information about the nexus format here. There are several packages that will allow you to convert the phylip-formatted file into a nexus-formatted one, and many alignment and analytical packages will let you output your data as either (Stacks is not one of these). It is also quite easy to create the file yourself from your phylip file or an aligned file in fasta format, as long as you know the relevant format and headers, and the dimensions and nature of your data. To take a look at our data in nexus format, you can navigate to the directory where the files for this chapter are located: cd ~/phylogenomics/snapp Then open the nexus file: less mobular_japanica.nex All input for beast2 needs to be converted to an 'xml' file, which will include the data (the sequence alignment), all the model parameters and priors needed for the analysis and the specifications of the run itself. Fortunately, beast2 provides a package - BEAUti (Bayesian Evolutionary Analysis Utility) - to prepare the data and set all these parameters. This is also a package manager for installing the various beast2 add-ons. We'll use this to prepare our input file. Launch BEAUti by typing beauti At this point, you may be asked whether an update to beast2 should be installed. Please choose \"No\" or \"Not yet\". Once open, you should see a graphical interface that looks like this. From File on the taskbar/menu, select Template and then SNAPP. The tabs of the window should change to accommodate our SNAPP input and should be different to the image above, with only four tabs above the window. Now import your nexus-alignment, by selecting File, then Add Alignment, navigating to and selecting the file ('mobular_japanica.nex'). The taxa in our analysis will then appear in the 'Taxon sets' window. On the left is a list of the taxa in our file and on the right (under 'Species/Population') is the arrangement we are proposing with respect to species membership or taxonomy. BEAUti assigns these automatically based on the taxon or sample names in our alignment. You can click the Guess button at the bottom to specify how this assignment is done (i.e., specifying which characters are used as delimiters to parse the name, which parts of the name to use, etc.). Alternatively, Fill down will allow you to edit the text of the 'Species/Population' column directly. For the purposes of getting interesting output from this exercise, we will ignore the current taxonomy. Ordinarily, that would be a sensible scenario to evaluate, but a tree with just two divergent species (the synonymised Mobula japanica and Mobula mobular vs. Mobula alfredi) is not going to be very interesting and it's unlikely that there are many alternative topologies or any evidence of conflict underlying this species tree. Instead, to show the utility of this analysis, we will evaluate a tree with five different species. We will consider M. mobular and M. alfredi as separate species, and we will then regard the different geographic populations of M. japanica to each represent an independent species from the Western Pacific ('WP'), Persian Gulf ('PG') and Eastern Atlantic ('EA'), respectively. Use the Fill down button and assign each of the samples to one of these five species in the 'Species/Population' column. Alternatively, you can create five species of your own and randomly assign samples to these. Don't define more than five species or the analysis may take too long to run. Model parameters need to be set under the 'Model Parameters' tab. Here, we'll set the parameters for v (the mutation rate from allele 0 to allele 1), u (the mutation rate from allele 1 to allele 0) and the coalescent rate. You can leave u and v set at 1 or use the Calc mutation rates button below to estimate the mutation rate. By clicking the Sample button next to any parameter, we choose to have these sampled from the posterior distribution. For the mutation rates, it is not necessary to sample these; we merely need to set the initial values. We do want to sample the coalescent rate from the posterior distribution, otherwise we are assuming equal population sizes. In this version of BEAUti, you should see see little circular icons next to some of the fields; older versions might not have this. These open windows that allow us to set upper and lower bounds on these rates. We'll leave these unset for now, but you may have reason (and the information) to set these in your own analysis. We need to indicate whether our data include non-polymorphic sites. Given the way our data were compiled, there shouldn't be any constant sites. Make sure that the 'Include non-polymorphic sites' box is not flagged/. Also be sure that the 'Mutation Only At Root' box is not checked. This specifies zero mutations except at the root, which will effectively make all the branches coalesce at the root and not within the tree. This is only really used to determine model parameters in some other applications. The 'Prior' tab lets us set the priors for the tree and rates. The choice of priors is always a contentious issue and too often the temptation is to use the defaults. You'll need to consider biologically relevant priors, carefully considering the nature of your data, or alternatively consider whether your data and analyses are robust to the choice of prior. Two important priors need our attention. The tree prior in SNAPP is a Yule prior for the topology and branch lengths of the species tree, which is controlled by a single parameter, lambda. In essence this is the speciation rate, which represents the prior expected 'height' of the tree. By setting a high value, we are providing prior information that we expect more species to be contained in the tree and delimited than with a low value. Generally, we can use fixed values or a broad prior (typically with a gamma distribution). There is a handy Python script 'pyule' for modelling lambda. The rate prior, which is determined by theta, specifies the mutation rate variation over the tree, which is influenced and modeled via ancestral population sizes at each node. If we are sampling any of the priors in our analysis, we need to specify hyperpriors. As in the Hosegood et al. (2020) paper, we will leave the prior settings for lambda, theta, u, v and the settings of the hyperpriors as the defaults. Lastly, we'll set the dimensions and \"instructions\" for the MCMC run in the MCMC tab. In order to keep this example short, we'll use only 100 000 generations ('Chain Length'), keeping every 100th generation sampled from the posterior distribution ('Store Every'). Edit these to reflect this. You can also set the parameters of the 'tracelog', 'screenlog' and 'treelog' by clicking the drop-down arrows further below. Edit the logging frequency of the 'tracelog' and 'treelog' to log every 100th generation to keep this \"in step\" with the actual sampling. Once that is done you can save the 'xml' file by selecting File and Save, giving it a 'mobula_japanica' as a name. The 'xml' extension is automatically added. You can then exit BEAUti by clicking File and Exit. 9.2.2 Starting the analysis Once the 'xml' file is set up, starting the analysis is straightforward. Call up beast2 with the following code and this will launch a graphic interface (like the image below): beast In this, navigate to and Open the file we just saved. The analysis will start on opening the file. If there are results or log-files from a previous beast2 run, you will be asked if you want to overwrite these. Please do. Once started, this SNAPP analysis will take a few minutes to run. 9.2.3 Examining the results The analysis will produce a 'snap.log' and a 'snap.trees' file. The 'snap.log' contains the likelihood and the theta parameters sampled from the posterior distribution at each sampled generation or step. The 'snap.trees' file contains the topologies, branch-lengths and theta estimates from each of these sampled generations. These each represent a potential species tree based on the likelihoods of allele changes and lineage coalescence of each marker through the tree. You can look at the two files with the following commands: # View the SNAPP log less snap.log # Press &#39;q&#39; to close # View the SNAPP trees file less snap.trees As with all Bayesian, MCMC-based approaches, it is important to evaluate the output for stationarity (determining the length of the burn-in, ensuring the MCMC run was sufficient to sample the posterior distribution effectively), convergence or the sufficient sampling of parameters (through the Effective Sample Size or ESS), and, in the case of multiple runs, that there was convergence on the same part of the posterior distrbution. We will not attempt that here; these results may not be particularly well-sampled anyway. However, Tracer is a useful tool to examine these features graphically and manipulate the log and tree files (e.g., specifying and removing the burn-in) for further analysis. We will, however, examine the species tree and the uncertainty around the underlying gene trees by generating a tree cloud or \"cloudogram\" in DensiTree. There is also an overview of DensiTree here. Open DensiTree, with the following command: densitree This will also open a graphic interface, which should look like this: Click File and Load or use the folder icon to navigate to your 'snap.trees' file and open it. The trees will now be displayed. There is a wealth of functionality available with respect to editing the display and \"look\" of the tree cloud and consensus trees, specifying and eliminating burn-in, putting confidence limits around clades. More excitingly, you can view your trees as animation, scrolling through them and building the full tree set from those sampled. The best way to come to terms with the features is to play around with the package. In terms of interpretation: The most popular/common tree is consensus is blue (as are any consensus trees) The next most popular tree is red. The third most popular tree is green. The remainder are all dark green. Any obvious conflicts and differences in topology should be apparent from different branching structures in the tree cloud, and Different branching depths represent difference coalescent events (depending on the markers) at different points in the past. Close DensiTree with File and Exit. Now that we have seen the uncertainty undelying that particular, wildly-inaccurate species arrangement, we will do more rigorous statistical testing of alternative taxonomic models. 9.3 Bayes Factor Delimitation 'Bayes Factor Delimitation (*with genomic data)' or BFD* is used to test various user-defined species boundaries or species-designation hypotheses. It essentially evaluates, within a Bayesian Posterior Probability framework, which assignment of genetic lineages to which species is most plausible. In this analysis, we will propose a base scenario or null hypothesis against which we will test alternative taxonomic hypotheses or arrangements. We will run each scenario and then do a quick \"desktop\" statistical evaluation to compare these. Unlike the exercise in a preceding chapter, where we tested these hypotheses purely on the basis of the tree topology, here we are using the MSC to model coalescent events backwards in time, considering the multitude of gene trees (and potential conflicts) and demography that underlie any given topology. Unfortunately, this analysis can be very time-consuming or limiting. There is a great tutorial available, and its authors do not recommend this analysis for testing more than 20 species. Hosegood et al. (2020) employed an elegant strategy where they focused on each clade in their tree separately, completing the analysis by testing smaller sets of species, but including a few representatives of a sister clade in each such test to consider the \"deeper\" species boundaries. The data we have compiled and used above is of one of these subsets. When analysing your own data, the number of samples has a much greater impact on the analysis time than the number of SNPs, so a good approach would be to include only a few representatives of the species you want to test and gradually add data (samples) to your analysis when it starts to produce reliable and reasonable results. 9.3.1 Setting up the analysis Although we will again use BEAUti to compile our data and run the analysis in beast2 and SNAPP, the implementation of this analysis is slightly different to that above. We will first create a base 'xml' file, which represents the taxonomic arrangement that we want to compare the others to - our null hypothesis. We'll similarly create 'xml' files for the alternative hypotheses. Then, some text editing of these files is required to enable the Path Analysis for the calculation of Marginal Likelihoods. Thereafter, some desktop statistical analysis is needed to compare and select the competing hypotheses. Open up BEAUti as you did above and load the SNAPP template. Open the 'mobular_japanica.xml' file that you created in the preceding section. We will use that as a basis for this analysis, changing a few parameters. For our base scenario, we will use the currently-accepted taxonomy. Change the species names of M. japanica to M. mobular (these species have been synonymised), and change Manta_alfredi to Mobula alfredi in the 'Species/Population' column. The model parameters need to be set up under the 'Model Parameters' tab exactly as you did above. These should not need any editing, but it is worthwhile to check the parameters. The only additional task is to tick the Use Log Likelihood Correction checkbox. This calculates corrected likelihood values in the Bayes Factor assessments of the different species arrangements. In setting the priors under the 'Prior' tab, we'll follow Hosegood et al. (2020) and implement a gamma distribution for the lambda prior. Select this from the drop-down to the right of 'lambda'. These authors also assessed the effect of a 1/X prior; testing alternative priors and seeing how these impact your results is a means of evaluating the robustness of your results and data, as mentioned above. Under the 'MCMC' tab, we'll use the same parameters as above. Make sure the 'Chain Length' is set at 100 000, 'Store Every' at 100, 'Pre Burnin' at zero and 'Num Initialization Attempts' at 10. Under the 'tracelog' drop-down (arrow) change the 'File Name' to 'runA.log', and set the 'File Name' under the 'treelog' drop-down to 'runA.trees'. You can leave the 'Log Every' for each of these set at 100. Save the 'xml' file as 'runA.xml', and exit BEAUti. This file needs some additional text editing to set up the Path Analysis in beast2 to the estimate the Marginal Likelihood. The tutorial instructions for making these edits. This has already been done and the file has been saved here as 'runA.xml' in a directory with the same name ('runA'). Through this editing, the analysis has been set to run 10 path sampling steps with a MCMC chain of 10 000 steps for each. A burn-in of 10% was also specified for analysing the log files. Note that these parameters and the MCMC settings above could be unrealistically low and will need to be explored in a full analysis; we are only using these so that we can complete the analysis during the workshop. Following the same procedure, we need to compile and edit similar 'xml' files for the other species delimitation models or scenarios we want to test in subsequent runs. These, which represent the scenarios tested by Hosegood et al. (2020), are: Run B: Manta alfredi and Mobula mobular are lumped as one species, an arrangement that was proposed before, with Mobula japanica as another species. Run C: A split between Atlantic-Mediterranean and Indo-Pacific individuals. Run D: A random assignment of individuals to two species. These files are already compiled and placed in the respective 'runB', 'runC' and 'runD' directories. 9.3.2 Running the analysis We can now complete the Path Analysis for each of these scenarios. Navigate to the 'runA' subdirectory. cd runA We will launch beast2 from a command line. This is handy as the GUI does not always work for some applications, like this one. The command line also lets you set the number of threads to use, which you can't do in this particular version of the beast2 GUI. /pub14/tea/nsc206/bin/beast/bin/beast -threads 8 mobular_japanica.xml If you use this approach outside of the webVNC, /pub14/tea/nsc206/bin/beast/bin/beast will need to be replaced with the path to the beast executable on your computer or HPC. /pub14/tea/nsc206/bin/beast/bin/beast -threads 8 runA.xml The analysis will take a few minutes to run. Once it is complete, the Marginal Likelihood for this hypothesis will be presented in the terminal. Also note the Effective Sample Size (ESS) scores. What is the Marginal Likelihood for the Run A hypothesis (the current taxonomy)? Note that your estimates may not match exactly. 930.5 953.1 1,856.2 2,236.0 The ESS scores are very low, indicating that these parameters have not converged and are not well-sampled. At a minimum, we would like these to be &gt; 200. This is not suprising given the low sampling by the chains in both the SNAPP and Path Analysis parts of our analysis. We can then repeat this for Run B. # Change directory to &#39;runB&#39; cd ../runB # Start the analysis /pub14/tea/nsc206/bin/beast/bin/beast -threads 8 runB.xml What is the Marginal Likelihood for the Run B hypothesis: Mobula mobular and Manta alfredi are conspecific, excluding Mobula japanica)? 930.5 953.1 1,856.2 2,236.0 Now do the same for Runs C and D. # Change directory to &#39;runC&#39; cd ../runC # Start the analysis /pub14/tea/nsc206/bin/beast/bin/beast -threads 8 runC.xml What is the Marginal Likelihood for the Run C hypothesis: an Atlantic-Mediterranean species and a Pacific species? 930.5 953.1 1,856.2 2,236.0 # Change directory to &#39;runD&#39; cd ../runD # Start the analysis /pub14/tea/nsc206/bin/beast/bin/beast -threads 8 runD.xml What is the Marginal Likelihood for the Run D hypothesis (a random assignment of individuals to two species)? 930.5 953.1 1,856.2 2,236.0 9.3.3 Selecting the taxonomic hypothesis Once this is done, we'll rank our Marginal Likelihoods and use Bayes Factors to evaluate competing hypotheses. This is a small desktop exercise that is done as follows: Sequentially compare the Marginal Likelihood of each alternative hypothesis to that of the null hyopthesis or base model. You could also compare the alternative hyoptheses with each other, if of interest. The Bayes Factor is calculated as BF = 2 x (Marginal Likelihood of model 1 - Marginal Likelihood of model 2) The resulting BF can be interpreted in the following framework: 0 &lt; BF &lt; 2: Negligible difference between the two models, 2 &lt; BF &lt; 6: Positive evidence that one model is better, 6 &lt; BF &lt; 10: Strong evidence that one model is better, 10 &lt; BF: It's decisive! One model is better. Is the current taxonomy the best species arrangement, given the limited data we have used and our very limited analysis)? This is not answered definitively here, because you may have had different likelihoods calculated by the sampling regime in your analysis. Based on the values in the MCQs above, the arrangement by ocean region is a decisively better arrangement than the current taxonomy and the other arrangements are decisively worse. This could well be an artefact of the sampling distribution of the examples we included to represent the current taxonomy. Congratulations! You have made it to the end of the initial part of the workshop. Please feel free to work through additional examples in the supplemental workbook. "],["10-Appendix.html", "A Mamba installs A.1 Mamba installation and environment B Wildcards and loops B.1 A simple wildcard B.2 Loops C Optimising loci reconstruction in denovo_map.pl in Stacks", " A Mamba installs A.1 Mamba installation and environment Mamba is a re-implementation of conda. It is a great tool for installing bioinformatic packages, including R packages, and creating the environment in which to run them. Mamba github: https://github.com/mamba-org/mamba Mamba installation: https://github.com/conda-forge/miniforge#mambaforge Mamba guide: https://mamba.readthedocs.io/en/latest/user_guide/mamba.html To create the mamba environment phylogenomics, identical to the one we have used in this workshop, run the following commands in your bash. You will need to have installed mamba first. #phylogenomics mamba create -n phylogenomics mamba activate phylogenomics #Install the packages you need from from bioconda channel mamba install -c bioconda stacks iqtree beast2 B Wildcards and loops In the main workbook, we ran the quality assessment and quality control on a single set of paired-end reads (the R1 and R2 files) of multiplexed RADseq data. Your sequencing output may consist of many paired read-files. You may wonder if there is a simpler way to run fastQC on multiple sets of fastq files. Likewise, you may wonder how you would go about running process_radtags in Stacks on multiple pairs of files. You really don't want to type the same command multiple times (even using the Linux history). Besides being tedious and time-consuming, it is a recipe for losing focus, and for errors and typos to creep in. Fortunately, there are ways around this - like using wildcards or loops. B.1 A simple wildcard A wildcard is a character that can be used to represent and stand in a 'placeholder' for any number of single or multiple characters. A simple and versatile one is '*'; other wildcards are explained here. This could easily be used in the fastQC example. By executing the command fastqc -o fastqc_output *.fastq.gz we are asking fastQC to process any file that has the 'fastq.gz' extension, with the '*' representing all the possible filename prefixes that are present in the directory. B.2 Loops Another approach is to run a loop which will cycle through each of your samples and run all the commands that you specify. Put simply, the syntax for a for loop is as follows: for &lt;variable name&gt; in &lt;a list of items&gt;; do &lt;run a command&gt; $&lt;variable name&gt;; done The &lt;variable name&gt; is the name of the variable you will use in the do part of your loop. It contains the item in your list the loop is currently using at that point. In the example of a simple loop below, we call this variable f, but you could use any name as long as you use it consistently in the do section. The &lt;a list of items&gt; is anything that returns a list of items, e.g. a list of files in our case. B.2.1 An example of a simple loop Below is an example of a loop, which can be adjusted for your samples, the software and the code you are running. This simplified loop will clearly demonstrate what the first part of the code is doing. Make sure that you are in a terminal window and in the directory ~/phylogenomics/demultiplexed_data. (This directory contains demultiplexed RADseq data in paired read files for five samples). for f in *.1.fq.gz; #This command loops through all files ending in .1.fq.gz do BASE=${f%.1.fq.gz}; #This sets file name as variable &#39;$f&#39;, and also adds it to a variable called &#39;BASE&#39; removing the &#39;.1.fq.gz&#39; extension echo $f; #This prints the file name ($f) to the terminal echo ${BASE}; #This prints the ${BASE} variable to the terminal done By using echo to print the following variables to the console we can see: echo $f: prints the full filename. echo ${BASE}: prints just the start of the filename (without the '.1.fq.gz' extension, as specified in our code) The loop then repeats the commands on the next file in the list and will keep going until it has run through all files ending with '1.fq.gz' in the directory. done specifies the end of the commands in the loop. All commands between do and done in the loop will be executed. In this case we only have five files but this would work with any number of files. This is a useful bit of code as it means we can keep the filename but change the ending/extension of the file in our loops. This is especially useful when a command will produce output with a different file extension, which we would then need to use as input in the next command in the loop. B.2.2 Stacks process_radtags loop example As a more practical example related to the the workbook exercise, we can write a loop to run the Stacks module process_radtags to work through and demultiplex several paired read files and assign sequences to samples, using the code above as a base. for f in *_001_R1.fastq.gz; #This will loop through all the files ending in &#39;_001_R1.fastq.gz&#39; do BASE=${f%_001_R1.fastq.gz}; #Assigns the file name &#39;$f&#39; to a variable called &#39;BASE&#39; and removes the extension &#39;_001_R1.fastq.gz&#39;. This means that you can use this filename to refer to files with other extensions process_radtags -1 ${BASE}_001_R1.fastq.gz -2 ${BASE}_001_R2.fastq.gz -P \\ -i gzfastq \\ -b ${BASE}.barcodes \\ -o processed_data/ \\ --inline-null --renz_1 SbfI --filter_illumina \\ -c -q -r \\ --adapter_1 AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAGACCGATC --adapter_mm 2; done #Ends the commands. Here we had only ran one command (`process_radtags`), but we can loop many together. In the above code, we use the start of the file name that is kept as a variable in '{BASE}' and then specify different extensions to specify the two files of the read pair that we want to run process_radtags on in that iteration of the loop. We also use the start of the filename in '{BASE}' to refer to the corresponding barcode file. For this command to work, you will need to set up a separate barcode file corresponding to each set of reads beforehand, and name it using the filename prefix that is assigned in '{BASE}'. This code can be edited to run other software in a loop. A nice basic intro to Linux loops can be found here. C Optimising loci reconstruction in denovo_map.pl in Stacks As indicated in the main workbook, Stacks offers a de novo pipeline for assembling loci and calling variants (SNPs) in order to genotype individuals in the absence of a high-quality reference genome. There are, however, parameters that need to be optimised beforehand to increase accuracy in reconstruction and to maximise the number of loci prior to running denovo_map.pl on the full set of study samples. The procedure is explained in a book chapter by Rivera-Colon &amp; Catchen (2022). The chapter provides a handy introduction to designing a RADseq study, and using the various features, modules and pipelines of Stacks to analyse the resulting data. It also links through to some handy R-script for visualising and evaluating your data as it passes through Stacks. The are two main parameters that need to be optimised. The first is -M, which is the number of mismatches that ustacks allows when combining stacks (matched sequence reads that represent an allele within an sample) into loci in that sample. If this is set too low, alleles won't be collapsed into loci, and these will be kept apart as separate homozygous loci. If set too high, alleles belonging to different loci will be combined as a single locus. Once loci are assembled in an individual, cstacks combines all the loci from those individuals into a catalogue of loci for the metapopulation. Here, -n, which is the maximum number of mismatches between individual loci and those in the catalogue, needs to be optimised. If -n is too low, loci in the catalogue will be \"undermerged\" (a locus that is homozygous for one allele will not be merged with an orthologous locus that is homozygous for an alternative allele, and these will be retained as two loci). If too high, different loci will be \"overmerged\" and considered the same in the catalogue. Their procedure involves optimising these parameters so that the number of R80 loci (loci present in at least 80% of the metapopulation) is maximised. The denovo_map.pl is run repeatedly, with -M and -n set to 1 and increased in each subsequent iteration (until -M = -n = 10 or 12, depending on the data and biological context). The change (increase or decrease) in the number of R80 loci are then evaluated across the runs. At the optimal value for these parameters, there will be no or negligible change in the number of loci. Above the optimum, the number of R80 loci should decrease again, as in the figure (taken from the book chapter) below. Selecting the samples on which to optimise these parameters is an important consideration. You obviously don't want to optimise using all your data, unless you have limitless time. These samples you choose should be representative of your library or some biological reality (e.g., representatives of different populations for a population genomic study). You'd want samples that are fairly similar in terms of the amount of input data. Choosing samples with high coverage is sensible, as these will lead to the most accurate allele calling. You may be tempted to include those samples with the highest numbers of reads, disregarding coverage, but in this case you may be optimising for loci that are not going to be present in those samples with lower numbers of reads. There is no \"golden rule\" and each data set will be different and subject to its own quirks. It is always advisable to do some exploration here and to remember that your results are always a hypothesis, based on the data and a number of assumptions that you make through the course of the analysis. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
