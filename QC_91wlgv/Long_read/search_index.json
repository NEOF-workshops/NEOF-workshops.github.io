[["01-Long_read_QC.html", "Long read Quality Control Chapter 1 Overview Table of contents", " Long read Quality Control Matthew R. Gemmell &amp; Helen Hipperson 2023-10-30 Chapter 1 Overview This practical session aims to introduce you to Long read Quality Control. Table of contents Data intro ONT background ONT setup Porechop NanoPlot NanoFilt ONT final check PacBio background CLR setup CLR QC CLR filtering CLR QC Appendix This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Intro.html", "Chapter 2 Data 2.1 The data", " Chapter 2 Data 2.1 The data The data for today is in the \"QC_workshop\" directory that you copied into your home directory (\"~\") in the Illumina QC day. Check you have the directory. ls ~ If you do not have the \"QC_workshop\" directory, the command to copy it all to your home directory is below. cp -r /pub14/tea/nsc206/NEOF/QC_workshop/ ~ "],["03-ONT_background.html", "Chapter 3 ONT background 3.1 File formats 3.2 Basecalling", " Chapter 3 ONT background In this section we are going to carry out a quick QC of ONT data using the tool suite NanoPack and the tool Porechop. For this we will use the fastq files. Before carrying out analysis we will cover some background of ONT data. This will include file formats, &amp; basecalling. 3.1 File formats Although we will only be using fastq files for this tutorial it is important to know the other file formats you may encounter when working with ONT data. 3.1.1 Fast5 The raw data from ONT machines comes as Fast5 (.fast5) files. This contains the signal data from the pore which can be processed into more useful files. Fast5 files can contain: Raw signal data Run metadata fastq-basecalls Other additional analyses The Fast5 file format is an implementation of the HDF5 file format specifically for ONT sequencing data. For more information on Fast5 and a tool to interface with these types of file please see: https://github.com/nanoporetech/ont_fast5_api 3.1.2 Summary file The MinION and GridION output a single sequencing summary file in addition to the Fast5 files. This file contains metdata which describes each sequenced read. We will not use Fast5 or summary files for this tutorial as they are not needed most of the time. It is likely these files will not be initially provided to you by a sequencing service as they are very large. However, if you do require them you can always ask but be careful with how long the sequencing centre will retain your data. 3.1.3 BAM file BAM files (.bam) are the binary version of SAM files (.sam). SAM stands for \"Sequence Alignment/Map\" (the B in BAM is Binary). SAM files are tab delimited and contain alignment information. BAM files are not human readable whilst SAM files are. SAM files are larger than BAM. Generally programs that can work on SAM files can also work on BAM files. Due to the size difference it is preferable to store data in BAM format over SAM. Even though a BAM file is smaller than its matching SAM file, BAM files are still very large and can be &gt;100GB. It can be useful to contain unaligned reads from sequencing machines (e.g. PacBio and ONT) in BAM files as they can contain more metadata in the header and per-record auxiliary tags compared to fastq files. If you are working with SAM/BAM files the following link will prove useful: https://www.htslib.org/ For more information on the SAM and BAM format please see: https://samtools.github.io/hts-specs/SAMv1.pdf 3.1.4 fastq file The fastq file format is very consistent and so there is no real difference between fastq files for Illumina, PacBio, and ONT data. All fastq files will contain a number of lines divisible by 4. This is because each entry/sequence will take up four lines consisting of the following information: Header for fastq entry known as the fastq header. This always begins with a '@' This is where you might see the most difference between different data. Different machines and different public databases will use different formats for fastq headers. Sequence content Quality header Always begins with a '+'. Sometimes also contains the same information as fastq header. Quality Each base in the 2nd line will have a corresponding quality value in this line. Note this uses Phred encoding, of which there are different formats. Most, if not all, new files will use Phred+33 but be careful if you are using older data as it may use a different one. See the \"Encoding\" section in the following link for more info: https://en.wikipedia.org/wiki/FASTQ_format. NOTE: '@' can be used as a quality value. An example of the information of one sequence/entry is: @Sequence 1 CTGTTAAATACCGACTTGCGTCAGGTGCGTGAACAACTGGGCCGCTTT + =&lt;&lt;&lt;=&gt;@@@ACDCBCDAC@BAA@BA@BBCBBDA@BB@&gt;CD@A@B?B@@ 3.2 Basecalling In short, basecalling is the converting of ONT signals to bases and quality. This step converts the Fast5 files to BAM and/or fastq files. There are many tools to carry out basecalling with ONT data. ONT sequencing machines carry this out with in built programs. However, if you are interested in the tools used for this the primary ones are Guppy and Albacore. Unfortunately it is quite hard to find information about these tools unless you own an ONT machine. Basecalling with Guppy tutorial: https://denbi-nanopore-training-course.readthedocs.io/en/latest/basecalling/basecalling.html Basecalling with Albacore tutorial: https://denbi-nanopore-training-course.readthedocs.io/en/stable/basecalling/basecalling.html "],["04-ONT_setup.html", "Chapter 4 ONT Setup", " Chapter 4 ONT Setup We need to initialise the environment to use the programs we need. This needs to be done in each terminal you want to run the commands from this ONT section in. Each terminal will only remember what you have told that terminal. Run the following command in a new terminal. To open a new terminal window, right click on the main screen, choose Terminal. . useontqc Before carrying out any more commands we will move into the relevant directory. cd ~/QC_workshop/ONT_QC/ Look in the directory called data and you will notice there are a few directories. You can see the contents of all these directories with the below command. ls data/* Each directory has one fastq file. ONT data likes to be organised with data for one sample being in one directory. To start we will only use the fastq file within the directory called \"Acinetobacter\". As you may have figured out, this contains ONT sequencing data of an Acinetobacter genome. Specifically the data is a subset of the SRA (Sequence Read Archive) Run: SRR7119550. "],["05-NanoStat.html", "Chapter 5 NanoStat 5.1 NanoStat: run 5.2 NanoStat: output 5.3 NanoStat: MCQs 5.4 NanoStat: summary", " Chapter 5 NanoStat The first step is to acquire stats for the sequences in our fastq file. We will use NanoStat (https://github.com/wdecoster/nanostat). NanoStat is one of the many tools contained in the NanoPack suite (https://github.com/wdecoster/nanopack). We will also use the tools NanoPlot and NanoFilt downstream. 5.1 NanoStat: run We want to have a tidy set of directories at the end of this analysis. It would be an untidy mess if we had all the output files in the current directory. We will therefore be making directories and subdirectories to send our output. mkdir nanostats Finally we will now run NanoStat. The options used are: -n : File name/path for the output. -t : Number of threads the script can use. --fastq : Input data is in fastq format. Other options that can be used are --fasta, --summary, and --bam. NanoStat -n nanostats/Acinetobacter_nanostats.tsv \\ -t 4 --fastq ./data/Acinetobacter/Acinetobacter_genome_ONT.fastq When the command has finished you can look at the output text file. In this case we will use the convenient less command. less nanostats/Acinetobacter_nanostats.tsv 5.2 NanoStat: output The file contains four sections with informative headers. These are: General summary A list of general summary metrics. Number, percentage and megabases of reads above quality cutoffs Based on the mean quality value of a read. The \"&gt;Q5\" line shows the number and % of reads with a mean quality above Q5. It also shows the total amount of megabases these reads contain. Top 5 highest mean basecall quality scores and their read lengths Shows the top 5 reads with the highest mean quality scores. Top 5 longest reads and their mean basecall quality score Shows the top 5 longest reads. 5.3 NanoStat: MCQs With the output of NanoStat and the above info on the output please answer the following Multiple-choice questions. What is the mean base quality for the read with the highest mean base quality? 9.2 9.5 12.3 What is the mean base quality for the longest read? 9.2 9.5 12.3 What is the mean read quality for all the data? 9.2 9.5 12.3 What is the length of the read with the 2nd highest mean quality? 5,770 6,700.5 25,072 What is the length of the 3rd longest read? 5,770 6,700.5 25,072 What is the median read length of the data? 5,770 6,700.5 25,072 To the nearest 0.1 Megabases (Mb, i.e. 100,000 bases), what is the total number of bases of the data? 0.2Mb 8.8Mb 14.1Mb How many Megabases are within reads with a quality cutoff of &gt;Q10? 0.2Mb 8.8Mb 14.1Mb How many Megabases are within reads with a quality cutoff of &gt;Q5? 0.2Mb 8.8Mb 14.1Mb How many Megabases are within reads with a quality cutoff of &gt;Q12? 0.2Mb 8.8Mb 14.1Mb 5.4 NanoStat: summary We have carried out our first step of QC by quality checking the fastq file with NanoStat. Overall the data has good quality, for ONT data &gt;Q10 is good. However, there is some poor quality data so quality control will be needed. "],["06-Porechop.html", "Chapter 6 Porechop 6.1 Porechop: run 6.2 Porechop: MCQs 6.3 Porechop: summary", " Chapter 6 Porechop Porechop is a tool to find and remove adapters from ONT data (https://github.com/rrwick/Porechop). Adapters are artificial sequences essential for sequencing but of no biological value and so you will typically want them removed. Porechop is no longer supported but it still works and is a good tool. Depending on which basecaller was used on your data, adapter removal may have already been carried out. However, it is always best to run porechop if you are not sure. Porechop has a list of known adapters it will look for and remove. These contain: Ligation kit adapters Rapid kit adapters PCR kit adapters Barcodes Native barcoding Rapid barcoding Porechop will look for these adapters at the start and end of each read. Then it will look for adapters within the sequence (known as middle adapters to Porechop). If it finds a middle adapter it will conclude that the read is chimeric (a recombinant read containing sequences from 2 or more reads) and split the read. Depending on the number of middle adapters the chimeric read may split into 2 or more reads. 6.1 Porechop: run With all that explanation we will now run Porechop. Thankfully the command is relatively straight forward with the options: -t: Number of threads to be used. -i: Input path of a fasta file, fastq file, or a directory. If a directory is specified it will be recursively searched for fastq files. -o: Output path. This will either be a fastq or fasta file name. #Create output directory mkdir porechop #Run porechop command porechop -t 4 -i ./data/Acinetobacter/Acinetobacter_genome_ONT.fastq \\ -o porechop/Acinetobacter.porechop.fastq Porechop will take a while to run. Whilst it is running, look at the screen output to get an idea of what it is doing. When finished, look at the bottom of the printed results. 6.2 Porechop: MCQs How many reads had adapters trimmed from their start? 0 1,952 79,015 How many bases were removed by adapters being trimmed from the end of reads? 0 1,952 79,015 How many reads were split based on middle adapters? 0 1,952 79,015 6.3 Porechop: summary Porechop did its job and removed adapters successfully. The next step is to view the quality of our reads prior to so we can be informed when we filter out poor quality reads and bases. For more uses of Porechop please see the below links: https://github.com/rrwick/Porechop#quick-usage-examples https://github.com/rrwick/Porechop#full-usage "],["07-NanoPlot.html", "Chapter 7 NanoPlot 7.1 NanoPlot: run 7.2 NanoPlot: output 7.3 NanoPlot: MCQs 7.4 NanoPlot: summary", " Chapter 7 NanoPlot NanoPlot can be thought of as the fastqc for ONT data. It produces a lot of useful visualisations to investigate the quality of ONT sequencing data. It can be used for fastq, fasta, BAM, and sequencing summary files. The link for its github page is: https://github.com/wdecoster/NanoPlot ONT data has much lower quality scores than Illumina with Q10 being good. If you have enough coverage and length the low quality can be corrected by downstream processes not covered in this tutorial. 7.1 NanoPlot: run Prior to running NanoPlot we will make a directory for the NanoPlot output. As NanoPlot creates a lot of files, we'll make a subdirectory for the NanoPlot output for the porechopped data. mkdir nanoplot mkdir nanoplot/porechop Now to run NanoPlot. The options we will use are: -t: Number of threads to be used. --fastq: Specifies the input path which is a fastq file. -o: Directory where the output will be created. -p: Prefix of output files. It is useful to have \"_\" at the end of the prefix. NanoPlot -t 4 \\ --fastq porechop/Acinetobacter.porechop.fastq \\ -o nanoplot/porechop -p Acinetobacter_ 7.2 NanoPlot: output List the files in the output directory. ls nanoplot/porechop/ There are quite a few files. These should all start with \"Acinetobacter_\" thanks to the -p option. To quickly check all the results we can open the report html file with firefox. firefox nanoplot/porechop/Acinetobacter_NanoPlot-report.html The first section contains NanoStat output. Quickly look over this and see how it compares to the NanoStat output of the pre-porechopped reads. NanoStat of pre-porechopped reads General summary values Mean read length 7,070.8 Mean read quality 9.5 Median read length 6,700.5 Median read quality 10.4 Number of reads 2,000.0 Read length N50 8,143.0 STDEV read length 3,336.0 Total bases 14,141,646.0 Number, percentage and megabases of reads above quality cutoffs &gt;Q5 2000 (100.0%) 14.1Mb &gt;Q7 1998 (99.9%) 14.1Mb &gt;Q10 1231 (61.6%) 8.8Mb &gt;Q12 29 (1.5%) 0.2Mb &gt;Q15 0 (0.0%) 0.0Mb Top 5 highest mean basecall quality scores and their read lengths 1 12.3 (7039) 2 12.3 (5770) 3 12.3 (8052) 4 12.2 (8120) 5 12.2 (3041) Top 5 longest reads and their mean basecall quality score 1 34661 (9.2) 2 31549 (8.5) 3 25072 (10.0) 4 24871 (10.3) 5 23185 (10.7) After the Summary Statistics section there is a Plots section. These are all plots created by plotly meaning they are interactive. With these plots you can: Make boxes to zoom into a specific area. Click and drag left or right only to zoom into a specific part of the x axis. Click and drag up or down only to zoom into a specific part of the y axis. Click the home icon ( ) on the top right to reset the axes. Hover over a points/bars to see the specific values of it. The plots are: Weighted Histogram of read lengths Histogram of \"Number of bases\" (y) against \"Read length\" (x). Weighted Histogram of read lengths after log transformation Histogram of \"Number of bases\" (y) against log transformed \"Read length\" (x). Non weighted histogram of read lengths Histogram of \"Number of reads\" (y) against \"Read length\" (x). Non weighted histogram of read lengths after log transformations Histogram of \"Number of reads\" (y) against log transformed \"Read length\" (x). Yield by length Plot showing the \"Cumulative yield for minimal length\" (y) by \"Read length\" (x). The cumulative yield is measured in Gigabases (billion bases). This plot is useful to know how many bases you would retain if you filtered reads based on read length. Read lengths vs Average read quality plot using dots Each dot represents a single read. At the top of the plot is a histogram of Number of reads against read lengths. At the right of the plot is a sideways histogram of number of reads against average read quality. Read lengths vs Average read quality kde plot Each dot represents a single read with a topology map showing the density of plots. THe darker the area the more dense. This is the most informative plot. At the top of the plot is a histogram of Number of reads against read lengths. At the right of the plot is a sideways histogram of number of reads against average read quality. 7.3 NanoPlot: MCQs Please attempt the below MCQs with the information in the report html. What is the mean read quality for all the data? 9.5 6,962.1 8.8Mb What is the mean read length of the data? 9.5 6,962.1 8.8Mb How many Megabases are within reads with a quality cutoff of &gt;Q10? 9.5 6,962.1 8.8Mb Approximately how many bases would you retain if you filtered out/removed reads with a length &lt;10,000bp (Yield by length)? 10 146 4Mb (0.004Gb) Look at the \"Read lengths vs Average read quality plot\". In what average quality range are the majority of reads? 6-8 8-10 10-12 7.4 NanoPlot: summary We have use NanoPlot to produce read quality and length stats in table and plot form. Our data still looks good after porechopping but we still need to carry out some filtering. You can use NanoPlot with different input files. This will give you different plots. For more details please see the \"Plots Generated\" section on: https://github.com/wdecoster/NanoPlot. "],["08-NanoFilt.html", "Chapter 8 NanoFilt 8.1 NanoFilt: run", " Chapter 8 NanoFilt NanoFilt can be used to remove/filter reads by quality and/or read length (https://github.com/wdecoster/nanofilt). This is very useful as you will most likely want long and good quality reads for downstream processes, such as genome assemblies. It is always important to know what your data is and if your planned filtering and trimming is appropriate. For example, you may be working with amplicon data where the read lengths will vary between 500bp and 750bp. In that case it is useful to set a min length of 500 and a maxlength of 750. This data is from a genome so wanting long and high quality reads is appropriate. Overall the quality of our data looks good. The main exception is the low amount of sequencing/coverage. The low coverage is because it is a subset of the whole data for workshop purposes (the full dataset would take too long to run here). There are some shorter length and lower quality reads which we will remove. 8.1 NanoFilt: run First we will create an output directory. mkdir nanofilt We will filter out sequences that are shorter than 500bp (-l 500bp) and filter out sequences with a average read quality less than Q10 (-q 10). We have chosen these values as they are a good default for genomic data. Q10 appeared to be a good choice from NanoPlot as the majority of reads had a quality of &gt;Q10. cat porechop/Acinetobacter.porechop.fastq | \\ NanoFilt -l 500 -q 10 &gt; \\ nanofilt/Acinetobacter.nanofiltq10minlen500.porechop.fastq Are the chosen options appropriate? Let's find out. "],["09-ONT_final_check.html", "Chapter 9 Final check 9.1 Final NanoPlot 9.2 ONT QC considerations 9.3 NanoFilt Reattempts 9.4 ONT Recap", " Chapter 9 Final check After trimming or filtering reads (quality control) it is always important to carry out a quality check. We will therefore run NanoPlot again. 9.1 Final NanoPlot Note: We are using a long informative output directory name. This is important as we may need to rerun NanoFilt a few more times with different options until we are happy. #Make an output directory before running NanoPlot mkdir nanoplot/nanofiltq10minlen500_porechop #Run NanoPlot on the fastq file with the filtered and porechopped data NanoPlot -t 4 \\ --fastq nanofilt/Acinetobacter.nanofiltq10minlen500.porechop.fastq \\ -o nanoplot/nanofiltq10minlen500_porechop/ -p Acinetobacter_ Now inspect the report firefox nanoplot/nanofiltq10minlen500_porechop/Acinetobacter_NanoPlot-report.html 9.2 ONT QC considerations Ultimately when quality checking we need to take into account how much data we are left with. We need to make sure we have a good amount of bases and reads for our application. However, we also want to make sure we aren't left with a lot of poor quality data. Depending on our application the quality of the reads may be less important, or the other way around. This is the same with the amount of reads and bases. What is needed will be clearer when you know what type of data you have. These considerations will be covered in our future workshops of the specific data types A quick example is data for a genome assembly. For this type of analysis we will want our number of bases to reach a certain coverage. We'll want a decent quality but a high coverage can help overcome lower quality (more on this in a future workshop). Generally for ONT assemblies you will want 20X - 100X coverage with a higher coverage producing a better assembly (100x coverage= 100bp sequencing data per 1bp of the genome size). Higher coverages may be worse as it can be too much information for an assembly algorithm to cope with. In addition to the amount of bases, the length of reads is also important. One read that is 10kbp long is likely better than 10 reads that are each 1kbp long for a genome assembly. This is because less reads need to be assembled. 9.3 NanoFilt Reattempts Keeping all this in mind, let's say this sequencing is for a genome with a size of 0.5 Mbp (0.5 million base pairs). We want at least 20X coverage (i.e 10,000,000 total bases). Do we have this for the data that was porechopped and nanofiltered? If not, try running NanoFilt with a different choice for --q until you do. Then attempt to answer the questions below. These aren't MCQs as you will get different answers based on what value you use for --q. What is an option that works to get the desired coverage? Is the overall quality decent (Mean &amp; Median read quality &gt; 10)? What is the N50 of your QC'd reads? How many extra reads and bases did you retain compared to the Q10 filtering? Tips: Make sure your new output paths have unique names. Use NanoPlot results to compare the Q10 filtering to your own. The NanoStat part is especially helpful. 9.4 ONT Recap That is the end of the ONT QC section. We carried out: NanoStat: Produced read quality and length stats of our raw reads. Porechop: Removed adapters from our reads. NanoPlot: Produced read quality and length stats and plots of our porechopped reads. NanoFilt: Filtered our porechopped reads based on quality and length. Final check: NanoPlot of our filtered reads. It is always good to check the quality after every quality control step. "],["10-PB_background.html", "Chapter 10 PacBio Background 10.1 File formats 10.2 CLR &amp; CCS 10.3 Tools", " Chapter 10 PacBio Background In this section we will quality check PacBio sequencing raw data (BAM files), perform a filter to retain only the longest reads and output the data in fastq format. 10.1 File formats Sequence data from PacBio are output as subreads in unaligned BAM format. (Further explanation of the BAM format is in section 3.1.3). The figure below shows a schematic of a circular DNA molecule of a PacBio library with the colours representing: Black, I = Insert DNA, i.e. the double-stranded DNA of your sample Red, B = Barcodes, optional single (Left or Right) or double (Left &amp; Right) for multiplexing samples Green, A = Adapter, SMRTbell adapters with a double-stranded stem and single-stranded hairpin loop. Sequencing follows the direction of the arrows and continues around until either the end of the sequencing run or if the DNA polymerase fails. PacBio sequencing takes place in Zero-Mode Waveguides (ZMWs) on a sequencing cell. These are tiny holes where a DNA polymerase binds a single molecule of DNA for sequencing, and each DNA base is detected when a labelled nucleotide is incorporated and fluoresces when excited by light. A ZMW read consists of the data collected from a single ZMW, with the HQ (high quality) region recorded when just a single molecule of DNA is present in the ZMW. This schematic shows the structure of a ZMW read in linear format. Insert DNA subreads are interspersed with the barcode and adapter sequence of the SMRTbell library. For more information see: https://pacbiofileformats.readthedocs.io/en/10.0/Primer.html subreads.bam file This contains the sequence for each read (or pass) of the insert DNA (grey). scraps.bam file This contains the adapter (green) and barcode (red) sequences, as well as any low quality (black) regions. These BAM files will usually have an accompanying PacBio BAM index file (subreads.bam.pbi and scraps.bam.pbi). 10.2 CLR &amp; CCS PacBio data can be generated in two ways depending on the goal of the experiment. Continuous Long Reads (CLR) the aim is to produce sequence reads as long as possible, such as for genome assembly, sequencing through long repeat regions or finding other large structural variants. CLR runs usually generate one subread (i.e. there would be just one or one and a bit of the grey insert regions shown above) and sequences have an error rate of ~ 10-15%. Circular Consensus Sequences (CCS) the aim is to sequence shorter molecules (up to 10-15kb max), such as amplicons, and to generate accurate consensus sequences. CCS runs generate higher-accuracy sequences from a consensus of many subreads (i.e. there would be several of the grey insert regions shown above in every ZMW read). For this tutorial we will carry out quality checking and control on both CLR and CCS BAM files. 10.3 Tools 10.3.1 SequelTools https://github.com/ISUgenomics/SequelTools We will use the SequelTools program to both assess and filter our PacBio CLR and CCS data. 10.3.2 smrttools https://www.pacb.com/wp-content/uploads/SMRT_Tools_Reference_Guide_v90.pdf We will also use smrttools from PacBio to: Generate any missing index files Generate CCS reads from subreads BAM files Convert BAM to fasta/q format. 10.3.3 RabbitQCPlus https://github.com/RabbitBio/RabbitQCPlus Lastly, we will also use RabbitQCPlus to assess the CCS assembled reads in fastq format. "],["11-PB_CLR_setup.html", "Chapter 11 CLR setup", " Chapter 11 CLR setup We need to initialise the environment to use the programs we need. This needs to be done in each terminal you want to run the commands from this ONT section in. Each terminal will only remember what you have told that terminal. Run the following command in a new terminal. To open a new terminal window, right click on the main screen, choose Terminal. . usepbqc Before carrying out any specific commands we will move into the relevant directory. cd ~/QC_workshop/PB_QC/data/CLR Use ls to list the contents of this directory. You will see that there are BAM subreads and scraps files for three samples, plus their BAM index files. These data were generated from high molecular weight DNA for de novo genome assembly. We will use the SequelTools program to both assess and filter our PacBio CLR data. First we need to make a file of filenames (fofn) for the subreads files. You can do this with the following commands: find $(pwd) -name &quot;*subreads.bam&quot; | sort &gt; CLR_subreads.txt This command line will find all of the files in our current working directory whose names end in 'subreads.bam', sorts them alphanumerically and prints these names into a new text file. It is a good idea to check the contents of the file with less to ensure it contains the file names. In this case this would be a .subreads.bam for each sample. "],["12-PB_CLR_QC.html", "Chapter 12 CLR QC 12.1 CLR QC: run 12.2 CLR QC: output 12.3 CLR QC: MCQs 12.4 CLR QC: summary", " Chapter 12 CLR QC In this section we will quality control the CLR (Continuous Long Read) reads with SequelTools. 12.1 CLR QC: run The SequelTools program has three different tools, specified with the -t argument, which are: Q for quality control S for subsampling the data F for filtering the data We will use Q to assess our data. Other options used are: -u to specify the file containing the list of subread .bam files -o to specify the name of an output folder for the plots -p to specify which plots to produce. b: A few basic plots i: The basic plots and a few more detailed plots a: generates all available plots. This is what we will use. SequelTools.sh -t Q -p a -u CLR_subreads.txt -o CLR_QC 12.2 CLR QC: output When this has finished running the CLR_QC folder will contain a 'summaryTable.txt' file with values on the number and lengths of sequence reads for all three samples, plus a series of plots saved as pdf files. We can use firefox to view the pdf plots. To open all the pdf files containing the plots you can run the below commands. firefox CLR_QC/*.pdf totalBasesBarplot.pdf: Shows the total amount of sequence data (total bases; y axis). It displays this for: All of the subreads present Only the 'longestSubs'. This is the longest subread within each CLR. Ideally we want each CLR to only consist of 1 subread. n50s.pdf: Shows the N50 values of the samples. N50 = the median sequence length (in bp) of the data; 50% of the sequence data generated is in subreads equal to or greater than this length. l50s.pdf: Shows the L50 values of the samples. L50 = the minimum number of subreads whose length makes up the N50 value. subreadSizesBoxplots.pdf: Box plots showing the range of subread lengths for the samples. .readLenHists.pdf: Contains 2 histograms for subread length. Histogram of subread lengths Histogram of longest subread ('longestSubs') lengths. Each sample has its own readLenHists.pdf file, e.g. Sample1.readLenHists.pdf. psrs.pdf: Shows the PSR value for each sample. PSR stands for polymerase-to-subread ratio It is calculated as the total bases from the longest subreads per CLR divided by the total bases from subreads. This is a measure of the effectiveness of library preparation. When PSR is close to one the DNA template is mostly the reads of interest. A total failure of library preparation would result in no reads of interest and a PSR of zero. zors.pdf: Shows the ZOR value for each sample. ZOR stands for ZMW-occupancy-ratio. This is calculated as the number of CLRs with subreads divided by the number of subreads. This is a measure of the effectiveness of matching DNA templates with ZMWs. When ZOR is zero there are no DNA templates in ZMWs. When ZOR is above one then there are more than one DNA template per ZMW on average. Ideally, this value is exactly one, indicating there was 1 DNA template per ZMW which is what we would want for CLRs. 12.3 CLR QC: MCQs With the produced files attempt the below questions. Which sample has the highest total amount of sequence data? Sample1 Sample2 Sample3 Which sample has the highest N50 sizes? Sample1 Sample2 Sample3 Which sample has the highest L50 sizes? Sample1 Sample2 Sample3 Which sample has the best PSR value (closest to 1)? Sample1 Sample2 Sample3 Which sample has the best ZOR value (closest to 1 but not above 1)? Sample1 Sample2 Sample3 Which sample has the biggest difference when comparing their \"Histogram of subread lengths\" versus their \"Histogram of longest subread length\"? Sample1 Sample2 Sample3 12.4 CLR QC: summary With the QC we can see that Sample2 has the largest amount of data, and for all three samples most of the data is contained within the longest subreads. This is expected with CLR data - long fragments of DNA are extracted for sequencing and we often achieve and aim for just a single pass of this insert during the PacBio sequencing run. We can see that the N50 is larger for Sample3 compared to Sample1 and Sample2, suggesting the subreads are longer for Sample3. Conversely, the L50 is higher for Sample2, suggesting the subreads are shorter for this sample as more subreads are required to make up the N50 value. The boxplots show that Sample3 does indeed have the longest subread lengths, and highest N50 as indicated by the blue diamond. Sample2 has a larger range of subread lengths than Sample1, but has a slightly lower median length and N50 value. The histograms show the distribution of subread lengths in more detail. Sample3 shows a big spike of very short subreads. We'll discuss this after the filtering step! "],["13-PB_CLR_filtering.html", "Chapter 13 CLR filtering 13.1 CLR filtering: run 13.2 Quality check filtered data 13.3 BAM to fastq 13.4 CLR filtering: summary", " Chapter 13 CLR filtering Our quality check of the PacBio data doesn't include any assessment of sequence quality. Unaligned PacBio data doesn't have a quality score in the same way as the Illumina data we looked at on Tuesday. Quality scores for PacBio are generated when the reads are aligned - either to a reference sequence or subreads aligned to each other to generate CCS reads. However, it is often a good idea to use only the longest PacBio reads for a de novo assembly. 13.1 CLR filtering: run To filter the data by minimum CLR length we will use SequelTools with the -t F option. This sets the task mode to filtering. Prior to the command, we need to make a .fofn file (file of file names) for the scraps.bam files, as these are needed when using the filtering tool. find $(pwd) -name &quot;*scraps.bam&quot; | sort &gt; CLR_scraps.txt I recommend you view the output file (CLR_scrap.txt) with less to ensure it contains the file names (1 scraps.bam file for each sample in this case). Then we can run the filtering command. We will retain CLRs with a length of at least 10,000 bp. SequelTools.sh -t F -u CLR_subreads.txt -c CLR_scraps.txt \\ -C -Z 10000 -f b -o filtered Parameters: -t: Task to carry out. F for filtering. -u: Input file containing the list of subread .bam files. -c: Input file containing the list of scraps .bam files. -C: Filter by minimum CLR length. -Z: Minimum length to keep a CLR. -f: Format of the output files: s=sam b=bam 2=both -o: Name of an output folder for the filtered data files. 13.2 Quality check filtered data When this has finished running the 'filtered' folder will contain filtered BAM files for all three samples. Move into this folder using cd: cd filtered From here let's run the quality control tool on the filtered files and compare the plots to those from the raw files. First we need a new fofn for the filtered subread files: find $(pwd) -name &quot;*subreads.bam&quot; | sort &gt; filt_subreads.txt Then run the quality control tool again: SequelTools.sh -t Q -p a -u filt_subreads.txt -o filt_QC When this has finished running the 'filt_QC' folder will contain a text file of summary values and a series of plots saved as pdf files. We can view these as before, for example: firefox filt_QC/subreadSizesBoxplots.pdf Compare this to the boxplots from the unfiltered data: firefox ~/QC_workshop/PB_QC/data/CLR/CLR_QC/subreadSizesBoxplots.pdf The difference looks quite subtle on the plots as the size range of reads is large, but the subread N50 value (indicated by the blue diamond) has increased by ~2,000 bp in the filtered data for Sample2 and Sample3. There are also still subreads present in the data that are &lt; 10,000 bp. This is because we have filtered out CLRs that are &lt; 10,000 bp, but a CLR &gt; 10,000bp can still be made up of subreads smaller than this. This is definitely the case for Sample1 and Sample2, where the DNA fragment length of the library was shorter than the CLR lengths, hence there are subreads &lt; 10,000 bp within longer CLRs. Sample3 shows a more obvious difference before and after filtering: firefox ~/QC_workshop/PB_QC/data/CLR/CLR_QC/Sample3.readLenHists.pdf \\ filt_QC/Sample3_flt.readLenHists.pdf For this sample the DNA fragment length of the library was much greater than 10,000 bp, and so the majority of CLRs consist of just one pass of this long insert. The large number of short subreads remaining in the filtered file (blue bars) represent subreads only partially sequenced on the 'return journey' of the circular library DNA molecule. These are not present when we look only at the longest subread present in each CLR (green bars). 13.3 BAM to fastq SequelTools doesn't have an option for filtering subreads by length. To do this we can convert the filtered BAM file to a fastq or fasta file and use another downstream tool. The fastq file will not contain any useful information on sequence quality (every base is assigned a quality character of '!' which is equal to zero), but this file format is useful if only fastq format is accepted as input for a downstream tool. To do this use the commands pbindex and bam2fastq from the smrttools package. Prior to converting from bam to fastq we need to index the bam file. This will generate a .pbi index file. pbindex Sample1_flt.subreads.bam The bam2fastq command requires an output file prefix name, specified by -o, and the name of the BAM file we want to convert: bam2fastq -o Sample1_filtered Sample1_flt.subreads.bam Note: You may see a long warning message ending with \"No such file or directory\". However, if you run ls you should see that the output file has been generated. You will now have a compressed fastq file for the CLR-filtered subreads from Sample1. You can also carry this out for Sample2 &amp; Sample3 but it is not required. 13.4 CLR filtering: summary We have quality checked and quality controlled our CLR data. This has produced both filtered CLRs and filtered subreads. Next we will learn how to QC CCS data. "],["14-CCS.html", "Chapter 14 Circular Consensus Sequences 14.1 QC the subreads 14.2 CCS generation 14.3 RabbitQCPlus 14.4 Final summary", " Chapter 14 Circular Consensus Sequences We will now look at some long amplicon sequencing data. These data were generated from ~4 kb PCR amplicons of a mammalian gene region. For this section we will start with the subreads and carry out the following steps: QC the subreads Generate CCS reads form the subreads. QC the CCS reads with RabbitQC. Before carrying out any specific commands we will first move into the relevant directory. cd ~/QC_workshop/PB_QC/data/CCS 14.1 QC the subreads OUr current directory contains one file of subreads from the amplicons sequencing run, named 'Sample4.subreads.bam'. Even though we just have one file we want to QC, SequelTools still requires a fofn to run: find $(pwd) -name &quot;*subreads.bam&quot; &gt; subreads.txt With our file of file names (.fofn) created we can run SequelTools for quality checking. SequelTools.sh -t Q -p a -u subreads.txt -o S4QC When these commands have run you will have a 'S4QC' folder containing the 'summaryTable.txt' file and the series of plots saved as pdf files. Let's have a look at the subread length histogram. firefox S4QC/Sample4.readLenHists.pdf Many of the longest subreads (green bars) are ~ 4kb (the length of our amplicon), but there are also lots of shorter reads. 14.2 CCS generation To generate CCS reads we will use the ccs tool from the smrttools package. This will also act as a form of filtering. The full command to generate CCS reads is: ccs --minLength 3500 --minPasses 7 -j 1 Sample4.subreads.bam S4_ccs.bam Parameters: --minLength to specify the minimum subread length to use to generate a ccs read --minPasses to specify the minimum number of subreads per ZMW read to generate a ccs read -j to specify the number of threads to use The second last parameter is the input file. In this case Sample4.subreads.bam. The last input is the output file name. In this case S4_ccs.bam. Important! Even though this is a heavily downsized dataset this command still takes ~20 minutes to run. To prevent you having to wait we have included the results file already in your directory. If you have typed in the above command to run you can kill it by pressing Ctrl - C. To see the results summary: cd ccs_results less ccs_report.txt 14.2.1 CCS MCQs What percentage of the subreads generated a CCS? 0% 29.16% 70.62% What was the main reason for failure of CCS generation? No usable subreads Insert size too long or small CCS did not converge 14.3 RabbitQCPlus Lastly, let's perform a quality check on the 'ccs.bam' file to assess the lengths of the ccs reads. SequelTools only works with the raw subread BAM files, so we will use an alternative program - RabbitQC. However, this program needs a fastq file as input so we will first generate that using the bam2fastq tool from smrttools. Thankfully an index file .pbi was already created when we generated the ccs bam file. We therefore don't need to create this. The bam2fastq command requires an output file prefix name, specified by -o, and the name of the BAM file we want to convert. We'll also include the option -u to give an uncompressed fastq output. bam2fastq -u -o S4_ccs S4_ccs.bam Note: You may see a long warning message ending with \"No such file or directory\". However, if you run ls you should see that the output file has been generated. With the CCS fastq file we're ready to run RabbitQC. RabbitQCPlus -i S4_ccs.fastq -w 1 --TGS Parameters: -i to specify the input fastq file name -w to specify the number of threads --TGS to specifiy the data is Third Generation Sequencing data (ONT or PB) View the output using firefox. firefox S4_ccs_RabbitQCPlus.html The \"Reads Length Distribution\" shows all but three ccs reads to be &gt; 4,000bp. The majority are ~ 4.4kb. In fact the amplicon is a little longer than 4kb plus the reads contain an index sequence added during PCR to allow multiplexing of multiple samples, but this tight distribution of sizes is a good indication that the amplification and sequencing has worked well. The other plots show the % nucleotide frequencies (AGCT) and mean quality scores for the beginning and end of the reads. The mean quality plots unfortunately do not show anything because it is such a short region. The main advantage of RabbitQC is that it is a very fast way to assess the processed read lengths. 14.4 Final summary That is the end of this book. You have learnt how to QC ONT &amp; PacBio data. Of course there are various other tools out there and the field is always advancing so we suggest you also have a look what is out there. Thanks for reading. "],["15-Appendix.html", "A Mamba installs A.1 Mamba installation A.2 ONT QC environment A.3 PB QC environment", " A Mamba installs A.1 Mamba installation Mamba is a reimplementation of conda. It is a great tool for installing bioinformatic packages including R packages. Mamba github: https://github.com/mamba-org/mamba The best way to use Mamba is to install Miniforge. It has both Conda and Mamba commands. Miniforge installation: https://github.com/conda-forge/miniforge Mamba guide: https://mamba.readthedocs.io/en/latest/user_guide/mamba.html A.2 ONT QC environment To create the mamba environment ont_qc run the below commands in your bash. You will need to have installed mamba first. #Oxford Nanopore QC mamba create -n ont_qc mamba activate ont_qc #Install pip mamba install -c anaconda pip #Install nanopack with previously installed pip pip install nanopack #Install porechop mamba install -c bioconda porechop A.3 PB QC environment To create the mamba environment pb_qc run the below commands in your bash. You will need to have installed mamba first. #PacBio QC mamba create -n pb_qc mamba activate pb_qc #Install smrttools mamba install -c hcc smrtlink-tools #Install git &amp; make for below installs mamba install -c anaconda git mamba install -c anaconda make #R needed for Sequeltools mamba install -c conda-forge r-base Install SequelTools and RabbitQCPlus via git. I suggest creating a directory called \"git_installs\" in your home directory and running this code there. #git_installs directory mkdir ~/git_installs cd ~/git_installs #Sequeltools git clone https://github.com/ISUgenomics/SequelTools.git cd SequelTools/Scripts chmod +x *.sh *.py *.R cd ~/git_installs #RabbitQCPlus git clone https://github.com/RabbitBio/RabbitQCPlus.git cd RabbitQCPlus make -j4 To run these programs in your own machines you will need to use the full path of the command files. If you installed these in your \"~/git_installs\" examples are below. #Sequeltools.sh help manual ~/git_installs/SequelTools/Scripts/SequelTools.sh #RabbitQCPlus help manual ~/git_installs/RabbitQCPlus/RabbitQCPlus "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
