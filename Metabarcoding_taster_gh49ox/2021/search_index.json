[["01-Metabarcoding.html", "Metabarcoding for diet analysis and environmental DNA Chapter 1 Introduction", " Metabarcoding for diet analysis and environmental DNA Katy Maher, Helen Hipperson &amp; Graeme Fox 2025-12-01 Chapter 1 Introduction This practical session aims to introduce you to analysing metabarcoding data using DADA2. The topics covered are: Example dataset information and background Introduction to R and DADA2 Cluster intro Loading your data into R and dada2 Checking the quality of your data Cleaning your data Identifying ASVs Assigning taxonomy Further analysis Summary This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["02-Example_dataset.html", "Chapter 2 Background", " Chapter 2 Background Advances in sequencing technology mean that it is now feasible to characterise eDNA and diet content through DNA-based approaches by simultaneously sequencing short standardised DNA sequences (DNA barcodes) from a variety of environmental samples, a process known as DNA metabarcoding. The dataset we will use today to work through our metabarcoding analysis is from a study by Doble et al. 2019 on tropical fish communities from Lake Tanganyika. The study’s authors collected water eDNA samples from sites along the shore of Lake Tanganyika in Tanzania. Samples were then filtered, extracted and made into individual libraries using a two-step PCR method to amplify barcoding genes and attach indexes and Illumina adapters. Four different primer sets were used to assess the community structure of fishes in Lake Tanganyika. We will examine one of these primer sets today: MiFish‐U (Miya et al. 2015) These primers target a hypervariable region of the mitochondrial 12S rRNA gene. The 12S gene occupies 1/16th of the mitochondrial genome. It is the mitochondrial homologue of the prokaryotic 16S and eukaryotic nuclear 18S ribosomal RNAs. Sequencing was carried out with an Illumina MiSeq 300 cycle sequencing run (2x150bp paired end sequencing). The subset of samples you will analyse today were sampled in triplicate from sites along the shore of Lake Tanganyika in 2017. Also included is a filter control, an extraction control and 5 aquarium samples. The aquarium samples contained five cichlid species endemic to the lake. "],["03-R_DADA2_intro.html", "Chapter 3 Introduction to DADA2 and R 3.1 DADA2 3.2 R", " Chapter 3 Introduction to DADA2 and R 3.1 DADA2 DADA2 is an open-source software package for modeling and correcting Illumina-sequenced amplicon errors. It has several advantages over other amplicon pipelines: Resolution: DADA2 infers amplicon sequence variants (ASVs) from amplicon data rather than operational taxonomic units (OTUs) clustering based methods (we will discuss what these are and how they differ below). Accuracy: DADA2 reports fewer false positive sequence variants than other methods. It incorporates quality information, quantitative abundances, sequence differences and parameterises its error model using the data provided. Comparability: The ASVs output by DADA2 can be directly compared between studies, without the need to reprocess the pooled data. Computational Scaling: The compute time and memory requirements are lower than some other methods. 3.1.1 OTUs vs ASVs OTUs (operational taxonomic units) are generated by clustering reads into groups based on similarity. A threshold, such as 97% similarity, is used to cluster sequences together often with the hope that all those sequences will belong to the same taxonomic group. Because of this clustering step different datasets cannot be compared. Commonly used software that are used for clustering OTUs include: UCLUST VSEARCH CD-Hit SWARM ASVs (amplicon sequence variants) are generated using the following methods. First the number of times an exact sequence has been sequenced is calculated. This is then combined with an error model unique for the sequencing run to determine the probability that a particular read is not found in the dataset due to a sequencing error. Reads can then be filtered out according to this calculation with the aim to remove sequencing errors and leave only true sequences. ASVs are able to be compared across studies using the same amplicon sequence as exact sequences are used rather than the clusters generated by OTU analysis. 3.1.2 Workflow The workflow we will be following is summarised below: Load paired-end data (with primers removed) into the R environment Use DADA2 to examine data quality by plotting error profiles Filter and trim the files to remove poor quality sequences Generate an error model, dereplicate reads and infer ASVs from the clean data Merge the paired end reads Make an ASV matrix Remove potential chimeric reads from the dataset Assign taxonomy Multivariate analysis 3.2 R R is an open-source free language and environment which was designed for statistical computing and graphics. It is possible to install R on many UNIX platforms, Windows and MacOS. In addition to its base environment, that contains capabilities for data manipulation, calculation and graphical display, there are many packages (including DADA2) that can be installed and contain additional functions, data and code for analysing, manipulating and plotting different types of data. You do not need to understand all the code you will be running today but a basic foundation in R can be useful when you come to analysing your own data and running further analysis. Here are some useful links to get you started if you want to learn more about R after this course. Tutorial: https://swirlstats.com/ R Website: https://www.r-project.org/ Cran Website: https://cran.r-project.org/ Rstudio Website: https://www.rstudio.com/ If you are not familiar with R we have decribed some of the common terminology and put a few hints to help you understand how to run a few basic R commands below. You do not need to type these commands out anywhere, they are provided for reference. Feel free to skip this section and move onto the next chapter if you have used R before and are comfortable with the basics. 3.2.1 The console window The console window is the place where the user can type R commands to submit and then view the results, just like the terminal window in linux. R shows it is ready to accept commands by showing a &gt; prompt. You can then submit a command using the enter button. If you haven’t completed a command (for example you forget to close a set of parentheses) the + symbol will show on a new line until you finish the command. 3.2.2 Your working directory Your working directory is the folder you are currently working on on your computer. If you set this when you read in or write an output to a file R will automatically look for and save files in this directory. You can set your working directory using: setwd(&quot;/path/to/working_directory/&quot;) 3.2.3 Packages Packages contain additional functions, data and code for analysing, manipulating and plotting different types of data. Many common packages will be installed as default when you install R. Other more specialised packages, such as the DADA2 package, must be installed by the user. Packages found on The Comprehensive R Archive Network (CRAN) which is R’s central software repository can be installed easily using the following command. install.packages(&quot;package_name&quot;) Every time you reload R you will need to load the packages you need if they are not one of the ones installed by default. To do this type: library(&quot;package_name&quot;) 3.2.4 Functions Functions are a set of commands that are organised to perform a specific task. There are a lot of functions that come pre-installed in packages in R and others can be installed as parts of packages. It is also possible for users to create their own functions. For example a specific function can be called like this, we will use the sum function: sum(1,2) # which sums 1+2 and prints 3 to the console window. And to write your own function you would type something like this: function_name &lt;- function(arg_1, arg_2, ...) { Function body } 3.2.5 Assigning values to variables A variable provides a means of accessing the data stored in R’s memory. Data can be assigned to a variable using either &lt;- or =. For example: x&lt;-sum(1,2) If you run this command and then type x into the R console 3 will be printed to the screen. You can then use this variable in another function and assign to another variable if desired: y&lt;-sum(x,x) y # y would be assigned the value 6 and be printed to the console When you have a large script containing many commands it is beneficial to give variables logical names which can help you remember what they contain and help you call them quickly when needed. 3.2.6 A few useful commands and tips rm(list=ls()) can be used to clear your environment when you start a new analysis. This will delete all the variables you have stored in R’s memory so be careful when you use this. The read.table() can be used to read in a file. R likes to work with simple formats such as .csv and .txt files. write.table() can be used to write a data frame or a matrix to a file. Just like linux, remember you can press the up arrow on your keyboard to list previous commands run in the current R session. To get the more information and help on the function you want to use you can call using ?_function.name_ e.g.: ?sum "],["04-Cluster_Introduction.html", "Chapter 4 Cluster Introduction 4.1 Logon instructions 4.2 The Terminal Window", " Chapter 4 Cluster Introduction 4.1 Logon instructions For this workshop we will be using Virtual Network Computing (VNC). Connect to the VNC with a browser by using the webVNC link you have been given. You will now be in a logged-in Linux VNC desktop with two terminals. You will see something as below (there may be only one terminal which is fine). If you do not see something similar please ask for assistance. If the VNC is taking up too much/little space of your browser you can use the zoom of your browser to adjust the size. Ensure you can see one whole terminal. These instructions will not work outside of this workshop. If you would like to install your own Linux OS on your desktop or laptop we would recommend Ubuntu. The following link is a guide to install Ubuntu: https://www.ubuntu.com/download/desktop/install-ubuntu-desktop. If you use a USB you need to create a bootable USB stick. The following link will assist: https://www.ubuntu.com/download/desktop/create-a-usb-stick-on-windows 4.2 The Terminal Window In our case the terminal window looks like the picture below. We are using the terminal window as our shell to interpret our commands to the kernel. Depending on your system and preferences it may look different. Already there is useful information for us on the terminal window. nsc006: This is the login name, also known as the username. In this case nsc006 is a demonstrator’s account. Your screen should show a different account name which will be your username for the Linux machine/cluster you are logged into. gauss03: This is the machine name the user is logged into. ~: This represents the current directory of the user, or the directory a command was run in. In the Linux OS and others ‘~’ is a shortcut to the user’s home directory. Everything after the ‘$’ is where commands are typed into the terminal. This is also referred to as the command line. To open a new terminal window, right click on the main screen, choose Applications -&gt; Shell -&gt; bash "],["05-Loading_data.html", "Chapter 5 Loading data into R and DADA2 5.1 Loading R 5.2 Loading DADA2 5.3 Loading our data", " Chapter 5 Loading data into R and DADA2 Before we can start we first need to make a directory which will be used to contain all the files you generate throughout this workshop. To do this type the commands shown in black below. Lines starting with a # character are comments describing what the code will do, and you do not need to type these (the light blue text in the box below). #Change directory to home cd ~ #Make a directory in home called &quot;EAB_Metabarcoding&quot; mkdir EAB_Metabarcoding #Change directory to &quot;EAB_Metabarcoding&quot; cd EAB_Metabarcoding You will need to load the metabarcoding tools before continuing. Carry this out with the following command. Note: Ensure you include the dot and space (. ) at the start before usemetabarcoding . usemetabarcoding 5.1 Loading R Now we can load R. R You should get a screen that looks like this. This is the R console window. From now on the commands will be run using the R programming language. 5.2 Loading DADA2 To start with we will need to load a few packages (see section 3.2.3 for a description of what a package is if you are not familiar with R) which we will need for running the first part of our analysis. library(dada2) library(Biostrings) library(ShortRead) 5.3 Loading our data 5.3.1 Saving the paths for our input and output directories We are now nearly ready to input our data but first we need to set the main paths we will be using which contain the input files (the raw sequencing data files) and where we want to save the output files we generate (the ‘EAB_Metabarcoding’ directory we made above). input.path &lt;- &quot;/pub39/tea/nsc006/NEOF/metabarcoding_workshop/trimmed&quot; output.path &lt;- &quot;~/EAB_Metabarcoding&quot; We can now check what files are contained in our input directory list.files(input.path) You should see a list of paired fastq sequencing files listed. Forward and reverse fastq filenames have format: SAMPLENAME_L001_R1_001.fastq and SAMPLENAME_L001_R2_001.fastq. This shows that we have set our input path correctly. 5.3.2 Inputting the forward and reverse reads We will assign the input path and specify all file names which end in R1_001.fastq as our forward reads (stored as the variable fnFs - file name ForwardReads) and R2_001.fastq as our reverse reads (stored as the variable fnRs - file name ReverseReads). fnFs &lt;- sort(list.files(input.path, pattern = &quot;R1_001.fastq&quot;, full.names = TRUE)) fnRs &lt;- sort(list.files(input.path, pattern = &quot;R2_001.fastq&quot;, full.names = TRUE)) "],["06-quality_checks.html", "Chapter 6 Checking the quality of your data", " Chapter 6 Checking the quality of your data DADA2 has its own quality control option, which plots a figure of read length by sequence quality. To run we will extract the sample names from the imported fastq files and then run the plotQualityProfile function # Extract sample names get.sample.name &lt;- function(fname) strsplit(basename(fname), &quot;_&quot;)[[1]][2] sample.names &lt;- unname(sapply(fnFs, get.sample.name)) head(sample.names) # check the quality for the first file plotQualityProfile(fnFs[1:1]) To interpret this plot, the gray-scale heatmap shows the the frequency of each quality score along the forward read lengths. The green line is the median quality score and the orange lines are the quartiles. The red line at the bottom of the plot represents the proportion of reads of that particular length. The quality is very good for our forward reads. You can also see that the majority of the forward reads are ~130bp long as the primer sequences have been removed from the raw fastq files. Now check the quality of the reverse file in the same way plotQualityProfile(fnRs[1:1]) The reverse reads also look like they are good quality. To check the quality of the second and third fastq files we would type. plotQualityProfile(fnFs[2:3]) plotQualityProfile(fnRs[2:3]) "],["07-clean.html", "Chapter 7 Cleaning your data", " Chapter 7 Cleaning your data We will now filter our data to remove any poor quality reads. First set the path to a directory to store the filtered output files called filtered. filtFs &lt;- file.path(output.path, &quot;../filtered&quot;, basename(fnFs)) filtRs &lt;- file.path(output.path, &quot;../filtered&quot;, basename(fnRs)) Now run filterAndTrim, using the standard filtering parameters: maxN=0 After truncation, sequences with more than 0 Ns will be discarded. (DADA2 requires sequences contain no Ns) truncQ = 2 Truncate reads at the first instance of a quality score less than or equal to 2 rm.phix = TRUE Discard reads that match against the PhiX genome maxEE=c(2, 2) After truncation, reads with higher than 2 “expected errors” will be discarded minLen = 60 Remove reads with length less than 60 multithread = TRUE input files are filtered in parallel out &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, maxN = 0, maxEE = c(2, 2), truncQ = 2, minLen = 60, rm.phix = TRUE, compress = TRUE, multithread = TRUE) out Some samples have very low read numbers after this filtering step. These could be poor quality samples but we also have negatives controls in this dataset so we would expect these to contain zero or very few reads. "],["08-ASV_identification.html", "Chapter 8 Identification of ASVs 8.1 Generate an error model 8.2 Dereplication 8.3 Inferrence of ASVs 8.4 Merging paired end reads 8.5 Making our ASV matrix 8.6 Chimera detection and removal 8.7 Sequence tracking sanity check", " Chapter 8 Identification of ASVs 8.1 Generate an error model First we need to model the error rates of our dataset using both the forward and reverse reads. Each dataset will have a specific error-signature with errors introduced by PCR amplification and sequencing. errF &lt;- learnErrors(filtFs, multithread = TRUE) errR &lt;- learnErrors(filtRs, multithread = TRUE) We can use the plotErrors function to check the estimated error rates. plotErrors(errF, nominalQ = TRUE) You will see the Warning messages: 1: Transformation introduced infinite values in continuous y-axis 2: Transformation introduced infinite values in continuous y-axis Do not worry about these warning messages. This is a message from the plotting function to let you know that there were some zero values in the data plotted (which turn into infinities on the log-scale). This is expected, it results from the fact that not every combination of error type (e.g. A-&gt;C) and quality score (e.g. 33) is observed in your data which is normal. Interpreting the plots. The error rates for each possible transition (e.g. A→C, A→G) are shown Red line - expected based on the quality score. (These are plotted when nominalQ = TRUE is included in the plot command) Black line - estimate Black dots - observed What we are expecting to see here is that the observed match up with estimates. Here we can see that the black dots track well with the black line. We can also see that the error rates drop with increasing quality score as we would expect. Sanity checks complete, we can proceed with the analysis. If you are worried about what your error plots look like when you fit your own dataset, one possible way to to improve the fit is to try increasing the number of bases the function is using (the default is 100 million) 8.2 Dereplication The next step is to dereplicate identical reads. This is a common step in many workflows used for processing amplicons. This saves time and processing power as indentical reads are collapsed together. For example instead of processing 100 identical sequences, only one is processed but the original number (i.e. 100) is associated with it. exists &lt;- file.exists(filtFs) # check that all the samples are still present after filtering derepFs &lt;- derepFastq(filtFs[exists], verbose=TRUE) derepRs &lt;- derepFastq(filtRs[exists], verbose=TRUE) # Name the derep-class objects by the sample names names(derepFs) &lt;- sample.names[exists] names(derepRs) &lt;- sample.names[exists] 8.3 Inferrence of ASVs Now we are ready to infer the ASVs in our dataset. To do this DADA2 uses the error models created above to infer the true sample composition (follow this link for more details). Here we will run the inference algorithm on single samples to save time but it is also possible to run samples together in pseudo-pools to increase the ability to identify ASVs of low abundance. Low abundance ASVs in a sample may be filtered out when run separately but if they are found in higher numbers in another sample then the chance of that ASV being real increases. Pseudo-pooling increases the chances of these “real” low abundance ASVs being retained within samples. Whether or not to use the pseudo-pooling option will depend on your dataset and experimental design (see https://benjjneb.github.io/dada2/pseudo.html#Pseudo-pooling for more information). dadaFs &lt;- dada(derepFs, err = errF, multithread = TRUE) dadaRs &lt;- dada(derepRs, err = errR, multithread = TRUE) It is important to note that you want to run the error and inference steps on datasets generated from a single Illumina run. Data generated from different runs can have different error structures. 8.4 Merging paired end reads Up until now we have carried out all filtering, error correction and inference on the forward and reverse reads separately. It is now time to merge the two files. By default the minimum overlap allowed between the two samples is 12 bp. mergers &lt;- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE) 8.5 Making our ASV matrix Now it is time to make the counts table. Each column represents a single ASV and each row is an individual sample. seqtab &lt;- makeSequenceTable(mergers) dim(seqtab) Q. How many ASVs are in our matrix? 8.6 Chimera detection and removal The last step in generating our ASV matrix is to detect and remove any chimeric sequences. Chimeric sequences are formed when two or more biological sequences joined together. This is fairly common in amplicon sequencing. DADA2 uses a method whereby it combines the left and right segments of abundant reads and compares these with lower abundant sequences. Any low abundant sequences that match are removed. seqtab.nochim &lt;- removeBimeraDenovo(seqtab, method=&quot;consensus&quot;, multithread=TRUE, verbose=TRUE) dim(seqtab.nochim) Q. How many ASVs remain after filtering out chimeras? Although this is a large proportion of sequence variants it should be a smaller proportion of the total sequences. sum(seqtab.nochim)/sum(seqtab) In these data ~ 17% of the merged sequence reads were identified as chimeric. Check the range of ASV lengths: table(nchar(getSequences(seqtab.nochim))) Sequences that are much longer or shorter than the expected amplicon length could be the result of non-specific priming, and can be removed from your sequence table (e.g. seqtab.len &lt;- seqtab[,nchar(colnames(seqtab)) %in% 150:180]). This is analogous to “cutting a band” in-silico to get amplicons of the targeted length, e.g. the above command would keep amplicons between 150 bp and 180 bp. This is dependent on the amplicon and you should only remove these if you do not expect any length variation, or after further investigation of what these short or long sequences are. For this example we will proceed without any amplicon length filtering. 8.7 Sequence tracking sanity check The last thing to do in this section is to track the number of sequences through the pipeline to check whether everything has run as expected and whether there are any steps where we loose a disproportionate number of sequences. If we end up with too few reads to run further analysis we can use this table to identify any step which might require further investigation and optimisation. getN &lt;- function(x) sum(getUniques(x)) track &lt;- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim)) colnames(track) &lt;- c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoisedF&quot;, &quot;denoisedR&quot;, &quot;merged&quot;, &quot;nonchim&quot;) rownames(track) &lt;- sample.names track "],["09-Assign_taxonomy.html", "Chapter 9 Assigning taxonomy", " Chapter 9 Assigning taxonomy To assign taxonomy we will use a custom reference database containing fish sequences available for the MiFish-U amplicon region for species found in Lake Tanganyika and its broader catchment area. taxa &lt;- assignTaxonomy(seqtab.nochim, &quot;/pub39/tea/matthew/NEOF/metabarcoding_workshop/taxonomy/MiFish_Reference_Database_taxonomy.fasta&quot;, multithread=TRUE, verbose = T) taxa.print &lt;- taxa rownames(taxa.print) &lt;- NULL head(taxa.print) This displays the taxonomic assignment of the first six ASVs at taxonomic levels from kingdom to species. The second ASV has not been assigned to any taxon in the reference database. Of the other five, two have been assigned at the species level, two at the genus level and one at the family level. "],["10-Further_analysis.html", "Chapter 10 Further analysis 10.1 R packages for further analysis 10.2 Rarefaction curves 10.3 Alpha diversity 10.4 Beta diversity", " Chapter 10 Further analysis In this section we will briefly discuss a few basic types of multivariate data analysis and data visualisation which are often used in metabarcoding studies. It is important to remember that there are several different ways to plot and analyse your data and we have presented only a few examples here. It might be that for your own data set and questions it would be better to approach your analysis in a different way. First we will load in a table which contains the meta data. meta&lt;-read.csv(&quot;/pub39/tea/matthew/NEOF/metabarcoding_workshop/sample_info.csv&quot;, row.names = 1) Before we start we will clean up the dataset. Our negative controls seem to be mostly clean of contaminants (with only 0 and 4 reads assigned to them). We will remove these for further analysis. Here we remove rows 25 and 31 which contain the filter (S62) and extraction (S72) control samples from our ASV table and metadata table. seqtab.rmcontrol&lt;-seqtab.nochim[-c(25,31),] meta.rmcontrol&lt;-meta[-c(25,31),] If you are concerned about contaminants in your own analysis then it could be useful to explore the R package decontam which is designed to identify and filter contaminants. We will also remove samples which appear to have failed/have very low sequence numbers in our dataset. #Check number of sequences per sample rowSums(seqtab.rmcontrol) # S40, S41, S50 all have low sequence counts. So we will remove these rows. seqtab.rmlow&lt;-seqtab.rmcontrol[-c(13,14,23),] meta.rmlow&lt;-meta.rmcontrol[-c(13,14,23),] # Print the minimum sequence number in one sample. min(rowSums(seqtab.rmlow)) # The lowest number of sequences in one sample is now 26667. 10.1 R packages for further analysis We will use three more handy R packages in order to further explore our data, phyloseq and vegan to explore the diverity in our data, plus the visualisation package ggplot2 for graphics. Load these libraries before proceeding with the steps below. library(phyloseq) library(vegan) library(ggplot2) 10.2 Rarefaction curves The more deeply we sequence a sample the more species we discover, until this accumulation levels off as we have found all or as many ASVs/species as we are going to find. This becomes a problem as samples are often sequenced at different depths so will have reached different points in the curve. This is a common difficulty as pooling and sequencing equal amounts of each sample can be tricky. One way to visualise this is to plot a rarefaction curve for each sample. rarecurve(seqtab.rmlow, step=100, col=meta.rmlow$COLOUR, lwd=2, ylab=&quot;ASVs&quot;, label=F) # add a vertical line to represent the fewest sequences in any sample abline(v=(min(rowSums(seqtab.rmlow)))) You can see that each sample levels off at a different sequencing depth and each sample has been sequenced at different depths. This also gives us an indication of how many ASVs are in each sample, with the aquarium samples here (dark blue lines) amongst the least diverse, as we’d expect. Rarefaction is one of the common approaches people use to correct for differences in library sequencing depth between samples and was originally proposed for traditional ecological studies (Sanders 1968). It involves randomly removing reads until you reach a number often equal to or less than the number of reads in the smallest sample. Random subsampling can result in a loss of data and generate artificial variation. Due to these concerns other methods of transformation have been suggested. Other methods of normalisation include normalising counts as a proportion of the total library size (McMurdie and Holmes 2014), centered log-ratio transformations (Gloor et al. 2017), geometric mean pairwise ratios (Chen et al. 2018), and also variance stabilising transformations and relative log expressions (Badri et al. 2018). Choosing the type of normalisation method to use for your own dataset is not trivial and the most appropriate method can vary between studies and datasets. Further reading is recommended. A couple of papers to get you started are listed below: McMurdie and Holmes (2014) Willis (2019) Cameron et al. (preprint) 10.3 Alpha diversity Measures of alpha diversity are used to describe diversity within a sample. We will use the R package phyloseq to plot alpha diversity. For this example we will proceed with unnormalised data. It is generally recommended not to normalise count data before calculating alpha diversity measures in the phyloseq FAQ. First make a phyloseq object. To do this we first read in our ASV, taxonomy and metadata tables before making the plyloseq object phylo. seqtab.rmlow2&lt;-t(as.data.frame(seqtab.rmlow)) phylo_asv &lt;- otu_table(seqtab.rmlow2, taxa_are_rows=TRUE) phylo_tax &lt;- tax_table(taxa) phylo_samples &lt;- sample_data(meta.rmlow) phylo &lt;- phyloseq(phylo_asv, phylo_tax, phylo_samples) sample_names(phylo) rank_names(phylo) sample_variables(phylo) The two alpha diversity metrics we will plot are: 1. Shannon’s diversity index A measure of diversity where a higher number means higher diversity. Shannon’s index accounts for the abundance and evenness of the features present. If richness and evenness increase the diversity score increases. Values can range from one (in case of a single dominant species) to the total number of all species (in case of all species having equal abundance). Equation: \\[ H = -\\sum_{i=1}^{S} p_i lnp_i \\] H = Shannon diversity index pi = is the proportion of species i S = Number of species 2. Simpson diversity index A measure of diversity based on number of features present and the relative abundance of each feature. If richness and evenness increase the diversity score increases. The values range from 0 (no diversity) to 1 (infinite diversity). Equation: \\[ D = 1 - {\\sum_{i=1}^{S} {p_i} ^{2}} \\] D = Simpson diversity index pi = is the proportion of species i S = Number of species Plot the two methods of calculating alpha diversity. plot_richness(phylo, measures=c(&quot;Shannon&quot;, &quot;Simpson&quot;), color = &quot;SITE&quot;) plot_richness(phylo, x=&quot;SITE&quot;, measures=c(&quot;Shannon&quot;, &quot;Simpson&quot;), color = &quot;SITE&quot;) + geom_boxplot() The first figure plots the diversity measure per sample and colours the output by site. The second figure combines the replicates to plot as a boxplot. 10.4 Beta diversity Beta diversity compares the difference in diversity between two sites, or to put it another way it calculates the number of species that are not the same in the two sites. We will normalise the data before running the beta diversity calculation. We will transform the data into proportions to be used for Bray-Curtis distances. ps.prop &lt;- transform_sample_counts(phylo, function(otu) otu/sum(otu)) We then generate and plot the NMDS (Non-metric MultiDimenstional Scaling) using Bray-Curtis distances. Equation: \\[ d_jk = \\sum\\frac{|x_{ij}-x_{ik}|} {(x_{ij}+x_{ik})} \\] ord.nmds.bray &lt;- ordinate(ps.prop, method=&quot;NMDS&quot;, distance=&quot;bray&quot;) plot_ordination(ps.prop, ord.nmds.bray, color=&quot;SITE&quot;, title=&quot;Bray NMDS&quot;) In general the samples do seem to cluster roughly by site in the NMDS plot. We will then calculate the Bray–Curtis distances using the distance function and perform a PERMANOVA (permutational multivariate analysis of variance) using the adonis function from Vegan to check whether the separation of samples by site is significantly different. bray.dist&lt;-distance(ps.prop, method=&quot;bray&quot;) sampledf &lt;- data.frame(sample_data(phylo)) adonis(bray.dist ~ SITE, data = sampledf) The PERMANOVA results suggest that there is a statistical difference in communities between sites. Lastly, let’s plot the proportion of ASV sequences within each sample that belong to different taxonomic families. plot_bar(ps.prop, fill = “Family”)+ geom_bar(aes(color=Family, fill=Family), stat=“identity”, position=“stack”)+ facet_grid(~SITE, scales = “free”, space = “free”) We can see that the five aquarium samples contain mostly Cichlidae sequences, apart from a very small number of unassigned (NA) and misassigned (there were only cichlid fish in the aquarium) sequences. Site 11 has more than 50% unassigned sequences in all three replicate samples. All of the samples containing &gt; 10% Procatopodidae sequences are grouped together in the NMDS plot (especially sites 19 &amp; 20, but also two samples from site 2, and the one sample from site 18), suggesting this may be in part driving the observed patterns of community similarity. "],["11-Exporting_data.html", "Chapter 11 Exporting data 11.1 Fasta file 11.2 Sequence count matrix 11.3 Table of taxon names", " Chapter 11 Exporting data It is useful to be able to export our ASV sequences and a matrix table of counts from DADA2/R as we might want to visualise or analyse these using other software. Our ASV sequences and counts per sample are stored in the object seqtab.nochim. The ASVs are not named, so first let’s name them (ASV_1, ASV_2, etc.). # The column names of seqtab.nochim are actually the ASV sequences, # so extract these and assign them to `mifish_seqs` mifish_seqs &lt;- colnames(seqtab.nochim) # Make a new variable for ASV names, `mifish_headers`, #with length equal to the number of ASVs mifish_headers &lt;- vector(dim(seqtab.nochim)[2], mode=&quot;character&quot;) # Fill the vector with names formatted for a fasta header (&gt;ASV_1, &gt;ASV_2, etc.) for (i in 1:dim(seqtab.nochim)[2]) { mifish_headers[i] &lt;- paste(&quot;&gt;ASV&quot;, i, sep=&quot;_&quot;) } 11.1 Fasta file Now we have our sequences and names as variables we can join them and make a fasta file. mifish_fasta &lt;- c(rbind(mifish_headers, mifish_seqs)) write(mifish_fasta, &quot;MiFish_ASVs.fa&quot;) You should now have this fasta file in your working directory on the server. 11.2 Sequence count matrix Next make a table of sequence counts for each sample and ASV. # First transpose the `seqtab.nochim` and assign this to the variable `mifish_tab` mifish_tab &lt;- t(seqtab.nochim) # Name each row with the ASV name, omitting the &#39;&gt;&#39; used in the fasta file row.names(mifish_tab) &lt;- sub(&quot;&gt;&quot;, &quot;&quot;, mifish_headers) write.table(mifish_tab, &quot;MiFish_ASV_counts.tsv&quot;, sep=&quot;\\t&quot;, quote=F, col.names=NA) You should now have an ASV by sample matrix with sequence counts in a tab-separated value (tsv) file in your working directory. 11.3 Table of taxon names Lastly, if we’ve used dada2 to assign taxonomy we can make a table of taxon names for each ASV. # Replace the row names in `taxa` with the ASV names, # omitting the &#39;&gt;&#39; used for the fasta file. rownames(taxa) &lt;- gsub(pattern=&quot;&gt;&quot;, replacement=&quot;&quot;, x=mifish_headers) write.table(taxa, &quot;MiFish_ASV_taxonomy.tsv&quot;, sep = &quot;\\t&quot;, quote=F, col.names=NA) You should now have a tsv file of taxonomic assignments for each ASV in your working directory. "],["12-Summary.html", "Chapter 12 Summary 12.1 NEOF training courses 12.2 Feedback and access to example data", " Chapter 12 Summary Over the last few hours we have gone from raw sequencing fastq data to calling ASVs and performing some initial downstream analyses of the diversity in our metabarcoding eDNA water samples. We hope this has given you an insight into the process and how you might apply these methods in your own field of research. 12.1 NEOF training courses For a more in-depth workshop covering more aspects of metabarcoding analysis, and indeed many other topics, visit our NEOF training web page Currently open for registration are the following courses We also run a 1-day course on using Linux command line and a 4-day course on the basics of R. To be kept informed of the next dates these courses will run you can subscribe to our newsletter here 12.2 Feedback and access to example data Thank you for coming! We hope you have enjoyed the workshop. We would love it if you could complete this very short feedback form to let us know what you’ve enjoyed and also importantly what can be improved. The link to the material will be available once you’ve completed this feedback. "]]
